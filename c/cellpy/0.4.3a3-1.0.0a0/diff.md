# Comparing `tmp/cellpy-0.4.3a3.tar.gz` & `tmp/cellpy-1.0.0a0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "cellpy-0.4.3a3.tar", last modified: Fri Mar 10 08:46:43 2023, max compression
+gzip compressed data, was "cellpy-1.0.0a0.tar", last modified: Mon May  1 14:46:53 2023, max compression
```

## Comparing `cellpy-0.4.3a3.tar` & `cellpy-1.0.0a0.tar`

### file list

```diff
@@ -1,172 +1,215 @@
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:43.517863 cellpy-0.4.3a3/
--rw-rw-rw-   0        0        0      456 2022-06-03 19:47:44.000000 cellpy-0.4.3a3/AUTHORS.rst
--rw-rw-rw-   0        0        0     3086 2022-09-20 08:21:07.000000 cellpy-0.4.3a3/CONTRIBUTING.rst
--rw-rw-rw-   0        0        0     3139 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/HISTORY.rst
--rw-rw-rw-   0        0        0     1089 2021-12-21 09:11:58.000000 cellpy-0.4.3a3/LICENSE
--rw-rw-rw-   0        0        0      645 2022-05-27 12:07:50.000000 cellpy-0.4.3a3/MANIFEST.in
--rw-rw-rw-   0        0        0     5437 2023-03-10 08:46:43.517863 cellpy-0.4.3a3/PKG-INFO
--rw-rw-rw-   0        0        0     1573 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/README.rst
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:42.543048 cellpy-0.4.3a3/cellpy/
--rw-rw-rw-   0        0        0      874 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/__init__.py
--rw-rw-rw-   0        0        0      108 2023-03-10 08:46:26.000000 cellpy-0.4.3a3/cellpy/_version.py
--rw-rw-rw-   0        0        0    48016 2023-03-09 12:36:14.000000 cellpy-0.4.3a3/cellpy/cli.py
--rw-rw-rw-   0        0        0     1192 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/exceptions.py
--rw-rw-rw-   0        0        0     4840 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/log.py
--rw-rw-rw-   0        0        0     1750 2021-12-21 09:11:58.000000 cellpy-0.4.3a3/cellpy/logging.json
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:42.594049 cellpy-0.4.3a3/cellpy/parameters/
--rw-rw-rw-   0        0        0     3144 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/parameters/.cellpy_prms_default.conf
--rw-rw-rw-   0        0        0        4 2021-12-21 09:11:58.000000 cellpy-0.4.3a3/cellpy/parameters/__init__.py
--rw-rw-rw-   0        0        0    13754 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/parameters/internal_settings.py
--rw-rw-rw-   0        0        0    12042 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/parameters/internal_settings_old.py
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:42.598049 cellpy-0.4.3a3/cellpy/parameters/legacy/
--rw-rw-rw-   0        0        0        0 2021-12-21 09:11:58.000000 cellpy-0.4.3a3/cellpy/parameters/legacy/__init__.py
--rw-rw-rw-   0        0        0     8784 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/parameters/legacy/internal_settings.py
--rw-rw-rw-   0        0        0     9455 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/parameters/prmreader.py
--rw-rw-rw-   0        0        0     9373 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/parameters/prms.py
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:42.607670 cellpy-0.4.3a3/cellpy/readers/
--rw-rw-rw-   0        0        0        2 2021-12-21 09:11:58.000000 cellpy-0.4.3a3/cellpy/readers/__init__.py
--rw-rw-rw-   0        0        0   223023 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/cellreader.py
--rw-rw-rw-   0        0        0    29817 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/core.py
--rw-rw-rw-   0        0        0    22382 2022-06-29 14:40:38.000000 cellpy-0.4.3a3/cellpy/readers/dbreader.py
--rw-rw-rw-   0        0        0     8120 2022-06-03 19:58:41.000000 cellpy-0.4.3a3/cellpy/readers/filefinder.py
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:42.779173 cellpy-0.4.3a3/cellpy/readers/instruments/
--rw-rw-rw-   0        0        0        0 2021-12-21 09:11:58.000000 cellpy-0.4.3a3/cellpy/readers/instruments/__init__.py
--rw-rw-rw-   0        0        0    58172 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/arbin_res.py
--rw-rw-rw-   0        0        0    19484 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/arbin_sql.py
--rw-rw-rw-   0        0        0    11188 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/arbin_sql_csv.py
--rw-rw-rw-   0        0        0     9930 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/arbin_sql_xlsx.py
--rw-rw-rw-   0        0        0    46783 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/backup_arbin.py
--rw-rw-rw-   0        0        0    25239 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/base.py
--rw-rw-rw-   0        0        0    23336 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/biologics_mpr.py
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:42.854194 cellpy-0.4.3a3/cellpy/readers/instruments/configurations/
--rw-rw-rw-   0        0        0     6607 2022-06-03 19:58:41.000000 cellpy-0.4.3a3/cellpy/readers/instruments/configurations/__init__.py
--rw-rw-rw-   0        0        0     1699 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/configurations/maccor_txt_four.py
--rw-rw-rw-   0        0        0     4083 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/configurations/maccor_txt_one.py
--rw-rw-rw-   0        0        0     1989 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/configurations/maccor_txt_three.py
--rw-rw-rw-   0        0        0     1787 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/configurations/maccor_txt_two.py
--rw-rw-rw-   0        0        0     3548 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/configurations/maccor_txt_zero.py
--rw-rw-rw-   0        0        0     2323 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/configurations/neware_txt_one.py
--rw-rw-rw-   0        0        0     2256 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/configurations/neware_txt_zero.py
--rw-rw-rw-   0        0        0    10316 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/custom.py
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:42.881860 cellpy-0.4.3a3/cellpy/readers/instruments/delete_these_in_february2022/
--rw-rw-rw-   0        0        0        0 2022-05-27 12:07:50.000000 cellpy-0.4.3a3/cellpy/readers/instruments/delete_these_in_february2022/__init__.py
--rw-rw-rw-   0        0        0     4073 2022-05-27 12:07:50.000000 cellpy-0.4.3a3/cellpy/readers/instruments/delete_these_in_february2022/_testing_txt.py
--rw-rw-rw-   0        0        0    27458 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/delete_these_in_february2022/maccor_txt_old.py
--rw-rw-rw-   0        0        0     4177 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/ext_nda_reader.py
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:42.894944 cellpy-0.4.3a3/cellpy/readers/instruments/loader_specific_modules/
--rw-rw-rw-   0        0        0        0 2022-06-03 19:58:41.000000 cellpy-0.4.3a3/cellpy/readers/instruments/loader_specific_modules/__init__.py
--rw-rw-rw-   0        0        0    22115 2022-06-03 19:58:41.000000 cellpy-0.4.3a3/cellpy/readers/instruments/loader_specific_modules/biologic_file_format.py
--rw-rw-rw-   0        0        0     1056 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/local_instrument.py
--rw-rw-rw-   0        0        0    12876 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/maccor_txt.py
--rw-rw-rw-   0        0        0     3520 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/neware_txt.py
--rw-rw-rw-   0        0        0    20433 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/old_custom.py
--rw-rw-rw-   0        0        0    16728 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/pec_csv.py
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:42.968943 cellpy-0.4.3a3/cellpy/readers/instruments/processors/
--rw-rw-rw-   0        0        0        0 2022-05-27 12:07:50.000000 cellpy-0.4.3a3/cellpy/readers/instruments/processors/__init__.py
--rw-rw-rw-   0        0        0    15146 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/readers/instruments/processors/post_processors.py
--rw-rw-rw-   0        0        0     1449 2022-05-27 12:07:50.000000 cellpy-0.4.3a3/cellpy/readers/instruments/processors/pre_processors.py
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:43.063951 cellpy-0.4.3a3/cellpy/utils/
--rw-rw-rw-   0        0        0      192 2021-12-21 09:11:58.000000 cellpy-0.4.3a3/cellpy/utils/__init__.py
--rw-rw-rw-   0        0        0    45082 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/utils/batch.py
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:43.085945 cellpy-0.4.3a3/cellpy/utils/batch_tools/
--rw-rw-rw-   0        0        0        0 2021-12-21 09:11:58.000000 cellpy-0.4.3a3/cellpy/utils/batch_tools/__init__.py
--rw-rw-rw-   0        0        0     7383 2023-03-09 15:37:57.000000 cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_analyzers.py
--rw-rw-rw-   0        0        0    18126 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_core.py
--rw-rw-rw-   0        0        0    24951 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_experiments.py
--rw-rw-rw-   0        0        0     2925 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_exporters.py
--rw-rw-rw-   0        0        0    13847 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_helpers.py
--rw-rw-rw-   0        0        0    23947 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_journals.py
--rw-rw-rw-   0        0        0    27767 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_plotters.py
--rw-rw-rw-   0        0        0      245 2021-12-21 09:11:58.000000 cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_reporters.py
--rw-rw-rw-   0        0        0     3339 2022-05-27 12:03:59.000000 cellpy-0.4.3a3/cellpy/utils/batch_tools/dumpers.py
--rw-rw-rw-   0        0        0     8794 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/utils/batch_tools/engines.py
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:43.096948 cellpy-0.4.3a3/cellpy/utils/data/
--rw-rw-rw-   0        0        0  3145889 2023-03-10 08:01:50.000000 cellpy-0.4.3a3/cellpy/utils/data/20160805_test001_45_cc.h5
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:43.114947 cellpy-0.4.3a3/cellpy/utils/data/raw/
--rw-rw-rw-   0        0        0  1613824 2021-12-21 09:11:58.000000 cellpy-0.4.3a3/cellpy/utils/data/raw/20160805_test001_45_cc_01.res
--rw-rw-rw-   0        0        0      260 2022-05-27 12:07:50.000000 cellpy-0.4.3a3/cellpy/utils/diagnostics.py
--rw-rw-rw-   0        0        0    79018 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/utils/easyplot.py
--rw-rw-rw-   0        0        0     1441 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/utils/example_data.py
--rw-rw-rw-   0        0        0    38187 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/utils/helpers.py
--rw-rw-rw-   0        0        0    37632 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/utils/ica.py
--rw-rw-rw-   0        0        0      189 2022-05-27 12:07:50.000000 cellpy-0.4.3a3/cellpy/utils/live.py
--rw-rw-rw-   0        0        0    23184 2023-03-10 07:49:23.000000 cellpy-0.4.3a3/cellpy/utils/ocv_rlx.py
--rw-rw-rw-   0        0        0    43176 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/cellpy/utils/plotutils.py
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:42.569048 cellpy-0.4.3a3/cellpy.egg-info/
--rw-rw-rw-   0        0        0     5437 2023-03-10 08:46:42.000000 cellpy-0.4.3a3/cellpy.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0     5124 2023-03-10 08:46:42.000000 cellpy-0.4.3a3/cellpy.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2023-03-10 08:46:42.000000 cellpy-0.4.3a3/cellpy.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0       42 2023-03-10 08:46:42.000000 cellpy-0.4.3a3/cellpy.egg-info/entry_points.txt
--rw-rw-rw-   0        0        0        2 2023-03-10 08:46:41.000000 cellpy-0.4.3a3/cellpy.egg-info/not-zip-safe
--rw-rw-rw-   0        0        0      259 2023-03-10 08:46:42.000000 cellpy-0.4.3a3/cellpy.egg-info/requires.txt
--rw-rw-rw-   0        0        0       13 2023-03-10 08:46:42.000000 cellpy-0.4.3a3/cellpy.egg-info/top_level.txt
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:43.273010 cellpy-0.4.3a3/docs/
--rw-rw-rw-   0        0        0     6939 2022-09-20 08:21:07.000000 cellpy-0.4.3a3/docs/Makefile
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:42.483049 cellpy-0.4.3a3/docs/_build/
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:42.482049 cellpy-0.4.3a3/docs/_build/.doctrees/
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:43.285003 cellpy-0.4.3a3/docs/_build/.doctrees/nbsphinx/
--rw-rw-rw-   0        0        0    15014 2023-01-20 10:44:21.000000 cellpy-0.4.3a3/docs/_build/.doctrees/nbsphinx/notebooks_tutorial_get_cap_6_0.png
--rw-rw-rw-   0        0        0    14599 2023-01-20 10:44:21.000000 cellpy-0.4.3a3/docs/_build/.doctrees/nbsphinx/notebooks_tutorial_get_cap_7_0.png
--rw-rw-rw-   0        0        0    13527 2023-01-20 10:44:21.000000 cellpy-0.4.3a3/docs/_build/.doctrees/nbsphinx/notebooks_tutorial_get_cap_8_0.png
--rw-rw-rw-   0        0        0    14101 2023-01-15 18:57:55.000000 cellpy-0.4.3a3/docs/_build/.doctrees/nbsphinx/notebooks_tutorial_simple_plot_2_0.png
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:43.305004 cellpy-0.4.3a3/docs/_build/_images/
--rw-rw-rw-   0        0        0    15014 2023-01-15 18:57:51.000000 cellpy-0.4.3a3/docs/_build/_images/notebooks_tutorial_get_cap_6_0.png
--rw-rw-rw-   0        0        0    14599 2023-01-15 18:57:51.000000 cellpy-0.4.3a3/docs/_build/_images/notebooks_tutorial_get_cap_7_0.png
--rw-rw-rw-   0        0        0    13527 2023-01-15 18:57:51.000000 cellpy-0.4.3a3/docs/_build/_images/notebooks_tutorial_get_cap_8_0.png
--rw-rw-rw-   0        0        0    14101 2023-01-15 18:57:55.000000 cellpy-0.4.3a3/docs/_build/_images/notebooks_tutorial_simple_plot_2_0.png
--rw-rw-rw-   0        0        0    88743 2023-01-18 20:40:28.000000 cellpy-0.4.3a3/docs/_build/_images/templates_jupyterlab_001.png
--rw-rw-rw-   0        0        0   296908 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/_build/_images/tutorials_utils_plotting_fig1.png
--rw-rw-rw-   0        0        0    54588 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/_build/_images/tutorials_utils_plotting_fig2.png
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:43.312011 cellpy-0.4.3a3/docs/_build/_static/
--rw-rw-rw-   0        0        0      286 2021-01-01 06:53:29.000000 cellpy-0.4.3a3/docs/_build/_static/file.png
--rw-rw-rw-   0        0        0       90 2021-01-01 06:53:29.000000 cellpy-0.4.3a3/docs/_build/_static/minus.png
--rw-rw-rw-   0        0        0       90 2021-01-01 06:53:29.000000 cellpy-0.4.3a3/docs/_build/_static/plus.png
--rw-rw-rw-   0        0        0       29 2021-12-21 09:11:59.000000 cellpy-0.4.3a3/docs/authors.rst
--rw-rw-rw-   0        0        0      301 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/basics.rst
--rw-rw-rw-   0        0        0    10477 2023-01-20 10:22:12.000000 cellpy-0.4.3a3/docs/conf.py
--rw-rw-rw-   0        0        0       34 2021-12-21 09:11:59.000000 cellpy-0.4.3a3/docs/contributing.rst
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:43.341004 cellpy-0.4.3a3/docs/developers/
--rw-rw-rw-   0        0        0      821 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/docs/developers/dev_cellpy_data_structure.rst
--rw-rw-rw-   0        0        0     5507 2022-11-17 18:21:31.000000 cellpy-0.4.3a3/docs/developers/dev_cellpy_folder_structure.rst
--rw-rw-rw-   0        0        0     1395 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/docs/developers/dev_various.rst
--rw-rw-rw-   0        0        0      153 2022-06-03 19:47:44.000000 cellpy-0.4.3a3/docs/developers_guide.rst
--rw-rw-rw-   0        0        0      202 2021-12-21 09:11:59.000000 cellpy-0.4.3a3/docs/examples.rst
--rw-rw-rw-   0        0        0    15953 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/docs/formats.rst
--rw-rw-rw-   0        0        0       29 2021-12-21 09:11:59.000000 cellpy-0.4.3a3/docs/history.rst
--rw-rw-rw-   0        0        0      826 2022-06-03 19:47:44.000000 cellpy-0.4.3a3/docs/index.rst
--rw-rw-rw-   0        0        0     4260 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/installation.rst
--rwxrwxrwx   0        0        0     6701 2022-09-20 08:21:07.000000 cellpy-0.4.3a3/docs/make.bat
--rw-rw-rw-   0        0        0      174 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/docs/notebooks.rst
--rw-rw-rw-   0        0        0       28 2021-12-21 09:11:59.000000 cellpy-0.4.3a3/docs/readme.rst
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:43.398863 cellpy-0.4.3a3/docs/source/
--rw-rw-rw-   0        0        0      769 2021-12-21 09:11:59.000000 cellpy-0.4.3a3/docs/source/cellpy.parameters.rst
--rw-rw-rw-   0        0        0     2112 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/source/cellpy.readers.instruments.rst
--rw-rw-rw-   0        0        0      960 2021-12-21 09:11:59.000000 cellpy-0.4.3a3/docs/source/cellpy.readers.rst
--rw-rw-rw-   0        0        0      698 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/source/cellpy.rst
--rw-rw-rw-   0        0        0     2405 2021-12-21 09:11:59.000000 cellpy-0.4.3a3/docs/source/cellpy.utils.batch_tools.rst
--rw-rw-rw-   0        0        0     1824 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/source/cellpy.utils.rst
--rw-rw-rw-   0        0        0       62 2021-12-21 09:11:59.000000 cellpy-0.4.3a3/docs/source/modules.rst
--rw-rw-rw-   0        0        0     1797 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/tips_and_tricks.rst
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:43.451869 cellpy-0.4.3a3/docs/tutorials/
--rw-rw-rw-   0        0        0     8436 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/docs/tutorials/basic_interaction.rst
--rw-rw-rw-   0        0        0     5898 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/docs/tutorials/configuring.rst
--rw-rw-rw-   0        0        0      981 2022-05-27 12:03:59.000000 cellpy-0.4.3a3/docs/tutorials/data_mining.rst
--rw-rw-rw-   0        0        0    12903 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/docs/tutorials/getting_started_tutorial.rst
--rw-rw-rw-   0        0        0     1222 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/docs/tutorials/pandas.rst
--rw-rw-rw-   0        0        0     1782 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/docs/tutorials/the_cellpy_cmd.rst
--rw-rw-rw-   0        0        0     3444 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/docs/usage.rst
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:43.509867 cellpy-0.4.3a3/docs/utils/
--rw-rw-rw-   0        0        0     4368 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/docs/utils/batch.rst
--rw-rw-rw-   0        0        0       87 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/utils/custom_file_loader.rst
--rw-rw-rw-   0        0        0       29 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/utils/easyplot.rst
-drwxrwxrwx   0        0        0        0 2023-03-10 08:46:43.514862 cellpy-0.4.3a3/docs/utils/figures/
--rw-rw-rw-   0        0        0   296908 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/utils/figures/tutorials_utils_plotting_fig1.png
--rw-rw-rw-   0        0        0    54588 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/utils/figures/tutorials_utils_plotting_fig2.png
--rw-rw-rw-   0        0        0     1080 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/utils/ica.rst
--rw-rw-rw-   0        0        0     1919 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/docs/utils/plotting.rst
--rw-rw-rw-   0        0        0       31 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/utils/templates.rst
--rw-rw-rw-   0        0        0     1186 2022-09-20 08:21:07.000000 cellpy-0.4.3a3/docs/utils/tut_ocv_rlx.rst
--rw-rw-rw-   0        0        0      327 2022-05-27 12:07:51.000000 cellpy-0.4.3a3/docs/utils.rst
--rw-rw-rw-   0        0        0      263 2022-06-03 19:47:44.000000 cellpy-0.4.3a3/pyproject.toml
--rw-rw-rw-   0        0        0       42 2023-03-10 08:46:43.518862 cellpy-0.4.3a3/setup.cfg
--rw-rw-rw-   0        0        0     3132 2023-03-09 10:27:42.000000 cellpy-0.4.3a3/setup.py
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.558574 cellpy-1.0.0a0/
+-rw-rw-rw-   0        0        0      480 2023-05-01 14:07:00.000000 cellpy-1.0.0a0/AUTHORS.rst
+-rw-rw-rw-   0        0        0     3086 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/CONTRIBUTING.rst
+-rw-rw-rw-   0        0        0     3908 2023-05-01 13:41:10.000000 cellpy-1.0.0a0/HISTORY.rst
+-rw-rw-rw-   0        0        0     1089 2021-12-21 09:11:58.000000 cellpy-1.0.0a0/LICENSE
+-rw-rw-rw-   0        0        0      645 2022-05-27 12:07:50.000000 cellpy-1.0.0a0/MANIFEST.in
+-rw-rw-rw-   0        0        0     6518 2023-05-01 14:46:53.557561 cellpy-1.0.0a0/PKG-INFO
+-rw-rw-rw-   0        0        0     1872 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/README.rst
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.248331 cellpy-1.0.0a0/cellpy/
+-rw-rw-rw-   0        0        0      805 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/__init__.py
+-rw-rw-rw-   0        0        0       24 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/_version.py
+-rw-rw-rw-   0        0        0    53661 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/cli.py
+-rw-rw-rw-   0        0        0     1228 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/exceptions.py
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.260332 cellpy-1.0.0a0/cellpy/internals/
+-rw-rw-rw-   0        0        0        0 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/internals/__init__.py
+-rw-rw-rw-   0        0        0    24093 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/internals/core.py
+-rw-rw-rw-   0        0        0     4838 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/log.py
+-rw-rw-rw-   0        0        0     1750 2021-12-21 09:11:58.000000 cellpy-1.0.0a0/cellpy/logging.json
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.268333 cellpy-1.0.0a0/cellpy/parameters/
+-rw-rw-rw-   0        0        0     3301 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/parameters/.cellpy_prms_default.conf
+-rw-rw-rw-   0        0        0        2 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/parameters/__init__.py
+-rw-rw-rw-   0        0        0    23361 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/parameters/internal_settings.py
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.271331 cellpy-1.0.0a0/cellpy/parameters/legacy/
+-rw-rw-rw-   0        0        0        0 2021-12-21 09:11:58.000000 cellpy-1.0.0a0/cellpy/parameters/legacy/__init__.py
+-rw-rw-rw-   0        0        0    24146 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/parameters/legacy/update_headers.py
+-rw-rw-rw-   0        0        0    12410 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/parameters/prmreader.py
+-rw-rw-rw-   0        0        0    12593 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/parameters/prms.py
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.282337 cellpy-1.0.0a0/cellpy/readers/
+-rw-rw-rw-   0        0        0        2 2021-12-21 09:11:58.000000 cellpy-1.0.0a0/cellpy/readers/__init__.py
+-rw-rw-rw-   0        0        0   227868 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/readers/cellreader.py
+-rw-rw-rw-   0        0        0    39447 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/readers/core.py
+-rw-rw-rw-   0        0        0    22998 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/readers/dbreader.py
+-rw-rw-rw-   0        0        0     9448 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/filefinder.py
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.308331 cellpy-1.0.0a0/cellpy/readers/instruments/
+-rw-rw-rw-   0        0        0        0 2021-12-21 09:11:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/__init__.py
+-rw-rw-rw-   0        0        0    50849 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/readers/instruments/arbin_res.py
+-rw-rw-rw-   0        0        0    19381 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/readers/instruments/arbin_sql.py
+-rw-rw-rw-   0        0        0    21062 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/arbin_sql_7.py
+-rw-rw-rw-   0        0        0    11134 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/arbin_sql_csv.py
+-rw-rw-rw-   0        0        0     7126 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/arbin_sql_h5.py
+-rw-rw-rw-   0        0        0     9883 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/arbin_sql_xlsx.py
+-rw-rw-rw-   0        0        0    27501 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/readers/instruments/base.py
+-rw-rw-rw-   0        0        0    22693 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/readers/instruments/biologics_mpr.py
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.321083 cellpy-1.0.0a0/cellpy/readers/instruments/configurations/
+-rw-rw-rw-   0        0        0     6607 2022-06-03 19:58:41.000000 cellpy-1.0.0a0/cellpy/readers/instruments/configurations/__init__.py
+-rw-rw-rw-   0        0        0     1700 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/configurations/maccor_txt_four.py
+-rw-rw-rw-   0        0        0     4084 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/configurations/maccor_txt_one.py
+-rw-rw-rw-   0        0        0     1990 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/configurations/maccor_txt_three.py
+-rw-rw-rw-   0        0        0     1788 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/configurations/maccor_txt_two.py
+-rw-rw-rw-   0        0        0     3549 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/configurations/maccor_txt_zero.py
+-rw-rw-rw-   0        0        0     2132 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/configurations/neware_txt_zero.py
+-rw-rw-rw-   0        0        0    10327 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/custom.py
+-rw-rw-rw-   0        0        0     3760 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/ext_nda_reader.py
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.324137 cellpy-1.0.0a0/cellpy/readers/instruments/loader_specific_modules/
+-rw-rw-rw-   0        0        0        0 2022-06-03 19:58:41.000000 cellpy-1.0.0a0/cellpy/readers/instruments/loader_specific_modules/__init__.py
+-rw-rw-rw-   0        0        0    22115 2022-06-03 19:58:41.000000 cellpy-1.0.0a0/cellpy/readers/instruments/loader_specific_modules/biologic_file_format.py
+-rw-rw-rw-   0        0        0     1067 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/local_instrument.py
+-rw-rw-rw-   0        0        0    12886 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/maccor_txt.py
+-rw-rw-rw-   0        0        0     3488 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/instruments/neware_txt.py
+-rw-rw-rw-   0        0        0    16769 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/readers/instruments/pec_csv.py
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.329260 cellpy-1.0.0a0/cellpy/readers/instruments/processors/
+-rw-rw-rw-   0        0        0        0 2022-05-27 12:07:50.000000 cellpy-1.0.0a0/cellpy/readers/instruments/processors/__init__.py
+-rw-rw-rw-   0        0        0    15265 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/readers/instruments/processors/post_processors.py
+-rw-rw-rw-   0        0        0     1450 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/readers/instruments/processors/pre_processors.py
+-rw-rw-rw-   0        0        0    26852 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/readers/sql_dbreader.py
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.354259 cellpy-1.0.0a0/cellpy/utils/
+-rw-rw-rw-   0        0        0      192 2021-12-21 09:11:58.000000 cellpy-1.0.0a0/cellpy/utils/__init__.py
+-rw-rw-rw-   0        0        0    49251 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/utils/batch.py
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.376257 cellpy-1.0.0a0/cellpy/utils/batch_tools/
+-rw-rw-rw-   0        0        0        0 2021-12-21 09:11:58.000000 cellpy-1.0.0a0/cellpy/utils/batch_tools/__init__.py
+-rw-rw-rw-   0        0        0     7578 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_analyzers.py
+-rw-rw-rw-   0        0        0    19566 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_core.py
+-rw-rw-rw-   0        0        0    40821 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_experiments.py
+-rw-rw-rw-   0        0        0     2931 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_exporters.py
+-rw-rw-rw-   0        0        0    13909 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_helpers.py
+-rw-rw-rw-   0        0        0    28305 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_journals.py
+-rw-rw-rw-   0        0        0    29066 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_plotters.py
+-rw-rw-rw-   0        0        0      245 2021-12-21 09:11:58.000000 cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_reporters.py
+-rw-rw-rw-   0        0        0     3339 2022-05-27 12:03:59.000000 cellpy-1.0.0a0/cellpy/utils/batch_tools/dumpers.py
+-rw-rw-rw-   0        0        0    10117 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/utils/batch_tools/engines.py
+-rw-rw-rw-   0        0        0     5294 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/utils/batch_tools/sqlite_from_excel_db.py
+-rw-rw-rw-   0        0        0    63308 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/utils/collectors.py
+-rw-rw-rw-   0        0        0    45461 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/utils/collectors_old.py
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.378258 cellpy-1.0.0a0/cellpy/utils/data/
+-rw-rw-rw-   0        0        0  3700143 2023-04-30 18:07:05.000000 cellpy-1.0.0a0/cellpy/utils/data/20160805_test001_45_cc.h5
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.388258 cellpy-1.0.0a0/cellpy/utils/data/raw/
+-rw-rw-rw-   0        0        0  1613824 2021-12-21 09:11:58.000000 cellpy-1.0.0a0/cellpy/utils/data/raw/20160805_test001_45_cc_01.res
+-rw-rw-rw-   0        0        0      260 2022-05-27 12:07:50.000000 cellpy-1.0.0a0/cellpy/utils/diagnostics.py
+-rw-rw-rw-   0        0        0    79016 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/utils/easyplot.py
+-rw-rw-rw-   0        0        0     1576 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/utils/example_data.py
+-rw-rw-rw-   0        0        0    39446 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/utils/helpers.py
+-rw-rw-rw-   0        0        0    38967 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/utils/ica.py
+-rw-rw-rw-   0        0        0      189 2022-05-27 12:07:50.000000 cellpy-1.0.0a0/cellpy/utils/live.py
+-rw-rw-rw-   0        0        0    24037 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/utils/ocv_rlx.py
+-rw-rw-rw-   0        0        0    45397 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/cellpy/utils/plotutils.py
+-rw-rw-rw-   0        0        0     1787 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/cellpy/utils/processor.py
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.258332 cellpy-1.0.0a0/cellpy.egg-info/
+-rw-rw-rw-   0        0        0     6518 2023-05-01 14:46:52.000000 cellpy-1.0.0a0/cellpy.egg-info/PKG-INFO
+-rw-rw-rw-   0        0        0     7846 2023-05-01 14:46:53.000000 cellpy-1.0.0a0/cellpy.egg-info/SOURCES.txt
+-rw-rw-rw-   0        0        0        1 2023-05-01 14:46:52.000000 cellpy-1.0.0a0/cellpy.egg-info/dependency_links.txt
+-rw-rw-rw-   0        0        0       42 2023-05-01 14:46:52.000000 cellpy-1.0.0a0/cellpy.egg-info/entry_points.txt
+-rw-rw-rw-   0        0        0        2 2023-05-01 14:46:52.000000 cellpy-1.0.0a0/cellpy.egg-info/not-zip-safe
+-rw-rw-rw-   0        0        0      337 2023-05-01 14:46:52.000000 cellpy-1.0.0a0/cellpy.egg-info/requires.txt
+-rw-rw-rw-   0        0        0       13 2023-05-01 14:46:52.000000 cellpy-1.0.0a0/cellpy.egg-info/top_level.txt
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.401260 cellpy-1.0.0a0/docs/
+-rw-rw-rw-   0        0        0     6939 2022-09-20 08:21:07.000000 cellpy-1.0.0a0/docs/Makefile
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.222812 cellpy-1.0.0a0/docs/_build/
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.221812 cellpy-1.0.0a0/docs/_build/.doctrees/
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.409259 cellpy-1.0.0a0/docs/_build/.doctrees/nbsphinx/
+-rw-rw-rw-   0        0        0    15014 2023-04-30 13:53:57.000000 cellpy-1.0.0a0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_6_0.png
+-rw-rw-rw-   0        0        0    14599 2023-04-30 13:53:57.000000 cellpy-1.0.0a0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_7_0.png
+-rw-rw-rw-   0        0        0    13527 2023-04-30 13:53:57.000000 cellpy-1.0.0a0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_8_0.png
+-rw-rw-rw-   0        0        0    25669 2023-04-30 13:54:02.000000 cellpy-1.0.0a0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_simple_plot_2_0.png
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.444899 cellpy-1.0.0a0/docs/_build/_images/
+-rw-rw-rw-   0        0        0    15014 2023-04-27 15:02:55.000000 cellpy-1.0.0a0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_6_0.png
+-rw-rw-rw-   0        0        0    14599 2023-04-27 15:02:55.000000 cellpy-1.0.0a0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_7_0.png
+-rw-rw-rw-   0        0        0    13527 2023-04-27 15:02:55.000000 cellpy-1.0.0a0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_8_0.png
+-rw-rw-rw-   0        0        0    25669 2023-04-27 15:03:02.000000 cellpy-1.0.0a0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_simple_plot_2_0.png
+-rw-rw-rw-   0        0        0    11678 2023-04-29 14:19:31.000000 cellpy-1.0.0a0/docs/_build/_images/graphviz-22185705e237fa530df24e4af50cb25833165e25.png
+-rw-rw-rw-   0        0        0    22040 2023-04-29 14:33:27.000000 cellpy-1.0.0a0/docs/_build/_images/graphviz-246f2486cd940f2ea40a07f847c86c22b2607ca8.png
+-rw-rw-rw-   0        0        0    21790 2023-04-29 14:26:00.000000 cellpy-1.0.0a0/docs/_build/_images/graphviz-42e9bc826d476a3d361a7f410e989ed34dc1aa85.png
+-rw-rw-rw-   0        0        0    32991 2023-04-29 14:26:01.000000 cellpy-1.0.0a0/docs/_build/_images/graphviz-619216a42370fd49669c083549129b8470c8fae1.png
+-rw-rw-rw-   0        0        0    32991 2023-04-30 20:02:23.000000 cellpy-1.0.0a0/docs/_build/_images/graphviz-6412a7c74952b4793798e9032f5bc4e7a1ab70c1.png
+-rw-rw-rw-   0        0        0     7231 2023-04-29 14:12:31.000000 cellpy-1.0.0a0/docs/_build/_images/graphviz-6deb64a460668e8ef9bf0ca653314119adeeae66.png
+-rw-rw-rw-   0        0        0    13360 2023-04-29 14:36:26.000000 cellpy-1.0.0a0/docs/_build/_images/graphviz-83b62e03ef369ff0a30f027892dba95b91ea8b6c.png
+-rw-rw-rw-   0        0        0    21790 2023-04-30 20:02:22.000000 cellpy-1.0.0a0/docs/_build/_images/graphviz-8ec82d564b1a6ea5b95a36a4a213f7a78aaedc63.png
+-rw-rw-rw-   0        0        0     5391 2023-04-29 14:10:02.000000 cellpy-1.0.0a0/docs/_build/_images/graphviz-ce8a9fe2ba01194aed847e0248d749db4093aca1.png
+-rw-rw-rw-   0        0        0    20826 2023-04-29 14:29:06.000000 cellpy-1.0.0a0/docs/_build/_images/graphviz-e94a5352318e02fcc5ef1f813e02a526c39af791.png
+-rw-rw-rw-   0        0        0    88743 2023-04-19 13:49:30.000000 cellpy-1.0.0a0/docs/_build/_images/templates_jupyterlab_001.png
+-rw-rw-rw-   0        0        0   296908 2022-05-27 12:07:51.000000 cellpy-1.0.0a0/docs/_build/_images/tutorials_utils_plotting_fig1.png
+-rw-rw-rw-   0        0        0    54588 2022-05-27 12:07:51.000000 cellpy-1.0.0a0/docs/_build/_images/tutorials_utils_plotting_fig2.png
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.450047 cellpy-1.0.0a0/docs/_build/_static/
+-rw-rw-rw-   0        0        0      286 2023-04-25 11:20:36.000000 cellpy-1.0.0a0/docs/_build/_static/file.png
+-rw-rw-rw-   0        0        0       90 2023-04-25 11:20:36.000000 cellpy-1.0.0a0/docs/_build/_static/minus.png
+-rw-rw-rw-   0        0        0       90 2023-04-25 11:20:36.000000 cellpy-1.0.0a0/docs/_build/_static/plus.png
+-rw-rw-rw-   0        0        0     1695 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/adapted_readme.rst
+-rw-rw-rw-   0        0        0    10731 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/conf.py
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.464048 cellpy-1.0.0a0/docs/developers_guide/
+-rw-rw-rw-   0        0        0     1334 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/developers_guide/dev_cellpy_data_structure.rst
+-rw-rw-rw-   0        0        0     5904 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/developers_guide/dev_cellpy_folder_structure.rst
+-rw-rw-rw-   0        0        0      935 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/developers_guide/dev_cellpy_packaging_pypi.rst
+-rw-rw-rw-   0        0        0     3009 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/developers_guide/dev_cellpy_setup.rst
+-rw-rw-rw-   0        0        0     1821 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/developers_guide/dev_conda_package.rst
+-rw-rw-rw-   0        0        0     2136 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/developers_guide/dev_docs.rst
+-rw-rw-rw-   0        0        0     5218 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/developers_guide/dev_loaders_and_instruments.rst
+-rw-rw-rw-   0        0        0     1768 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/developers_guide/dev_various.rst
+-rw-rw-rw-   0        0        0      326 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/developers_guide/index.rst
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.474047 cellpy-1.0.0a0/docs/examples_and_tutorials/
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.488048 cellpy-1.0.0a0/docs/examples_and_tutorials/basic_interactions/
+-rw-rw-rw-   0        0        0    15786 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/basic_interactions/01_getting_started_tutorial.rst
+-rw-rw-rw-   0        0        0     3909 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/basic_interactions/02_read_cell_data.rst
+-rw-rw-rw-   0        0        0     5485 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/basic_interactions/03_more_about_get.rst
+-rw-rw-rw-   0        0        0     4465 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/basic_interactions/04_other_interactions.rst
+-rw-rw-rw-   0        0        0     5908 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/basic_interactions/05_configuring.rst
+-rw-rw-rw-   0        0        0     1052 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/basic_interactions/06_pandas.rst
+-rw-rw-rw-   0        0        0     1102 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/basic_interactions/07_data_mining.rst
+-rw-rw-rw-   0        0        0     8224 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/basic_interactions/08_the_cellpy_cmd.rst
+-rw-rw-rw-   0        0        0      484 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/basics.rst
+-rw-rw-rw-   0        0        0      366 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/examples.rst
+-rw-rw-rw-   0        0        0      174 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/index.rst
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.498046 cellpy-1.0.0a0/docs/examples_and_tutorials/loaders/
+-rw-rw-rw-   0        0        0       49 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/loaders/01_arbin.rst
+-rw-rw-rw-   0        0        0       52 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/loaders/02_maccor.rst
+-rw-rw-rw-   0        0        0       43 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/loaders/03_PEC.rst
+-rw-rw-rw-   0        0        0       54 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/loaders/04_Neware.rst
+-rw-rw-rw-   0        0        0       62 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/loaders/05_biologics.rst
+-rw-rw-rw-   0        0        0       54 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/loaders/06_custom.rst
+-rw-rw-rw-   0        0        0      260 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/loaders.rst
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.224850 cellpy-1.0.0a0/docs/examples_and_tutorials/notebooks/
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.502048 cellpy-1.0.0a0/docs/examples_and_tutorials/notebooks/images/
+-rw-rw-rw-   0        0        0    88743 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/notebooks/images/templates_jupyterlab_001.png
+-rw-rw-rw-   0        0        0    95663 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/notebooks/images/templates_jupyterlab_002.png
+-rw-rw-rw-   0        0        0      231 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/notebooks.rst
+-rw-rw-rw-   0        0        0     1797 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/tips_and_tricks.rst
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.513049 cellpy-1.0.0a0/docs/examples_and_tutorials/utils/
+-rw-rw-rw-   0        0        0     4388 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/utils/batch.rst
+-rw-rw-rw-   0        0        0       59 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/utils/collectors.rst
+-rw-rw-rw-   0        0        0       53 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/utils/easyplot.rst
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.518048 cellpy-1.0.0a0/docs/examples_and_tutorials/utils/figures/
+-rw-rw-rw-   0        0        0   296908 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/utils/figures/tutorials_utils_plotting_fig1.png
+-rw-rw-rw-   0        0        0    54588 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/utils/figures/tutorials_utils_plotting_fig2.png
+-rw-rw-rw-   0        0        0     1219 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/utils/ica.rst
+-rw-rw-rw-   0        0        0     2063 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/utils/plotting.rst
+-rw-rw-rw-   0        0        0      338 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/utils/templates.rst
+-rw-rw-rw-   0        0        0     1379 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/utils/tut_ocv_rlx.rst
+-rw-rw-rw-   0        0        0      371 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/examples_and_tutorials/utils.rst
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.522046 cellpy-1.0.0a0/docs/figures/
+-rw-rw-rw-   0        0        0     9981 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/figures/cellpy-icon-bw.png
+-rw-rw-rw-   0        0        0    10302 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/figures/cellpy-logo-v1.png
+-rw-rw-rw-   0        0        0      593 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/index.rst
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.534561 cellpy-1.0.0a0/docs/main_description/
+-rw-rw-rw-   0        0        0       32 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/main_description/authors.rst
+-rw-rw-rw-   0        0        0       37 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/main_description/contributing.rst
+-rw-rw-rw-   0        0        0    16327 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/main_description/formats.rst
+-rw-rw-rw-   0        0        0       32 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/main_description/history.rst
+-rw-rw-rw-   0        0        0      182 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/docs/main_description/index.rst
+-rw-rw-rw-   0        0        0     4288 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/main_description/installation.rst
+-rw-rw-rw-   0        0        0     3444 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/main_description/usage.rst
+-rwxrwxrwx   0        0        0     6701 2022-09-20 08:21:07.000000 cellpy-1.0.0a0/docs/make.bat
+drwxrwxrwx   0        0        0        0 2023-05-01 14:46:53.556582 cellpy-1.0.0a0/docs/source/
+-rw-rw-rw-   0        0        0      367 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/source/cellpy.internals.rst
+-rw-rw-rw-   0        0        0      447 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/source/cellpy.parameters.legacy.rst
+-rw-rw-rw-   0        0        0      847 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/source/cellpy.parameters.rst
+-rw-rw-rw-   0        0        0     1911 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/source/cellpy.readers.instruments.configurations.rst
+-rw-rw-rw-   0        0        0      631 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/source/cellpy.readers.instruments.loader_specific_modules.rst
+-rw-rw-rw-   0        0        0      783 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/source/cellpy.readers.instruments.processors.rst
+-rw-rw-rw-   0        0        0     3413 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/source/cellpy.readers.instruments.rst
+-rw-rw-rw-   0        0        0     1139 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/source/cellpy.readers.rst
+-rw-rw-rw-   0        0        0      721 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/source/cellpy.rst
+-rw-rw-rw-   0        0        0     2610 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/source/cellpy.utils.batch_tools.rst
+-rw-rw-rw-   0        0        0     2222 2023-05-01 10:21:37.000000 cellpy-1.0.0a0/docs/source/cellpy.utils.rst
+-rw-rw-rw-   0        0        0       62 2023-05-01 10:14:23.000000 cellpy-1.0.0a0/docs/source/modules.rst
+-rw-rw-rw-   0        0        0      281 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/pyproject.toml
+-rw-rw-rw-   0        0        0       42 2023-05-01 14:46:53.558574 cellpy-1.0.0a0/setup.cfg
+-rw-rw-rw-   0        0        0     2952 2023-04-30 13:34:58.000000 cellpy-1.0.0a0/setup.py
```

### Comparing `cellpy-0.4.3a3/CONTRIBUTING.rst` & `cellpy-1.0.0a0/CONTRIBUTING.rst`

 * *Files 9% similar despite different names*

```diff
@@ -6,60 +6,60 @@
 
 Contributions are welcome, and they are greatly appreciated! Every
 little bit helps, and credit will always be given.
 
 You can contribute in many ways:
 
 Types of Contributions
-----------------------
+======================
 
 Report Bugs
-~~~~~~~~~~~
+-----------
 
 Report bugs at https://github.com/jepegit/cellpy/issues.
 
 If you are reporting a bug, please include:
 
 * Your operating system name and version.
 * Any details about your local setup that might be helpful in troubleshooting.
 * Detailed steps to reproduce the bug.
 
 Fix Bugs
-~~~~~~~~
+--------
 
 Look through the GitHub issues for bugs. Anything tagged with "bug"
 and "help wanted" is open to whoever wants to implement it.
 
 Implement Features
-~~~~~~~~~~~~~~~~~~
+------------------
 
 Look through the GitHub issues for features. Anything tagged with "enhancement"
 and "help wanted" is open to whoever wants to implement it.
 
 Write Documentation
-~~~~~~~~~~~~~~~~~~~
+-------------------
 
 cellpy could always use more documentation, whether as part of the
 official cellpy docs, in docstrings, or even on the web in blog posts,
 articles, and such.
 
 Submit Feedback
-~~~~~~~~~~~~~~~
+---------------
 
 The best way to send feedback is to file an issue at https://github.com/jepegit/cellpy/issues.
 
 If you are proposing a feature:
 
 * Explain in detail how it would work.
 * Keep the scope as narrow as possible, to make it easier to implement.
 * Remember that this is a volunteer-driven project, and that contributions
   are welcome :)
 
 Get Started!
-------------
+============
 
 Ready to contribute? Here's how to set up `cellpy` for local development.
 
 1. Fork the `cellpy` repo on GitHub.
 2. Clone your fork locally::
 
     $ git clone git@github.com:your_name_here/cellpy.git
@@ -92,22 +92,22 @@
     $ git commit -m "Your detailed description of your changes."
     $ git push origin name-of-your-bugfix-or-feature
 
 7. Submit a pull request through the GitHub website.
 
 
 Pull Request Guidelines
------------------------
+=======================
 
 Before you submit a pull request, check that it meets these guidelines:
 
 1. The pull request should include tests.
 2. The pull request should not include any gluten.
 
 Tips
-----
+====
 
 To self-hypnotize yourself to sleep well at night::
 
 $ echo "You feel sleepy"
 $ echo "You are a great person"
```

### Comparing `cellpy-0.4.3a3/LICENSE` & `cellpy-1.0.0a0/LICENSE`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/MANIFEST.in` & `cellpy-1.0.0a0/MANIFEST.in`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/PKG-INFO` & `cellpy-1.0.0a0/PKG-INFO`

 * *Files 19% similar despite different names*

```diff
@@ -1,32 +1,36 @@
 Metadata-Version: 2.1
 Name: cellpy
-Version: 0.4.3a3
-Summary: Extract and manipulate data from battery cell testers.
+Version: 1.0.0a0
+Summary: Extract and manipulate data from battery data testers.
 Home-page: https://github.com/jepegit/cellpy
 Author: Jan Petter Maehlen
 Author-email: jepe@ife.no
 License: MIT license
 Keywords: cellpy
-Classifier: Development Status :: 3 - Alpha
+Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Natural Language :: English
-Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
 Provides-Extra: batch
 Provides-Extra: fit
 Provides-Extra: all
 License-File: LICENSE
 License-File: AUTHORS.rst
 
-===============================
-cellpy
-===============================
+.. image:: https://github.com/jepegit/cellpy/blob/master/docs/_static/cellpy-icon-long.svg
+  :height: 100
+  :alt: cellpy
+
+===================================================================
+cellpy - *a library for assisting in analysing batteries and cells*
+===================================================================
 
 
 .. image:: https://img.shields.io/pypi/v/cellpy.svg
         :target: https://pypi.python.org/pypi/cellpy
 
 .. image:: https://img.shields.io/travis/jepegit/cellpy.svg
         :target: https://travis-ci.org/jepegit/cellpy
@@ -40,177 +44,198 @@
 
 
 This Python Package was developed to help the
 researchers at IFE, Norway, in their cumbersome task of
 interpreting and handling data from cycling tests of
 batteries and cells.
 
+
 Documentation
--------------
+=============
 
 The documentation for ``cellpy`` is hosted on `Read the docs
 <https://cellpy.readthedocs.io>`_.
 
 
 Installation and dependencies
------------------------------
+=============================
 
 The easiest way to install ``cellpy`` is to install with conda or pip.
 
 With conda::
 
-   conda install cellpy --channel conda-forge
+   conda install -c conda-forge cellpy
 
 Or if you prefer installing using pip::
 
-   pip install cellpy
+   python -m pip install cellpy
 
 Have a look at the documentation for more detailed installation procedures, especially
 with respect to "difficult" dependencies when installing with pip.
 
 Licence
--------
+=======
 
 ``cellpy`` is free software made available under the MIT License.
 
 Features
---------
+========
 
-* Load test-data and store in hdf5 format.
+* Load test-data and store in a common format.
+* Summarize and compare data.
 * Filter out the steps of interest.
 * Process and plot the data.
 * And more...
 
 
 
 
 =======
 History
 =======
 
+
+1.0.0 (2023) [under development]
+================================
+
+* Unit handling: renaming summary headers
+* Unit handling: new cellpy-file-format version
+* Unit handling: tool for converting old to new format
+* Templates: using one repository with sub-folders
+* Templates: adding more documentation
+* File handling: allow for external raw files (ssh)
+* Readers: neware.txt (one version/model)
+* Readers: arbin_sql7 (experimental, @jtgibson91)
+* Batch plotting: collectors for both data collection, plotting and saving
+* Internals: rename main classes (CellpyData -> CellpyCell, Cell -> Data)
+* Internals: rename .cell property to .data
+* Internals: allow for only one Data-object pr CellpyCell object
+
+
 0.4.3 (2023)
-------------
+============
 
 * Neware txt loader (supports one specific format only, other formats will have to wait for v.1.0)
 
 
 0.4.2 (2022)
-------------
+============
 
 * Changed definition of Coulombic Difference (negative of previous)
 * Updated loaders with hooks and additional base class TxtLoader with configuration mechanism
 * Support for Maccor txt files
 * Supports only python 3.8 and up
 * Optional parameters through batch and pages
 * Several bug fixes and minor improvements / adjustments
 * Restrict use of instrument label to only one option
+* Fix bug in example file (@kevinsmia1939)
 
 
 0.4.1 (2021)
-------------
+============
 
 * Updated documentations
 * CLI improvements
 * New argument for get_cap: max_cycle
 * Reverting from using Documents to user home for location of prm file in windows.
 * Easyplot by Amund
 * Arbin sql reader by Muhammad
 
 
 0.4.0 (2020)
-------------
+============
 
 * Reading arbin .res files with auxiliary data should now work.
 * Many bugs have been removed - many new introduced.
 * Now on conda-forge (can be installed using conda).
 
 
 0.4.0 a2 (2020)
----------------
+===============
 
 * Reading PEC files now updated and should work
 
 
 0.4.0 a1 (2020)
----------------
+===============
 
 * New column names (lowercase and underscore)
 * New batch concatenating and plotting routines
 
 
 0.3.3 (2020)
-------------
+============
 
 * Switching from git-flow to github-flow
 * New cli options for running batches
 * cli option for creating template notebooks
 * Using ruamel.yaml instead of pyyaml
 * Using python-box > 4
 * Several bug-fixes
 
 
 0.3.2 (2019)
-------------
+============
 
 * Starting fixing documentation
 * TODO: create conda package
 * TODO: extensive tests
 
 
 0.3.1 (2019)
-------------
+============
 
 * Refactoring - renaming from dfsummary to summary
 * Refactoring - renaming from step_table to steps
 * Refactoring - renaming from dfdata to raw
-* Refactoring - renaming cellpy.cell to cellpy.get
+* Refactoring - renaming cellpy.data to cellpy.get
 * Updated save and load cellpy files allowing for new naming
 * Implemented cellpy new and cellpy serve cli functionality
 
 
 0.3.0 (2019)
-------------
+============
 
 * New batch-feature
 * Improved make-steps and make-summary functionality
 * Improved cmd-line interface for setup
 * More helper functions and tools
 * Experimental support for other instruments
 * invoke tasks for developers
 
 0.2.1 (2018)
-------------
+============
 
 * Allow for using mdbtools also on win
 * Slightly faster find_files using cache and fnmatch
 * Bug fix: error in sorting files when using pathlib fixed
 
 
 0.2.0 (2018-10-17)
-------------------
+==================
 
 * Improved creation of step tables (much faster)
 * Default compression on cellpy (hdf5) files
 * Bug fixes
 
 
 0.1.22 (2018-07-17)
--------------------
+===================
 
 * Parameters can be set by dot-notation (python-box).
 * The parameter Instruments.cell_configuration is removed.
 * Options for getting voltage curves in different formats.
 * Fixed python 3.6 issues with Read the Docs.
 * Can now also be used on posix (the user must install mdb_tools first).
 * Improved logging allowing for custom log-directory.
 
 
 0.1.21 (2018-06-09)
--------------------
+===================
 
 * No legacy python.
 
 
 0.1.0 (2016-09-26)
-------------------
+==================
 
 * First release on PyPI.
```

### Comparing `cellpy-0.4.3a3/README.rst` & `cellpy-1.0.0a0/README.rst`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,14 @@
-===============================
-cellpy
-===============================
+.. image:: https://github.com/jepegit/cellpy/blob/master/docs/_static/cellpy-icon-long.svg
+  :height: 100
+  :alt: cellpy
+
+===================================================================
+cellpy - *a library for assisting in analysing batteries and cells*
+===================================================================
 
 
 .. image:: https://img.shields.io/pypi/v/cellpy.svg
         :target: https://pypi.python.org/pypi/cellpy
 
 .. image:: https://img.shields.io/travis/jepegit/cellpy.svg
         :target: https://travis-ci.org/jepegit/cellpy
@@ -18,44 +22,46 @@
 
 
 This Python Package was developed to help the
 researchers at IFE, Norway, in their cumbersome task of
 interpreting and handling data from cycling tests of
 batteries and cells.
 
+
 Documentation
--------------
+=============
 
 The documentation for ``cellpy`` is hosted on `Read the docs
 <https://cellpy.readthedocs.io>`_.
 
 
 Installation and dependencies
------------------------------
+=============================
 
 The easiest way to install ``cellpy`` is to install with conda or pip.
 
 With conda::
 
-   conda install cellpy --channel conda-forge
+   conda install -c conda-forge cellpy
 
 Or if you prefer installing using pip::
 
-   pip install cellpy
+   python -m pip install cellpy
 
 Have a look at the documentation for more detailed installation procedures, especially
 with respect to "difficult" dependencies when installing with pip.
 
 Licence
--------
+=======
 
 ``cellpy`` is free software made available under the MIT License.
 
 Features
---------
+========
 
-* Load test-data and store in hdf5 format.
+* Load test-data and store in a common format.
+* Summarize and compare data.
 * Filter out the steps of interest.
 * Process and plot the data.
 * And more...
```

### Comparing `cellpy-0.4.3a3/cellpy/exceptions.py` & `cellpy-1.0.0a0/cellpy/exceptions.py`

 * *Files 21% similar despite different names*

```diff
@@ -51,16 +51,18 @@
 
 class NullData(Error):
     """Raised when required data is missing (e.g. voltage = None or summary_frames are missing)"""
 
     pass
 
 
-class NoCellFound(Error):
-    """Raised when there are no cells, but a cell is needed."""
+class NoDataFound(Error):
+    """Raised when there are no cells, but a data is needed."""
 
     pass
 
 
 class UnderDefined(Error):
     """Raised when trying something that requires you to set
-    a missing prm first"""
+    a missing prm on environment variable first"""
+
+    pass
```

### Comparing `cellpy-0.4.3a3/cellpy/log.py` & `cellpy-1.0.0a0/cellpy/log.py`

 * *Files 0% similar despite different names*

```diff
@@ -12,16 +12,16 @@
 
 from cellpy import prms
 
 logging.raiseExceptions = False
 
 
 def setup_logging(
-    default_json_path=None,
     default_level=None,
+    default_json_path=None,
     env_key="LOG_CFG",
     custom_log_dir=None,
     reset_big_log=False,
     max_size=5_000_000,
     testing=False,
 ):
     """Setup logging configuration.
@@ -32,27 +32,26 @@
         env_key (str): use this environment prm to try to get default_json_path.
         custom_log_dir: path for saving logs.
         reset_big_log (bool): reset log if too big (max_size).
         max_size (int): if reset_log, this is the max limit.
         testing (bool): set as True if testing, and you don't want to create any .log files
 
     """
+    if default_level is None:
+        default_level = "CRITICAL"
 
     if not default_json_path:
         default_json_path = os.path.join(
             os.path.dirname(os.path.realpath(__file__)), "logging.json"
         )
     path = default_json_path
     value = os.getenv(env_key, None)
     if value:
         path = value
 
-    if default_level is None:
-        default_level = "CRITICAL"
-
     # loading logging configs
     if os.path.exists(path):
         with open(path, "rt") as f:
             config = json.load(f)
 
         if testing:
             log_dir = tempfile.mkdtemp()
```

### Comparing `cellpy-0.4.3a3/cellpy/logging.json` & `cellpy-1.0.0a0/cellpy/logging.json`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/cellpy/parameters/prmreader.py` & `cellpy-1.0.0a0/cellpy/parameters/prmreader.py`

 * *Files 22% similar despite different names*

```diff
@@ -7,34 +7,60 @@
 import sys
 import warnings
 from collections import OrderedDict
 from dataclasses import asdict, dataclass
 from pprint import pprint
 
 import box
+import dotenv
 import ruamel
 from ruamel.yaml import YAML
 from ruamel.yaml.error import YAMLError
 
 from cellpy.exceptions import ConfigFileNotRead, ConfigFileNotWritten
 from cellpy.parameters import prms
+from cellpy.parameters.internal_settings import OTHERPATHS
+from cellpy.internals.core import OtherPath
 
 DEFAULT_FILENAME_START = ".cellpy_prms_"
 DEFAULT_FILENAME_END = ".conf"
 USE_MY_DOCUMENTS = False
 
 DEFAULT_FILENAME = DEFAULT_FILENAME_START + "default" + DEFAULT_FILENAME_END
 
 # logger = logging.getLogger(__name__)
 
 yaml = YAML()
 
 
+def initialize():
+    """initializes cellpy by reading the config file and the environment file"""
+    try:
+        _read_prm_file(_get_prm_file())
+        _load_env_file()
+    except FileNotFoundError:
+        warnings.warn("Could not find the config-file")
+    except UserWarning:
+        warnings.warn("Could not read the config-file")
+
+
+def _load_env_file():
+    """loads the environment file"""
+    env_file = pathlib.Path(prms.Paths.env_file)
+    env_file_in_user_dir = pathlib.Path.home() / prms.Paths.env_file
+    if env_file.is_file():
+        dotenv.load_dotenv(env_file)
+    elif env_file_in_user_dir.is_file():
+        dotenv.load_dotenv(env_file_in_user_dir)
+    else:
+        logging.debug("No .env file found")
+
+
 def get_user_name():
-    """get the user name of the current user (cross platform)"""
+    """get the username of the current user (cross-platform)"""
     return getpass.getuser()
 
 
 def create_custom_init_filename(user_name=None):
     """creates a custom prms filename"""
     if user_name is None:
         return DEFAULT_FILENAME_START + get_user_name() + DEFAULT_FILENAME_END
@@ -74,46 +100,61 @@
             yaml.explicit_end = True
             yaml.dump(config_dict, config_file)
     except YAMLError:
         raise ConfigFileNotWritten
 
 
 def _update_prms(config_dict):
+    """updates the prms with the values in the config_dict"""
+    # config_dict is your current config
+    # _config_attr is the attribute in the prms module (i.e. the defaults)
     logging.debug("updating parameters")
     logging.debug(f"new prms: {config_dict}")
     for key in config_dict:
+        if config_dict[key] is None:
+            logging.debug(f"{config_dict[key]} is None")
+            continue
         if hasattr(prms, key):
             _config_attr = getattr(prms, key)
             is_path = isinstance(_config_attr, prms.PathsClass)
+            if _config_attr is None:
+                logging.debug(f"{_config_attr} is None")
+                continue
             for k in config_dict[key]:
                 z = config_dict[key][k]
                 if is_path:
                     _txt = f"{k}: {z}"
-                    if (
-                        not k.lower() == "db_filename"
-                    ):  # special hack because it is a filename and not a path
+                    if k.lower() == "db_filename":
+                        # special hack because it is a filename and not a path
+                        pass
+                    elif k.lower() in OTHERPATHS:
+                        # special hack because it is possibly an external location
+                        z = OtherPath(
+                            str(z)
+                        ).resolve()  # v1.0.0: this is only resolving local paths
+                    else:
                         z = pathlib.Path(z).resolve()
                     _txt += f" -> {z}"
                     logging.debug("converting to pathlib.Path")
                     logging.debug(_txt)
+
                 if isinstance(z, dict):
                     y = getattr(_config_attr, k)
                     z = box.Box({**y, **z})
                 if isinstance(z, ruamel.yaml.comments.CommentedMap):
                     z = box.Box(z)
                 setattr(_config_attr, k, z)
         else:
             logging.info("\n  not-supported prm: %s" % key)
 
 
 def _convert_instruments_to_dict(x):
     # Converting instruments to dictionary (since it contains box.Box objects)
     d = asdict(x)
     for k, v in d.items():
-
         try:
             d[k] = v.to_dict()
         except AttributeError:
             pass
 
     return d
 
@@ -123,33 +164,55 @@
         dictionary = x.to_dict()
     except AttributeError:
         dictionary = asdict(x)
     return dictionary
 
 
 def _convert_paths_to_dict(x):
-    try:
-        dictionary = x.to_dict()
-    except AttributeError:
-        dictionary = asdict(x)
-    dictionary = {k: str(dictionary[k]) for k in dictionary}
+    dictionary = {}
+    for k in x.keys():
+        # hack to get around the leading underscore (since they are properties):
+        if len(k) > 1 and k[0] == "_" and k.lower()[1:] in OTHERPATHS:
+            t = getattr(x, k).full_path
+            k = k[1:]
+        else:
+            t = str(getattr(x, k))
+        dictionary[k] = t
     return dictionary
 
 
+def _update_and_convert_to_dict(parameter_name):
+    """check that all the parameters are correct in the prm-file"""
+    # update from old prm-file (before v1.0.0):
+    if parameter_name == "DbCols":
+        if hasattr(prms, "DbCols"):
+            db_cols = _convert_to_dict(prms.DbCols)
+            if db_cols is None:
+                return prms.DbColsClass()
+
+            for k in db_cols:
+                if isinstance(db_cols[k], (list, tuple)):
+                    db_cols[k] = db_cols[k][0]
+            return db_cols
+        else:
+            return prms.DbColsClass()
+
+
 def _pack_prms():
     """if you introduce new 'save-able' parameter dictionaries, then you have
     to include them here"""
 
     config_dict = {
         "Paths": _convert_paths_to_dict(prms.Paths),
         "FileNames": _convert_to_dict(prms.FileNames),
         "Db": _convert_to_dict(prms.Db),
-        "DbCols": _convert_to_dict(prms.DbCols),
-        "DataSet": _convert_to_dict(prms.DataSet),
+        "DbCols": _update_and_convert_to_dict("DbCols"),
+        "CellInfo": _convert_to_dict(prms.CellInfo),
         "Reader": _convert_to_dict(prms.Reader),
+        "Materials": _convert_to_dict(prms.Materials),
         "Instruments": _convert_instruments_to_dict(prms.Instruments),
         "Batch": _convert_to_dict(prms.Batch),
     }
     return config_dict
 
 
 def _read_prm_file(prm_filename):
@@ -190,35 +253,35 @@
     """returns name of the prm file"""
     if file_name is not None:
         if os.path.isfile(file_name):
             return file_name
         else:
             logging.info("Could not find the prm-file")
 
-    default_name = prms._prm_default_name
-    prm_globtxt = prms._prm_globtxt
+    default_name = prms._prm_default_name  # NOQA
+    prm_globtxt = prms._prm_globtxt  # NOQA
 
     script_dir = os.path.abspath(os.path.dirname(__file__))
 
     search_path = dict()
     search_path["curdir"] = os.path.abspath(os.path.dirname(sys.argv[0]))
     search_path["filedir"] = script_dir
-    search_path["userdir"] = get_user_dir()
+    search_path["user_dir"] = get_user_dir()
 
     if search_order is None:
-        search_order = ["userdir"]  # ["curdir","filedir", "userdir",]
+        search_order = ["user_dir"]  # ["curdir","filedir", "user_dir",]
     else:
         search_order = search_order
 
     # The default name for the prm file is at the moment in the script-dir,@
-    # while default searching is in the userdir (yes, I know):
+    # while default searching is in the user_dir (yes, I know):
     prm_default = os.path.join(script_dir, default_name)
 
     # -searching-----------------------
-    search_dict = OrderedDict()
+    search_dict: OrderedDict[Any] = OrderedDict()
 
     for key in search_order:
         search_dict[key] = [None, None]
         prm_directory = search_path[key]
         default_file = os.path.join(prm_directory, default_name)
 
         if os.path.isfile(default_file):
@@ -249,41 +312,50 @@
     else:
         prm_filename = prm_default
     return prm_filename
 
 
 def _save_current_prms_to_user_dir():
     # This should be put into the cellpy setup script
-    file_name = os.path.join(prms.user_dir, prms._prm_default_name)
+    file_name = os.path.join(prms.user_dir, prms._prm_default_name)  # NOQA
     _write_prm_file(file_name)
 
 
 def info():
     """this function will show only the 'box'-type
     attributes and their content in the cellpy.prms module"""
     print("Convenience function for listing prms")
     print(prms.__name__)
     print(f"prm file (for current user): {_get_prm_file()}")
     print()
 
     for key, current_object in prms.__dict__.items():
-
-        if key.startswith("_") and not key.startswith("__") and prms._debug:
+        if key.startswith("_") and not key.startswith("__") and prms._debug:  # NOQA
             print(f"Internal: {key} (type={type(current_object)}): {current_object}")
 
         elif isinstance(current_object, box.Box):
             print()
             print(" OLD-TYPE PRM ".center(80, "="))
             print(f"prms.{key}:")
             print(80 * "-")
             for subkey in current_object:
                 print(f"prms.{key}.{subkey} = ", f"{current_object[subkey]}")
             print()
 
-        elif isinstance(current_object, prms.CellPyConfig):
+        elif key == "Paths":
+            print(" NEW-TYPE PRM WITH OTHERPATHS ".center(80, "*"))
+            attributes = {
+                k: v for k, v in vars(current_object).items() if not k.startswith("_")
+            }
+            for attr in OTHERPATHS:
+                attributes[attr] = getattr(current_object, attr)
+            pprint(attributes, width=1)
+
+        elif isinstance(current_object, (prms.CellPyConfig, prms.CellPyDataConfig)):
+            print(" NEW-TYPE PRM ".center(80, "="))
             attributes = {
                 k: v for k, v in vars(current_object).items() if not k.startswith("_")
             }
             print(f" {key} ".center(80, "="))
             pprint(attributes, width=1)
             print()
```

### Comparing `cellpy-0.4.3a3/cellpy/parameters/prms.py` & `cellpy-1.0.0a0/cellpy/parameters/prms.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,67 +1,91 @@
 """cellpy parameters"""
-
+from __future__ import annotations
 import os
 import sys
 from dataclasses import dataclass, field
 from pathlib import Path
-from typing import List, Tuple, Union
+from typing import List, Tuple, Union, Optional, TYPE_CHECKING
+
+# Using TYPE_CHECKING to avoid circular imports
+# (this will only work without from __future__ import annotations for python 3.11 and above)
+from cellpy.internals.core import OtherPath
 
 import box
 
 # When adding prms, please
 #   1) check / update the internal_settings.py file as well to
 #      ensure that copying / splitting cellpy objects
 #      behaves properly.
 #   2) check / update the .cellpy_prms_default.conf file
 
 # locations etc. for reading custom parameters
 script_dir = os.path.abspath(os.path.dirname(__file__))
 cur_dir = os.path.abspath(os.path.dirname(sys.argv[0]))
-user_dir = os.path.expanduser("~")
+user_dir = Path.home()
 wdir = Path(cur_dir)
+op_wdir = str(wdir)
 
 
 @dataclass
 class CellPyDataConfig:
-    """Settings that can be unique for each CellpyData instance."""
+    """Settings that can be unique for each CellpyCell instance."""
 
     ...
 
 
 @dataclass
 class CellPyConfig:
     """Session settings (global)."""
 
-    ...
+    def keys(self):
+        return self.__dataclass_fields__.keys()
 
 
 # If updating this, you will have to do a lot of tweaks.
 #   .cellpy_prms_default.conf
 #   cli.py (_update_paths)
 #   test_cli_setup_interactive (NUMBER_OF_DIRS)
 #   test_prms.py (config_file_txt)
+#   _convert_paths_to_dict
+
 
 # This can stay global:
 @dataclass
 class PathsClass(CellPyConfig):
     outdatadir: Union[Path, str] = wdir
-    rawdatadir: Union[Path, str] = wdir
-    cellpydatadir: Union[Path, str] = wdir
-    db_path: Union[Path, str] = wdir
+    _rawdatadir: Union[OtherPath, str] = op_wdir
+    _cellpydatadir: Union[OtherPath, str] = op_wdir
+    db_path: Union[Path, str] = wdir  # used for simple excel db reader
     filelogdir: Union[Path, str] = wdir
     examplesdir: Union[Path, str] = wdir
     notebookdir: Union[Path, str] = wdir
     templatedir: Union[Path, str] = wdir
     batchfiledir: Union[Path, str] = wdir
     instrumentdir: Union[Path, str] = wdir
-    db_filename: str = "cellpy_db.xlsx"
+    db_filename: str = "cellpy_db.xlsx"  # used for simple excel db reader
+    env_file: Union[Path, str] = user_dir / ".env_cellpy"
+
+    @property
+    def rawdatadir(self) -> OtherPath:
+        return OtherPath(self._rawdatadir)
+
+    @rawdatadir.setter
+    def rawdatadir(self, value: Union[OtherPath, Path, str]):
+        self._rawdatadir = OtherPath(value)
+
+    @property
+    def cellpydatadir(self) -> OtherPath:
+        return OtherPath(self._cellpydatadir)
+
+    @cellpydatadir.setter
+    def cellpydatadir(self, value: Union[OtherPath, Path, str]):
+        self._cellpydatadir = OtherPath(value)
 
 
-# This can stay global:
 @dataclass
 class BatchClass(CellPyConfig):
     template: str = "standard"
     fig_extension: str = "png"
     backend: str = "bokeh"
     notebook: bool = True
     dpi: int = 300
@@ -72,139 +96,205 @@
     summary_plot_width: int = 900
     summary_plot_height: int = 800
     summary_plot_height_fractions: List[float] = field(
         default_factory=lambda: [0.2, 0.5, 0.3]
     )
 
 
-# This can stay global:
 @dataclass
 class FileNamesClass(CellPyConfig):
     file_name_format: str = "YYYYMMDD_[NAME]EEE_CC_TT_RR"
     raw_extension: str = "res"
     reg_exp: str = None
     file_list_location: str = None
     file_list_type: str = None
     file_list_name: str = None
     cellpy_file_extension: str = "h5"
 
 
-# This can stay global:
 @dataclass
 class ReaderClass(CellPyConfig):
     diagnostics: bool = False
     filestatuschecker: str = "size"
     force_step_table_creation: bool = True
     force_all: bool = False  # not used yet - should be used when saving
     sep: str = ";"
     cycle_mode: str = "anode"
     sorted_data: bool = True  # finding step-types assumes sorted data
-    load_only_summary: bool = False
     select_minimal: bool = False
-    limit_loaded_cycles: Union[
-        int, None
+    limit_loaded_cycles: Optional[
+        int
     ] = None  # limit loading cycles to given cycle number
     ensure_step_table: bool = False
-    daniel_number: int = 5
+    ensure_summary_table: bool = False
     voltage_interpolation_step: float = 0.01
     time_interpolation_step: float = 10.0
     capacity_interpolation_step: float = 2.0
     use_cellpy_stat_file: bool = False
-    auto_dirs: bool = True  # search in prm-file for res and hdf5 dirs in loadcell
+    auto_dirs: bool = True  # search in prm-file for res and hdf5 dirs in cellpy.get()
 
 
-# This can stay global:
 @dataclass
 class DbClass(CellPyConfig):
     db_type: str = "simple_excel_reader"
-    db_table_name: str = "db_table"
-    db_header_row: int = 0
-    db_unit_row: int = 1
-    db_data_start_row: int = 2
-    db_search_start_row: int = 2
-    db_search_end_row: int = -1
-
-
-# This can stay global:
-@dataclass
-class DbColsClass(CellPyConfig):
-    id: Tuple[str, str] = ("id", "int")
-    exists: Tuple[str, str] = ("exists", "bol")
-    batch: Tuple[str, str] = ("batch", "str")
-    sub_batch_01: Tuple[str, str] = ("b01", "str")
-    sub_batch_02: Tuple[str, str] = ("b02", "str")
-    sub_batch_03: Tuple[str, str] = ("b03", "str")
-    sub_batch_04: Tuple[str, str] = ("b04", "str")
-    sub_batch_05: Tuple[str, str] = ("b05", "str")
-    sub_batch_06: Tuple[str, str] = ("b06", "str")
-    sub_batch_07: Tuple[str, str] = ("b07", "str")
-    project: Tuple[str, str] = ("project", "str")
-    label: Tuple[str, str] = ("label", "str")
-    group: Tuple[str, str] = ("group", "int")
-    selected: Tuple[str, str] = ("selected", "bol")
-    cell_name: Tuple[str, str] = ("cell", "str")
-    cell_type: Tuple[str, str] = ("cell_type", "cat")
-    experiment_type: Tuple[str, str] = ("experiment_type", "cat")
-    active_material: Tuple[str, str] = ("mass_active_material", "float")
-    total_material: Tuple[str, str] = ("mass_total", "float")
-    loading: Tuple[str, str] = ("loading_active_material", "float")
-    nom_cap: Tuple[str, str] = ("nominal_capacity", "float")
-    file_name_indicator: Tuple[str, str] = ("file_name_indicator", "str")
-    instrument: Tuple[str, str] = ("instrument", "str")
-    raw_file_names: Tuple[str, str] = ("raw_file_names", "Tuple[str, str]")
-    cellpy_file_name: Tuple[str, str] = ("cellpy_file_name", "str")
-    comment_slurry: Tuple[str, str] = ("comment_slurry", "str")
-    comment_cell: Tuple[str, str] = ("comment_cell", "str")
-    comment_general: Tuple[str, str] = ("comment_general", "str")
-    freeze: Tuple[str, str] = ("freeze", "bol")
-    argument: Tuple[str, str] = ("argument", "str")  # e.g. 'max_cycle:100;recalc:false'
-
-
-# TODO: This should not stay global:
-@dataclass
-class DataSetClass(CellPyDataConfig):
-    """Values used when processing the data (will be deprecated)"""
-
-    nom_cap: float = 3579
+    db_table_name: str = "db_table"  # used for simple excel db reader
+    db_header_row: int = 0  # used for simple excel db reader
+    db_unit_row: int = 1  # used for simple excel db reader
+    db_data_start_row: int = 2  # used for simple excel db reader
+    db_search_start_row: int = 2  # used for simple excel db reader
+    db_search_end_row: int = -1  # used for simple excel db reader
+    db_file_sqlite: str = "excel.db"  # used when converting from Excel to sqlite
+    # database connection string - used for more advanced db readers:
+    db_connection: Optional[str] = None
+
+
+@dataclass
+class DbColsClass(CellPyConfig):  # used for simple excel db reader
+    # Note to developers:
+    #  1) This is ONLY for the excel-reader (dbreader.py)! More advanced
+    #     readers should get their own way of handling the db-columns.
+    #  2) If you would like to change the names of the attributes,
+    #     you will have to change the names in the
+    #        a .cellpy_prms_default.conf
+    #        b. dbreader.py
+    #        c. test_dbreader.py
+    #        d. internal_settings.py (renaming when making sqlite from Excel)
+    #     As well as the DbColsTypeClass below.
+
+    id: str = "id"
+    exists: str = "exists"
+    project: str = "project"
+    label: str = "label"
+    group: str = "group"
+    selected: str = "selected"
+    cell_name: str = "cell"
+    cell_type: str = "cell_type"
+    experiment_type: str = "experiment_type"
+    mass_active: str = "mass_active_material"
+    area: str = "area"
+    mass_total: str = "mass_total"
+    loading: str = "loading_active_material"
+    nom_cap: str = "nominal_capacity"
+    file_name_indicator: str = "file_name_indicator"
+    instrument: str = "instrument"
+    raw_file_names: str = "raw_file_names"
+    cellpy_file_name: str = "cellpy_file_name"
+    comment_slurry: str = "comment_slurry"
+    comment_cell: str = "comment_cell"
+    comment_general: str = "comment_general"
+    freeze: str = "freeze"
+    argument: str = "argument"
+
+    batch: str = "batch"
+    sub_batch_01: str = "b01"
+    sub_batch_02: str = "b02"
+    sub_batch_03: str = "b03"
+    sub_batch_04: str = "b04"
+    sub_batch_05: str = "b05"
+    sub_batch_06: str = "b06"
+    sub_batch_07: str = "b07"
+
+
+@dataclass
+class DbColsUnitClass(CellPyConfig):
+    # Note to developers:
+    #  1) This is ONLY for the excel-reader (dbreader.py)! More advanced
+    #     readers should get their own way of handling the db-columns.
+
+    id: str = "str"
+    exists: str = "int"
+    project: str = "str"
+    label: str = "str"
+    group: str = "str"
+    selected: str = "int"
+    cell_name: str = "str"
+    cell_type: str = "str"
+    experiment_type: str = "str"
+    mass_active: str = "float"
+    area: str = "float"
+    mass_total: str = "float"
+    loading: str = "float"
+    nom_cap: str = "float"
+    file_name_indicator: str = "str"
+    instrument: str = "str"
+    raw_file_names: str = "str"
+    cellpy_file_name: str = "str"
+    comment_slurry: str = "str"
+    comment_cell: str = "str"
+    comment_general: str = "str"
+    freeze: str = "int"
+    argument: str = "str"
+
+    batch: str = "str"
+    sub_batch_01: str = "str"
+    sub_batch_02: str = "str"
+    sub_batch_03: str = "str"
+    sub_batch_04: str = "str"
+    sub_batch_05: str = "str"
+    sub_batch_06: str = "str"
+    sub_batch_07: str = "str"
+
+
+@dataclass
+class CellInfoClass(CellPyDataConfig):
+    """Values used for setting the parameters related to the cell and the cycling"""
+
+    voltage_lim_low: float = 0.0
+    voltage_lim_high: float = 1.0
+    active_electrode_area: float = 1.0
+    active_electrode_thickness: float = 1.0
+    electrolyte_volume: float = 1.0
+
+    electrolyte_type: str = "standard"
+    active_electrode_type: str = "standard"
+    counter_electrode_type: str = "standard"
+    reference_electrode_type: str = "standard"
+    experiment_type: str = "cycling"
+    cell_type: str = "standard"
+    separator_type: str = "standard"
+    active_electrode_current_collector: str = "standard"
+    reference_electrode_current_collector: str = "standard"
+    comment: str = ""
 
 
-# TODO: This should not stay global:
 @dataclass
 class MaterialsClass(CellPyDataConfig):
     """Default material-specific values used in processing the data."""
 
     cell_class: str = "Li-Ion"
     default_material: str = "silicon"
     default_mass: float = 1.0
-    default_nom_cap: float = 1.0  # not used yet - should replace the DataSet class
+    default_nom_cap: float = 1.0
+    default_nom_cap_specifics: str = "gravimetric"
 
 
 Paths = PathsClass()
 FileNames = FileNamesClass()
 Reader = ReaderClass()
 Db = DbClass()
 DbCols = DbColsClass()
-DataSet = DataSetClass()
+CellInfo = CellInfoClass()
 Materials = MaterialsClass()
 Batch = BatchClass(summary_plot_height_fractions=[0.2, 0.5, 0.3])
 
 
 # ------------------------------------------------------------------------------
 # Instruments
 #
-#  This should be updated - currently using dicts instead of sub-classes of
+#  This should be updated - currently using dicts instead of subclasses of
 #  dataclasses. I guess I could update this but is a bit challenging
 #  so maybe replace later  using e.g. pydantic
 # ------------------------------------------------------------------------------
 
+
 # This can stay global:
 # remark! using box.Box for each instrument
 @dataclass
 class InstrumentsClass(CellPyConfig):
-    tester: str
+    tester: Union[str, None]
     custom_instrument_definitions_file: Union[str, None]
     Arbin: box.Box
     Maccor: box.Box
     Neware: box.Box
 
 
 # Pre-defined instruments:
@@ -215,39 +305,40 @@
     "max_chunks": None,
     "use_subprocess": False,
     "detect_subprocess_need": False,
     "sub_process_path": None,
     "office_version": "64bit",
     "SQL_server": r"localhost\SQLEXPRESS",
     "SQL_UID": "sa",
-    "SQL_PWD": "Changeme123",
+    "SQL_PWD": "ChangeMe123",
     "SQL_Driver": "SQL Server",
 }
 
 Arbin = box.Box(Arbin)
 
 Maccor = {"default_model": "one"}
 Maccor = box.Box(Maccor)
 
 Neware = {"default_model": "one"}
 Neware = box.Box(Neware)
 
 Instruments = InstrumentsClass(
-    tester="arbin_res",  # TODO: moving this to DataSetClass (deprecate)
+    tester=None,  # TODO: moving this to DataSetClass (deprecate)
     custom_instrument_definitions_file=None,
     Arbin=Arbin,
     Maccor=Maccor,
     Neware=Neware,
 )
 
 
-# ---------------------------
-# Other secret- or non-config
-# ---------------------------
+# ------------------------------------------------------------------------------
+# Other secret- or non-config (only for developers)
+# ------------------------------------------------------------------------------
 
+_db_cols_unit = DbColsUnitClass()
 _debug = False
 _variable_that_is_not_saved_to_config = "Hei"
 _prm_default_name = ".cellpy_prms_default.conf"
 _prm_globtxt = ".cellpy_prms*.conf"
 _odbcs = ["pyodbc", "ado", "pypyodbc"]
 _odbc = "pyodbc"
 _search_for_odbc_driver = True
@@ -258,27 +349,34 @@
 _sort_if_subprocess = True
 
 _cellpyfile_root = "CellpyData"
 _cellpyfile_raw = "/raw"
 _cellpyfile_step = "/steps"
 _cellpyfile_summary = "/summary"
 _cellpyfile_fid = "/fid"
+_cellpyfile_common_meta = "/info"
+_cellpyfile_test_dependent_meta = "/info_test_dependent"
+
+_cellpyfile_raw_unit_pre_id = "raw_unit_"
+_cellpyfile_raw_limit_pre_id = ""
 
 _cellpyfile_complevel = 1
-_cellpyfile_complib = None  # currently defaults to "zlib"
+_cellpyfile_complib = None  # currently, defaults to "zlib"
 _cellpyfile_raw_format = "table"
 _cellpyfile_summary_format = "table"
 _cellpyfile_stepdata_format = "table"
 _cellpyfile_infotable_format = "fixed"
 _cellpyfile_fidtable_format = "fixed"
 
 # templates
+_standard_template_uri = "https://github.com/jepegit/cellpy_cookies.git"
+
 _registered_templates = {
-    "standard": "https://github.com/jepegit/cellpy_cookie_standard.git",
-    "ife": "https://github.com/jepegit/cellpy_cookie_ife.git",
+    "standard": (_standard_template_uri, "standard"),  # (repository, name-of-folder)
+    "ife": (_standard_template_uri, "ife"),
 }
 
 # used as global variables
 _globals_status = ""
 _globals_errors = []
 _globals_message = []
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/cellreader.py` & `cellpy-1.0.0a0/cellpy/readers/cellreader.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,76 +1,88 @@
 # -*- coding: utf-8 -*-
 """Datareader for cell testers and potentiostats.
 
 This module is used for loading data and databases created by different cell
-testers. Currently it only accepts arbin-type res-files (access) data as
-raw data files, but we intend to implement more types soon. It also creates
-processed files in the hdf5-format.
+testers and exporing them in a common hdf5-format.
 
 Example:
-    >>> d = CellpyData()
-    >>> d.loadcell(names = [file1.res, file2.res]) # loads and merges the runs
-    >>> voltage_curves = d.get_cap()
-    >>> d.save("mytest.hdf")
+    >>> c = cellpy.get(["super_battery_run_01.res", "super_battery_run_02.res"]) # loads and merges the runs
+    >>> voltage_curves = c.get_cap()
+    >>> c.save("super_battery_run.hdf")
 
 """
 
 import collections
 import copy
 import csv
 import itertools
 import logging
+import numbers
 import os
 import sys
 import time
 import warnings
-from pathlib import Path, PurePosixPath, PureWindowsPath
-from typing import Union
+from pathlib import Path
+from typing import Union, Sequence, List
+from dataclasses import asdict
 
 import numpy as np
 import pandas as pd
 from pandas.errors import PerformanceWarning
+from pint import Quantity
 from scipy import interpolate
 
-from cellpy.exceptions import DeprecatedFeature, NullData, WrongFileVersion, NoCellFound
+from cellpy.exceptions import (
+    DeprecatedFeature,
+    NullData,
+    WrongFileVersion,
+    NoDataFound,
+    UnderDefined,
+)
 from cellpy.parameters import prms
+from cellpy.parameters.legacy.update_headers import (
+    rename_summary_columns,
+    rename_raw_columns,
+    rename_fid_columns,
+    rename_step_columns,
+)
 from cellpy.parameters.internal_settings import (
-    ATTRS_CELLPYDATA,
-    ATTRS_CELLPYFILE,
-    ATTRS_DATASET,
-    ATTRS_DATASET_DEEP,
     get_cellpy_units,
     get_headers_normal,
     get_headers_step_table,
     get_headers_summary,
     headers_normal,
     headers_step_table,
     headers_summary,
     get_default_raw_units,
     get_default_output_units,
-)
-from cellpy.parameters.legacy import internal_settings as old_settings
-from cellpy.readers.core import (
     CELLPY_FILE_VERSION,
     MINIMUM_CELLPY_FILE_VERSION,
     PICKLE_PROTOCOL,
-    Cell,
+    CellpyUnits,
+    CellpyMetaCommon,
+    CellpyMetaIndividualTest,
+)
+
+from cellpy.readers.core import (
+    Data,
     FileID,
     identify_last_data_point,
     interpolate_y_on_x,
     pickle_protocol,
     xldate_as_datetime,
-    InstrumentFactory,
-    find_all_instruments,
     generate_default_factory,
+    Q,
+    convert_from_simple_unit_label_to_string_unit_label,
 )
+from cellpy.internals.core import OtherPath
 
-HEADERS_NORMAL = get_headers_normal()
-HEADERS_SUMMARY = get_headers_summary()
-HEADERS_STEP_TABLE = get_headers_step_table()
+HEADERS_NORMAL = get_headers_normal()  # TODO @jepe refactor this (not needed)
+HEADERS_SUMMARY = get_headers_summary()  # TODO @jepe refactor this (not needed)
+HEADERS_STEP_TABLE = get_headers_step_table()  # TODO @jepe refactor this (not needed)
 
 # TODO: @jepe - new feature - method for assigning new cycle numbers and step numbers
 #   - Sometimes the user forgets to increment the cycle number and it would be good
 #   to have a method so that its possible to set new cycle numbers manually
 #   - Some testers merges different steps into one (e.g CC-CV), it would be nice to have
 #   a method for "splitting that up"
 
@@ -80,173 +92,157 @@
     performance_warning_level, category=pd.io.pytables.PerformanceWarning
 )
 pd.set_option("mode.chained_assignment", None)  # "raise", "warn", None
 
 module_logger = logging.getLogger(__name__)
 
 
-class CellpyData:
+class CellpyCell:
     """Main class for working and storing data.
 
     This class is the main work-horse for cellpy where all the functions for
     reading, selecting, and tweaking your data is located. It also contains the
     header definitions, both for the cellpy hdf5 format, and for the various
     cell-tester file-formats that can be read. The class can contain
     several cell-tests and each test is stored in a list. If you see what I mean...
 
     Attributes:
-        cells (list): list of DataSet objects.
+        # TODO v.1.0.1: update this
+        data: cellpy.Data object
     """
 
     def __repr__(self):
-        txt = f"CellpyData-object (id={hex(id(self))})"
-        if self.name:
-            txt += f"\nname: {self.name}"
+        txt = f"CellpyCell-object (id={hex(id(self))})"
+        if self.session_name:
+            txt += f"\nname: {self.session_name}"
         if self.table_names:
             txt += f"\ntable_names: {self.table_names}"
         if self.tester:
             txt += f"\ntester: {self.tester}"
-
-        number_of_cells = len(self.cells)
-        txt += f"\ncells: {number_of_cells}"
         return txt
 
     def _repr_html_(self):
         header = f"""
-         <p>
-            <h3>CellpyData-object</h3>
+            <h2>CellpyCell-object</h2>
             <b>id</b>: {hex(id(self))} <br>
-            <b>name</b>: {self.name} <br>
+            <b>name</b>: {self.session_name} <br>
             <b>table names</b>: {self.table_names} <br>
             <b>tester</b>: {self.tester} <br>
-            <b>cells</b>: {len(self.cells)} <br>
             <b>cycle_mode</b>: {self.cycle_mode} <br>
             <b>sep</b>: {self.sep} <br>
-            <b>daniel_number</b>: {self.daniel_number} <br>
             <b>cellpy_datadir</b>: {self.cellpy_datadir} <br>
             <b>raw_datadir</b>: {self.raw_datadir} <br>
-         </p>
         """
         all_vars = "<p>"
         all_vars += f"""
             <b>capacity_modifiers</b>: {self.capacity_modifiers} <br>
             <b>empty</b>: {self.empty} <br>
             <b>ensure_step_table</b>: {self.ensure_step_table} <br>
             <b>filestatuschecker</b>: {self.filestatuschecker} <br>
             <b>force_step_table_creation</b>: {self.force_step_table_creation} <br>
             <b>forced_errors</b>: {self.forced_errors} <br>
             <b>limit_loaded_cycles</b>: {self.limit_loaded_cycles} <br>
-            <b>load_only_summary</b>: {self.load_only_summary} <br>
             <b>profile</b>: {self.profile} <br>
-            <b>raw_limits</b>: {self.raw_limits} <br>
-            <b>raw_units</b>: {self.raw_units} <br>
+            <b>cellpy_units</b>: {self.cellpy_units} <br>
             <b>select_minimal</b>: {self.select_minimal} <br>
-            <b>selected_cell_number</b>: {self.selected_cell_number} <br>
             <b>selected_scans</b>: {self.selected_scans} <br>
-            <b>status_datasets</b>: {self.status_datasets} <br>
             <b>summary_exists (deprecated)</b>: {self.summary_exists} <br>
-
-
         """
         all_vars += "</p>"
 
         cell_txt = ""
-        for i, cell in enumerate(self.cells):
-            cell_txt += f"<h4>cell {i + 1} of {len(self.cells)}</h4>"
-            cell_txt += cell._repr_html_()
-
+        cell_txt += f"<h3>data</h3>"
+        cell_txt += f"<blockquote>{self.data._repr_html_()}</blockquote>"
         return header + all_vars + cell_txt
 
     def __str__(self):
-        txt = "<CellpyData>\n"
-        if self.name:
-            txt += f"name: {self.name}\n"
+        txt = "<CellpyCell>\n"
+        if self.session_name:
+            txt += f"session name: {self.session_name}\n"
         if self.table_names:
-            txt += f"table_names: {self.table_names}\n"
+            txt += f"table names: {self.table_names}\n"
         if self.tester:
             txt += f"tester: {self.tester}\n"
-        if self.cells:
-            txt += "datasets: [ ->\n"
-            for i, d in enumerate(self.cells):
-                txt += f"   ({i})\n"
-                for t in str(d).split("\n"):
-                    txt += "     "
-                    txt += t
-                    txt += "\n"
+        if self.data:
+            txt += "data:\n"
+            for t in str(self.data).split("\n"):
+                txt += "     "
+                txt += t
                 txt += "\n"
-            txt += "]"
+            txt += "\n"
         else:
-            txt += "datasets: []"
+            txt += "datasets: EMPTY"
         txt += "\n"
         return txt
 
     def __bool__(self):
-        if self.cells:
+        if self.data:
             return True
         else:
             return False
 
     def __len__(self):
-        return len(self.cells)
+        if self.data:
+            return 1
+        else:
+            return 0
 
     def __init__(
         self,
         filenames=None,
         selected_scans=None,
         profile=False,
         filestatuschecker=None,  # "modified"
         tester=None,
         initialize=False,
+        cellpy_units=None,
+        output_units=None,
     ):
-        """CellpyData object
+        """CellpyCell object
 
         Args:
             filenames: list of files to load.
             selected_scans:
             profile: experimental feature.
             filestatuschecker: property to compare cellpy and raw-files;
                default read from prms-file.
             tester: instrument used (e.g. "arbin_res") (checks prms-file as
                default).
             initialize: create a dummy (empty) dataset; defaults to False.
+            cellpy_units (dict): sent to cellpy.parameters.internal_settings.get_cellpy_units
+            output_units (dict): sent to cellpy.parameters.internal_settings.get_default_output_units
         """
+
+        # TODO v 1.1: move to data (allow for multiple testers for same cell)
         if tester is None:
             self.tester = prms.Instruments.tester
+            logging.debug(f"reading instrument from prms: {prms.Instruments}")
         else:
             self.tester = tester
+
         self.loader = None  # this will be set in the function set_instrument
-        self.logger = logging.getLogger(__name__)
-        logging.debug("created CellpyData instance")
-        self.name = None
+        logging.debug("created CellpyCell instance")
+
+        self._session_name = None
         self.profile = profile
+
         self.minimum_selection = {}
-        if filestatuschecker is None:
-            self.filestatuschecker = prms.Reader.filestatuschecker
-        else:
-            self.filestatuschecker = filestatuschecker
+        self.filestatuschecker = filestatuschecker or prms.Reader.filestatuschecker
         self.forced_errors = 0
         self.summary_exists = False
 
-        if not filenames:
-            self.file_names = []
-        else:
-            self.file_names = filenames
-            if not self._is_listtype(self.file_names):
-                self.file_names = [self.file_names]
-        if not selected_scans:
-            self.selected_scans = []
-        else:
-            self.selected_scans = selected_scans
-            if not self._is_listtype(self.selected_scans):
-                self.selected_scans = [self.selected_scans]
-
-        self.cells = []
-        self.status_datasets = []
-        self.selected_cell_number = 0
-        self.number_of_datasets = 0
+        self.file_names = filenames or []
+        if not self._is_listtype(self.file_names):
+            self.file_names = [self.file_names]
+
+        self.selected_scans = selected_scans or []
+        if not self._is_listtype(self.selected_scans):
+            self.selected_scans = [self.selected_scans]
+
+        self._data = None
         self.overwrite_able = True  # attribute that prevents saving to the same filename as loaded from if False
 
         self.capacity_modifiers = ["reset"]
 
         self.list_of_step_types = [
             "charge",
             "discharge",
@@ -263,167 +259,231 @@
             "not_known",
         ]
         # - options
         self.force_step_table_creation = prms.Reader.force_step_table_creation
         self.force_all = prms.Reader.force_all
         self.sep = prms.Reader.sep
         self._cycle_mode = None
-        # self.max_res_filesize = prms.Reader.max_res_filesize
-        self.load_only_summary = prms.Reader.load_only_summary
         self.select_minimal = prms.Reader.select_minimal
-        # self.chunk_size = prms.Reader.chunk_size  # 100000
-        # self.max_chunks = prms.Reader.max_chunks
-        # self.last_chunk = prms.Reader.last_chunk
         self.limit_loaded_cycles = prms.Reader.limit_loaded_cycles
         self.limit_data_points = None
-        # self.load_until_error = prms.Reader.load_until_error
         self.ensure_step_table = prms.Reader.ensure_step_table
-        self.daniel_number = prms.Reader.daniel_number
-        # self.raw_datadir = prms.Reader.raw_datadir
-        self.raw_datadir = prms.Paths.rawdatadir
-        # self.cellpy_datadir = prms.Reader.cellpy_datadir
-        self.cellpy_datadir = prms.Paths.cellpydatadir
-        # search in prm-file for res and hdf5 dirs in loadcell:
+        self.ensure_summary_table = prms.Reader.ensure_summary_table
+        self.raw_datadir = OtherPath(prms.Paths.rawdatadir)
+        self.cellpy_datadir = OtherPath(prms.Paths.cellpydatadir)
         self.auto_dirs = prms.Reader.auto_dirs
 
         # - headers and instruments
         self.headers_normal = headers_normal
         self.headers_summary = headers_summary
         self.headers_step_table = headers_step_table
 
         self.table_names = None  # dictionary defined in set_instruments
         self.instrument_factory = None
         self.register_instrument_readers()
         self.set_instrument()
-
         # - units used by cellpy
-        self.cellpy_units = get_cellpy_units()
-        self.output_units = get_default_output_units()
+        self.cellpy_units = get_cellpy_units(cellpy_units)
+        self.output_units = get_default_output_units(output_units)
 
         if initialize:
             self.initialize()
 
     def initialize(self):
+        """Initialize the cellpycell object."""
+
         logging.debug("Initializing...")
-        self.cells.append(Cell())
+        # TODO: v.1.0.0: replace this
+        self._data = Data()
 
+    # the batch utility might be using session name
+    # the cycle and ica collector are using session name
+    # improvement suggestion: use data.cell_name instead
     @property
-    def raw_units(self):
-        return self.cell.raw_units
+    def session_name(self):
+        """returns the session name"""
+
+        if not self._session_name:
+            return self.data.cell_name
+        else:
+            return self._session_name
+
+    @session_name.setter
+    def session_name(self, n):
+        """sets the session name"""
+
+        self._session_name = n
+        if not self.data.cell_name:
+            self.data.cell_name = n
+
+    def _invent_a_session_name(self, filename=None, override=False):
+        if filename is None:
+            self.session_name = "nameless"
+            return
+        if self.session_name and not override:
+            return
+        path = Path(filename)
+        self.session_name = path.with_suffix("").name
 
     @property
-    def cell(self):
-        """returns the DataSet instance"""
-        try:
-            cell = self.cells[self.selected_cell_number]
-        except IndexError as e:
-            logging.critical("Sorry, I don't have any cells to give you!")
-            raise NoCellFound from e
-        return cell
-
-    @cell.setter
-    def cell(self, new_cell):
-        self.cells[self.selected_cell_number] = new_cell
+    def raw_units(self):
+        """returns the raw_units dictionary"""
+
+        return self.data.raw_units
 
     @property
-    def dataset(self):
+    def data(self):
         """returns the DataSet instance"""
-        # could insert a try-except thingy here...
-        warnings.warn(
-            "The .dataset property is deprecated, please use .cell instead.",
-            DeprecationWarning,
-        )
-        cell = self.cells[self.selected_cell_number]
-        return cell
+
+        if not self._data:
+            logging.critical("Sorry, I don't have any data to give you!")
+            logging.debug(
+                "NoDataFound - might consider defaulting to create one in the future"
+            )
+            raise NoDataFound
+        else:
+            return self._data
+
+    @data.setter
+    def data(self, new_cell):
+        """sets the DataSet instance"""
+
+        self._data = new_cell
 
     @property
     def empty(self):
-        """gives True if the CellpyData object is empty (or un-functional)"""
-        return not self.check()
+        """Gives True if the CellpyCell object is empty (or non-functional)"""
+
+        return not self._validate_cell()
 
     @classmethod
     def vacant(cls, cell=None):
-        """Create a CellpyData instance.
+        """Create a CellpyCell instance.
+
         Args:
-            cell (CellpyData instance): the attributes from the cell will be copied
-                to the new Cellpydata instance.
+            cell (CellpyCell instance): the attributes from the data will be
+                copied to the new Cellpydata instance.
 
          Returns:
-            CellpyData instance.
+            CellpyCell instance.
+
         """
 
         new_cell = cls(initialize=True)
         if cell is not None:
-            for attr in ATTRS_DATASET:
-                value = getattr(cell.cell, attr)
-                setattr(new_cell.cell, attr, value)
-
-            for attr in ATTRS_DATASET_DEEP:
-                value = getattr(cell.cell, attr)
-                setattr(new_cell.cell, attr, copy.deepcopy(value))
-
-            for attr in ATTRS_CELLPYDATA:
-                value = getattr(cell, attr)
-                setattr(new_cell, attr, value)
+            new_cell.data.meta_common = cell.data.meta_common
+            new_cell.data.meta_test_dependent = cell.data.meta_test_dependent
 
+            new_cell.data.raw_data_files = cell.data.raw_data_files
+            new_cell.data.raw_data_files_length = cell.data.raw_data_files_length
+            new_cell.data.raw_units = cell.data.raw_units
+            new_cell.data.raw_limits = cell.data.raw_limits
+
+            new_cell.data.loaded_from = cell.data.loaded_from
+            new_cell.data._raw_id = cell.data.raw_id
         return new_cell
 
+    def mod_raw_split_cycle(self, data_points: List) -> None:
+        """Split cycle(s) into several cycles.
+
+        Args:
+            data_points: list of the first data point(s) for additional cycle(s).
+
+        """
+
+        logging.info(f"splitting cycles at {data_points}")
+        for data_point in data_points:
+            self._mod_raw_split_cycle(data_point)
+        logging.warning(
+            f"splitting cycles at {data_points} -re-run make_step_table and make_summary to propagate change!"
+        )
+
+    def _mod_raw_split_cycle(self, data_point: int) -> None:
+        r = self.data.raw
+
+        hdr_data_point = self.headers_normal.data_point_txt
+        hdr_cycle = self.headers_normal.cycle_index_txt
+        hdr_c_cap = self.headers_normal.charge_capacity_txt
+        hdr_d_cap = self.headers_normal.discharge_capacity_txt
+        hdr_c_energy = self.headers_normal.charge_energy_txt
+        hdr_d_energy = self.headers_normal.discharge_energy_txt
+
+        # modifying cycle numbers
+        c_mask = r[hdr_data_point] >= data_point
+        r.loc[c_mask, hdr_cycle] = r.loc[c_mask, hdr_cycle] + 1
+
+        # resetting capacities
+        initial_values = r.loc[r[hdr_data_point] == data_point - 1, :]
+        cycle = r.loc[r[hdr_data_point] == data_point, hdr_cycle].values[0]
+
+        c_cap, d_cap, c_energy, d_energy = initial_values[
+            [hdr_c_cap, hdr_d_cap, hdr_c_energy, hdr_d_energy]
+        ].values[0]
+        cycle_mask = r[hdr_cycle] == cycle
+        r.loc[cycle_mask, hdr_c_cap] = r.loc[cycle_mask, hdr_c_cap] - c_cap
+        r.loc[cycle_mask, hdr_d_cap] = r.loc[cycle_mask, hdr_d_cap] - d_cap
+        r.loc[cycle_mask, hdr_c_energy] = r.loc[cycle_mask, hdr_c_energy] - c_energy
+        r.loc[cycle_mask, hdr_d_energy] = r.loc[cycle_mask, hdr_d_energy] - d_energy
+
     def split(self, cycle=None):
-        """Split experiment (CellpyData object) into two sub-experiments. if cycle
+        """Split experiment (CellpyCell object) into two sub-experiments. if cycle
         is not give, it will split on the median cycle number"""
 
         if isinstance(cycle, int) or cycle is None:
             return self.split_many(base_cycles=cycle)
 
     def drop_from(self, cycle=None):
-        """Select first part of experiment (CellpyData object) up to cycle number
+        """Select first part of experiment (CellpyCell object) up to cycle number
         'cycle'"""
+
         if isinstance(cycle, int):
             c1, c2 = self.split_many(base_cycles=cycle)
             return c1
 
     def drop_to(self, cycle=None):
-        """Select last part of experiment (CellpyData object) from cycle number
+        """Select last part of experiment (CellpyCell object) from cycle number
         'cycle'"""
+
         if isinstance(cycle, int):
             c1, c2 = self.split_many(base_cycles=cycle)
             return c2
 
     def drop_edges(self, start, end):
-        """Select middle part of experiment (CellpyData object) from cycle
-        number 'start' to 'end"""
+        """Select middle part of experiment (CellpyCell object) from cycle
+        number 'start' to 'end'"""
 
         if end < start:
             raise ValueError("end cannot be larger than start")
         if end == start:
             raise ValueError("end cannot be the same as start")
         return self.split_many([start, end])[1]
 
     def split_many(self, base_cycles=None):
-        """Split experiment (CellpyData object) into several sub-experiments.
+        """Split experiment (CellpyCell object) into several sub-experiments.
 
         Args:
             base_cycles (int or list of ints): cycle(s) to do the split on.
 
         Returns:
-            List of CellpyData objects
+            List of CellpyCell objects
+
         """
         h_summary_index = HEADERS_SUMMARY.cycle_index
         h_raw_index = HEADERS_NORMAL.cycle_index_txt
         h_step_cycle = HEADERS_STEP_TABLE.cycle
 
         if base_cycles is None:
             all_cycles = self.get_cycle_numbers()
             base_cycles = int(np.median(all_cycles))
 
         cells = list()
         if not isinstance(base_cycles, (list, tuple)):
             base_cycles = [base_cycles]
 
-        dataset = self.cell
+        dataset = self.data
         steptable = dataset.steps
         data = dataset.raw
         summary = dataset.summary
 
         # In case Cycle_Index has been promoted to index [#index]
         if h_summary_index not in summary.columns:
             summary = summary.reset_index(drop=False)
@@ -438,28 +498,28 @@
                 data[data[h_raw_index] >= b_cycle],
             ]
             summary0, summary = [
                 summary[summary[h_summary_index] < b_cycle],
                 summary[summary[h_summary_index] >= b_cycle],
             ]
 
-            new_cell = CellpyData.vacant(cell=self)
-            old_cell = CellpyData.vacant(cell=self)
+            new_cell = CellpyCell.vacant(cell=self)
+            old_cell = CellpyCell.vacant(cell=self)
 
             summary0 = summary0.set_index(h_summary_index)
 
-            new_cell.cell.steps = steptable0
-            new_cell.cell.raw = data0
-            new_cell.cell.summary = summary0
-            new_cell.cell = identify_last_data_point(new_cell.cell)
-
-            old_cell.cell.steps = steptable
-            old_cell.cell.raw = data
-            old_cell.cell.summary = summary
-            old_cell.cell = identify_last_data_point(old_cell.cell)
+            new_cell.data.steps = steptable0
+            new_cell.data.raw = data0
+            new_cell.data.summary = summary0
+            new_cell.data = identify_last_data_point(new_cell.data)
+
+            old_cell.data.steps = steptable
+            old_cell.data.raw = data
+            old_cell.data.summary = summary
+            old_cell.data = identify_last_data_point(old_cell.data)
 
             cells.append(new_cell)
 
         cells.append(old_cell)
         return cells
 
     def __register_external_readers(self):
@@ -467,14 +527,16 @@
             "Not implemented yet. Should allow registering readers "
             "for example installed as plug-ins."
         )
         self.__external_readers = dict()
         return
 
     def register_instrument_readers(self):
+        """Register instrument readers."""
+
         self.instrument_factory = generate_default_factory()
         # instruments = find_all_instruments()
         # for instrument_id, instrument in instruments.items():
         #     self.instrument_factory.register_builder(instrument_id, instrument)
 
     def _set_raw_units(self):
         raw_units = get_default_raw_units()
@@ -487,15 +549,15 @@
         return raw_units
 
     def _set_instrument(self, instrument, **kwargs):
         logging.debug(f"Setting new instrument: {instrument}")
         self.loader_class = self.instrument_factory.create(instrument, **kwargs)
         self.raw_limits = self.loader_class.get_raw_limits()
         # ----- create the loader ------------------------
-        self.loader = self.loader_class.loader
+        self.loader = self.loader_class.loader_executor
 
     def set_instrument(
         self,
         instrument=None,
         model=None,
         instrument_file=None,
         **kwargs,
@@ -526,23 +588,24 @@
                 header lines, different encoding).
             instrument_file: (path) instrument definition file,
             kwargs (dict): key-word arguments sent to the initializer of the
                 loader class
 
         Notes:
             If you are using a local instrument loader, you will have to register it first to the loader factory.
-            >>> c = CellpyData()  # this will automatically register the already implemented loaders
+
+            >>> c = CellpyCell()  # this will automatically register the already implemented loaders
             >>> c.instrument_factory.register_builder(instrument_id, (module_name, path_to_instrument_loader_file))
 
             It is highly recommended using the module_name as the instrument_id.
+
         """
 
         # constants:
         custom_instrument_splitter = "::"
-
         # consume keyword arguments:
         _override_local_instrument_path = kwargs.pop(
             "_override_local_instrument_path", False
         )
 
         # parse input (need instrument, instrument_file and model)
         if instrument is None and instrument_file is None:
@@ -581,49 +644,45 @@
 
         _instrument = instrument.split(custom_instrument_splitter)
         if len(_instrument) < 2:
             return instrument, None
 
         return _instrument
 
-    def _create_logger(self):
-        from cellpy import log
-
-        self.logger = logging.getLogger(__name__)
-        log.setup_logging(default_level="DEBUG")
-
     @property
     def cycle_mode(self):
+        # TODO: edit this from scalar to list
         try:
-            cell = self.cell
-            return cell.cycle_mode
-        except NoCellFound:
+            data = self.data
+            return data.meta_test_dependent.cycle_mode[0]
+        except NoDataFound:
             return self._cycle_mode
 
     @cycle_mode.setter
     def cycle_mode(self, cycle_mode):
+        # TODO: edit this from scalar to list
         logging.debug(f"-> cycle_mode: {cycle_mode}")
         try:
-            cell = self.cell
-            cell.cycle_mode = cycle_mode
+            data = self.data
+            data.meta_test_dependent.cycle_mode = [cycle_mode]
             self._cycle_mode = cycle_mode
-        except NoCellFound:
+        except NoDataFound:
             self._cycle_mode = cycle_mode
 
     def set_raw_datadir(self, directory=None):
         """Set the directory containing .res-files.
 
         Used for setting directory for looking for res-files.@
         A valid directory name is required.
 
         Args:
             directory (str): path to res-directory
 
         Example:
-            >>> d = CellpyData()
+            >>> d = CellpyCell()
             >>> directory = "MyData/Arbindata"
             >>> d.set_raw_datadir(directory)
 
         """
 
         if directory is None:
             logging.info("No directory name given")
@@ -640,15 +699,15 @@
         Used for setting directory for looking for hdf5-files.
         A valid directory name is required.
 
         Args:
             directory (str): path to hdf5-directory
 
         Example:
-            >>> d = CellpyData()
+            >>> d = CellpyCell()
             >>> directory = "MyData/HDF5"
             >>> d.set_raw_datadir(directory)
 
         """
 
         if directory is None:
             logging.info("No directory name given")
@@ -658,27 +717,25 @@
             return
         self.cellpy_datadir = directory
 
     def check_file_ids(self, rawfiles, cellpyfile, detailed=False):
         """Check the stats for the files (raw-data and cellpy hdf5).
 
         This function checks if the hdf5 file and the res-files have the same
-        timestamps etc to find out if we need to bother to load .res -files.
+        timestamps etc. to find out if we need to bother to load .res -files.
 
         Args:
             cellpyfile (str): filename of the cellpy hdf5-file.
             rawfiles (list of str): name(s) of raw-data file(s).
-            detailed (bool): return a dict containing True or False for each
-                individual raw-file
+            detailed (bool): return a dict containing True or False for each individual raw-file.
 
         Returns:
             If detailed is False:
                 False if the raw files are newer than the cellpy hdf5-file
-                    (update needed).
-                True if update is not needed.
+                (update needed). True if update is not needed.
              If detailed is True it returns a dict containing True or False for each
                 individual raw-file.
         """
 
         txt = f"Checking file ids - using '{self.filestatuschecker}'"
         logging.info(txt)
 
@@ -720,38 +777,48 @@
             # logging.debug(fid)
             if fid.name is None:
                 warnings.warn(f"file does not exist: {f}")
                 if abort_on_missing:
                     sys.exit(-1)
             else:
                 if strip_file_names:
-                    name = os.path.basename(f)
+                    name = f.name
                 else:
                     name = f
                 if check_on == "size":
                     ids[name] = int(fid.size)
                 elif check_on == "modified":
                     ids[name] = int(fid.last_modified)
                 else:
-                    ids[name] = int(fid.last_accessed)
+                    ids[name] = int(fid.last_modified)
         return ids
 
-    def _check_cellpy_file(self, filename):
+    def _check_cellpy_file(self, filename: OtherPath):
         """Get the file-ids for the cellpy_file."""
 
+        if not isinstance(filename, OtherPath):
+            logging.debug("filename must be an OtherPath object")
+            filename = OtherPath(filename)
+
         use_full_filename_path = False
-        parent_level = prms._cellpyfile_root
-        fid_dir = prms._cellpyfile_fid
+        parent_level = prms._cellpyfile_root  # noqa
+        fid_dir = prms._cellpyfile_fid  # noqa
         check_on = self.filestatuschecker
         logging.debug("checking cellpy-file")
         logging.debug(filename)
-        if not os.path.isfile(filename):
+        if not filename.is_file():
             logging.debug("cellpy-file does not exist")
             return None
         try:
+            # TODO: implement external handling of hdf5-files
+            if filename.is_external:
+                # I have not implemented any external handling of hdf5-files yet. So we need to
+                # copy the file to temporary directory (this will take some time, and therefore it is
+                # probably best not to put your cellpy files in a remote directory yet):
+                filename = filename.copy()
             store = pd.HDFStore(filename)
         except Exception as e:
             logging.debug(f"could not open cellpy-file ({e})")
             return None
         fidtable = None
         try:
             fidtable = store.select(parent_level + fid_dir)
@@ -782,15 +849,15 @@
                     name = full_name
 
                 if check_on == "size":
                     ids[name] = int(fid.size)
                 elif check_on == "modified":
                     ids[name] = int(fid.last_modified)
                 else:
-                    ids[name] = int(fid.last_accessed)
+                    ids[name] = int(fid.last_modified)
             return ids
         else:
             return None
 
     @staticmethod
     def _compare_ids(ids_raw, ids_cellpy_file):
         similar = True
@@ -827,55 +894,58 @@
         return similar
 
     def loadcell(
         self,
         raw_files,
         cellpy_file=None,
         mass=None,
-        summary_on_raw=False,
-        summary_ir=True,
-        summary_ocv=False,
-        summary_end_v=True,
-        only_summary=False,
+        summary_on_raw=True,
+        summary_on_cellpy_file=True,
+        find_ir=True,
+        find_end_voltage=True,
         force_raw=False,
         use_cellpy_stat_file=None,
         cell_type=None,
+        loading=None,
+        area=None,
+        estimate_area=True,
         selector=None,
         **kwargs,
     ):
-
         """Loads data for given cells.
 
         Args:
             raw_files (list): name of res-files
             cellpy_file (path): name of cellpy-file
             mass (float): mass of electrode or active material
-            summary_on_raw (bool): use raw-file for summary
-            summary_ir (bool): summarize ir
-            summary_ocv (bool): summarize ocv steps
-            summary_end_v (bool): summarize end voltage
-            only_summary (bool): get only the summary of the runs
+            summary_on_raw (bool): calculate summary if loading from raw
+            summary_on_cellpy_file (bool): calculate summary if loading from cellpy-file.
+            find_ir (bool): summarize ir
+            find_end_voltage (bool): summarize end voltage
             force_raw (bool): only use raw-files
             use_cellpy_stat_file (bool): use stat file if creating summary
                 from raw
-            cell_type (str): set the cell type (e.g. "anode"). If not, the default from
+            cell_type (str): set the data type (e.g. "anode"). If not, the default from
                the config file is used.
+            loading (float): loading in units [mass] / [area], used to calculate area if area not given
+            area (float): area of active electrode
+            estimate_area (bool): calculate area from loading if given (defaults to True).
             selector (dict): passed to load.
             **kwargs: passed to from_raw
 
         Example:
 
             >>> srnos = my_dbreader.select_batch("testing_new_solvent")
             >>> cell_datas = []
             >>> for srno in srnos:
             >>> ... my_run_name = my_dbreader.get_cell_name(srno)
             >>> ... mass = my_dbreader.get_mass(srno)
             >>> ... rawfiles, cellpyfiles = \
             >>> ...     filefinder.search_for_files(my_run_name)
-            >>> ... cell_data = cellreader.CellpyData()
+            >>> ... cell_data = cellreader.CellpyCell()
             >>> ... cell_data.loadcell(raw_files=rawfiles,
             >>> ...                    cellpy_file=cellpyfiles)
             >>> ... cell_data.set_mass(mass)
             >>> ... if not cell_data.summary_exists:
             >>> ...     cell_data.make_summary() # etc. etc.
             >>> ... cell_datas.append(cell_data)
             >>>
@@ -883,315 +953,123 @@
         # This is a part of a dramatic API change. It will not be possible to
         # load more than one set of datasets (i.e. one single cellpy-file or
         # several raw-files that will be automatically merged)
 
         # TODO @jepe Make setting or prm so that it is possible to update only new data
         # TODO @jepe Allow passing handle to progress-bar or update a global progressbar
 
-        logging.info("Started cellpy.cellreader.loadcell")
+        warnings.warn(
+            DeprecationWarning("loadcell is deprecated. Use cellpy.get instead.")
+        )
+        logging.debug("Started cellpy.cellreader.loadcell ")
+
         if cellpy_file is None:
             similar = False
         elif force_raw:
             similar = False
         else:
             similar = self.check_file_ids(raw_files, cellpy_file)
-        logging.debug("checked if the files were similar")
+            logging.debug(f"checked if the files were similar")
+        logging.debug(f"similar: {similar}")
 
-        if only_summary:
-            self.load_only_summary = True
-        else:
-            self.load_only_summary = False
+        if similar:
+            logging.debug(f"loading cellpy-file: {cellpy_file}")
+            self.load(cellpy_file, selector=selector)
 
-        if not similar:
+        else:
             logging.debug("cellpy file(s) needs updating - loading raw")
             logging.info("Loading raw-file")
             logging.debug(raw_files)
-            self.from_raw(raw_files, **kwargs)
-            if cell_type is not None:
-                self.cycle_mode = cell_type
-                logging.debug(f"setting cycle mode: {cell_type}")
-            logging.debug("loaded files")
-            # Check if the run was loaded ([] if empty)
-            if self.status_datasets:
-                if mass:
-                    self.set_mass(mass)
-                if summary_on_raw:
-                    nom_cap = kwargs.pop("nom_cap", None)
-                    if nom_cap is not None:
-                        self.set_nom_cap(nom_cap)
-                    self.make_summary(
-                        all_tests=False,
-                        find_ocv=summary_ocv,
-                        find_ir=summary_ir,
-                        find_end_voltage=summary_end_v,
-                        use_cellpy_stat_file=use_cellpy_stat_file,
-                        # nom_cap=nom_cap,
-                    )
-            else:
-                logging.warning("Empty run!")
-
-        else:
-            self.load(cellpy_file, selector=selector)
-            nom_cap = kwargs.pop("nom_cap", None)
-            if nom_cap is not None:
-                self.set_nom_cap(nom_cap)
-            if mass:
-                self.set_mass(mass)
-
-        return self
-
-    def dev_update_loadcell(
-        self,
-        raw_files,
-        cellpy_file=None,
-        mass=None,
-        summary_on_raw=False,
-        summary_ir=True,
-        summary_ocv=False,
-        summary_end_v=True,
-        force_raw=False,
-        use_cellpy_stat_file=None,
-        nom_cap=None,
-        selector=None,
-        **kwargs,
-    ):
 
-        logging.info("Started cellpy.cellreader.loadcell")
-
-        if cellpy_file is None or force_raw:
-            similar = None
-        else:
-            similar = self.check_file_ids(raw_files, cellpy_file, detailed=True)
+            self.from_raw(raw_files, **kwargs)
 
-        logging.debug("checked if the files were similar")
+        logging.debug("loaded files")
 
-        if similar is None:
-            # forcing to load only raw_files
-            self.from_raw(raw_files, **kwargs)
-            if self.status_datasets:
-                if mass:
-                    self.set_mass(mass)
-                if summary_on_raw:
-                    self.make_summary(
-                        all_tests=False,
-                        find_ocv=summary_ocv,
-                        find_ir=summary_ir,
-                        find_end_voltage=summary_end_v,
-                        use_cellpy_stat_file=use_cellpy_stat_file,
-                        nom_cap=nom_cap,
-                    )
-            else:
-                logging.warning("Empty run!")
+        if not self._validate_cell():
+            logging.warning("Empty run!")
             return self
 
-        self.load(cellpy_file, selector=selector)
-        if mass:
-            self.set_mass(mass)
-
-        if all(similar.values()):
-            logging.info("Everything is up to date")
-            return
+        logging.debug("setting cell_type")
+        if cell_type is not None:
+            self.cycle_mode = cell_type
+            logging.debug(f"setting cycle mode: {cell_type}")
 
-        start_file = True
-        for i, f in enumerate(raw_files):
-            f = Path(f)
-            if not similar[f.name] and start_file:
-                try:
-                    last_data_point = self.cell.raw_data_files[i].last_data_point
-                except IndexError:
-                    last_data_point = 0
-
-                self.dev_update_from_raw(
-                    file_names=f, data_points=[last_data_point, None]
-                )
-                self.cell = self.dev_update_merge()
+        logging.debug("setting mass")
+        if mass is not None:
+            self.set_mass(mass)
 
-            elif not similar[f.name]:
-                try:
-                    last_data_point = self.cell.raw_data_files[i].last_data_point
-                except IndexError:
-                    last_data_point = 0
+        logging.debug("setting nom_cap")
+        nom_cap = kwargs.pop("nom_cap", None)
+        if nom_cap is not None:
+            self.set_nom_cap(nom_cap)
+
+        logging.debug("calculating area")
+        if area is not None:
+            logging.debug(f"got area: {area}")
+            self.data.meta_common.active_electrode_area = area
+        elif loading and estimate_area:
+            logging.debug(f"got loading: {logging}")
+            area = self.data.mass / loading
+            logging.debug(
+                f"calculating area from loading ({loading}) and mass ({self.data.mass}): {area}"
+            )
+            self.data.meta_common.active_electrode_area = area
+        else:
+            logging.debug("using default area")
 
-                self.dev_update_from_raw(
-                    file_names=f, data_points=[last_data_point, None]
+        if similar:
+            if summary_on_cellpy_file:
+                self.make_summary(
+                    find_ir=find_ir,
+                    find_end_voltage=find_end_voltage,
+                    use_cellpy_stat_file=use_cellpy_stat_file,
                 )
-                self.merge()
-
-            start_file = False
-
-        self.dev_update_make_steps()
-        self.dev_update_make_summary(
-            all_tests=False,
-            find_ocv=summary_ocv,
-            find_ir=summary_ir,
-            find_end_voltage=summary_end_v,
-            use_cellpy_stat_file=use_cellpy_stat_file,
-        )
-        return self
-
-    def dev_update(self, file_names=None, **kwargs):
-        print("NOT FINISHED YET - but close")
-        if len(self.cell.raw_data_files) != 1:
-            logging.warning("Merged cell. But can only update based on the last file")
-            print(self.cell.raw_data_files)
-            for fid in self.cell.raw_data_files:
-                print(fid)
-        last = self.cell.raw_data_files[0].last_data_point
-
-        self.dev_update_from_raw(
-            file_names=file_names, data_points=[last, None], **kwargs
-        )
-        print("lets try to merge")
-        self.cell = self.dev_update_merge()
-        print("now it is time to update the step table")
-        self.dev_update_make_steps()
-        print("and finally, lets update the summary")
-        self.dev_update_make_summary()
-
-    def dev_update_merge(self):
-        print("NOT FINISHED YET - but very close")
-        number_of_tests = len(self.cells)
-        if number_of_tests != 2:
-            logging.warning("Cannot merge if you do not have exactly two cell-objects")
-            return
-        t1, t2 = self.cells
-
-        if t1.raw.empty:
-            logging.debug("OBS! the first dataset is empty")
-
-        if t2.raw.empty:
-            t1.merged = True
-            logging.debug("the second dataset was empty")
-            logging.debug(" -> merged contains only first")
-            return t1
-        test = t1
-
-        cycle_index_header = self.headers_normal.cycle_index_txt
 
-        if not t1.raw.empty:
-            t1.raw = t1.raw.iloc[:-1]
-            raw2 = pd.concat([t1.raw, t2.raw], ignore_index=True)
-            test.no_cycles = max(raw2[cycle_index_header])
-            test.raw = raw2
         else:
-            test.no_cycles = max(t2.raw[cycle_index_header])
-            test = t2
-        logging.debug(" -> merged with new dataset")
-
-        return test
-
-    def dev_update_make_steps(self, **kwargs):
-        old_steps = self.cell.steps.iloc[:-1]
-        # Note! hard-coding header name (might fail if changing default headers)
-        from_data_point = self.cell.steps.iloc[-1].point_first
-        new_steps = self.make_step_table(from_data_point=from_data_point, **kwargs)
-        merged_steps = pd.concat([old_steps, new_steps]).reset_index(drop=True)
-        self.cell.steps = merged_steps
-
-    def dev_update_make_summary(self, **kwargs):
-        print("NOT FINISHED YET - but not critical")
-        # Update not implemented yet, running full summary calculations for now.
-        # For later:
-        # old_summary = self.cell.summary.iloc[:-1]
-        cycle_index_header = self.headers_summary.cycle_index
-        from_cycle = self.cell.summary.iloc[-1][cycle_index_header]
-        self.make_summary(from_cycle=from_cycle, **kwargs)
-        # For later:
-        # (Remark! need to solve how to merge cumulated columns)
-        # new_summary = self.make_summary(from_cycle=from_cycle)
-        # merged_summary = pd.concat([old_summary, new_summary]).reset_index(drop=True)
-        # self.cell.summary = merged_summary
-
-    def dev_update_from_raw(self, file_names=None, data_points=None, **kwargs):
-        """This method is under development. Using this to develop updating files
-        with only new data.
-        """
-        print("NOT FINISHED YET - but very close")
-        # TODO @jepe: implement changes from original from_raw method introduced after this one was last edited.
-        if file_names:
-            self.file_names = file_names
-
-        if file_names is None:
-            logging.info(
-                "No filename given and no stored in the file_names "
-                "attribute. Returning None"
-            )
-            return None
-
-        if not isinstance(self.file_names, (list, tuple)):
-            self.file_names = [file_names]
-
-        raw_file_loader = self.loader
-
-        set_number = 0
-        cell = None
-
-        logging.debug("start iterating through file(s)")
-
-        for f in self.file_names:
-            logging.debug("loading raw file:")
-            logging.debug(f"{f}")
-
-            # get a list of cellpy.readers.core.Cell objects
-            cell = raw_file_loader(f, data_points=data_points, **kwargs)
-            # remark that the bounds are included (i.e. the first datapoint
-            # is 5000.
-
-            logging.debug("added the data set - merging file info")
-
-            # raw_data_file = copy.deepcopy(test[set_number].raw_data_files[0])
-            # file_size = test[set_number].raw_data_files_length[0]
-
-            # test[set_number].raw_data_files.append(raw_data_file)
-            # test[set_number].raw_data_files_length.append(file_size)
-            # return test
-        cell[set_number].raw_units = self._set_raw_units()
-        self.cells.append(cell[set_number])
+            if summary_on_raw:
+                self.make_summary(
+                    find_ir=find_ir,
+                    find_end_voltage=find_end_voltage,
+                    use_cellpy_stat_file=use_cellpy_stat_file,
+                )
 
-        self.number_of_datasets = len(self.cells)
-        self.status_datasets = self._validate_datasets()
-        self._invent_a_name()
         return self
 
     def from_raw(
         self,
         file_names=None,
         pre_processor_hook=None,
         post_processor_hook=None,
+        is_a_file=True,
         **kwargs,
     ):
         """Load a raw data-file.
 
         Args:
-            file_names (list of raw-file names): uses CellpyData.file_names if
+            file_names (list of raw-file names): uses CellpyCell.file_names if
                 None. If the list contains more than one file name, then the
                 runs will be merged together.
             pre_processor_hook (callable): function that will be applied to the data within the loader.
             post_processor_hook (callable): function that will be applied to the
                 cellpy.Dataset object after initial loading.
+            is_a_file (bool): performs an is_file check if set to True.
 
         Keyword Args for merging:
             recalc (bool): set to false if you don't want cellpy to automatically shift cycle number
                 and time (e.g. add last cycle number from previous file to the cycle numbers
                 in the next file).
 
         Other keywords depending on loader:
             [ArbinLoader]:
                 bad_steps (list of tuples): (c, s) tuples of steps s (in cycle c)
                     to skip loading.
-                dataset_number (int): the data set number to select if you are dealing
-                    with arbin files with more than one data-set.
                 data_points (tuple of ints): load only data from data_point[0] to
                     data_point[1] (use None for infinite). NOT IMPLEMENTED YET.
 
         """
-        # This function only loads one test at a time (but could contain several
-        # files). The function from_res() used to implement loading several
-        # datasets (using list of lists as input), however it is now deprecated.
-
         if file_names:
             self.file_names = file_names
 
         if not isinstance(self.file_names, (list, tuple)):
             self.file_names = [file_names]
 
         # file_type = self.tester
@@ -1204,186 +1082,105 @@
             self.set_instrument(instrument="custom", instrument_file=instrument_file)
         elif instrument:
             logging.info("Setting custom instrument")
             logging.info(f"-> {instrument}")
             self.set_instrument(instrument)
 
         raw_file_loader = self.loader
-        # test is currently a list of tests - this option will be removed in the future
-        # so set_number is hard-coded to 0, i.e. actual-test is always test[0]
-        set_number = 0
-        cells = None
-        counter = 0
+        try:
+            self.tester = self.loader_class.instrument_name
+        except AttributeError:
+            logging.debug(f"could not set instrument name")
+
+        # TODO: include this into prms (and config-file):
+        max_raw_files_to_merge = 20
+        if len(self.file_names) > max_raw_files_to_merge:
+            logging.debug("ERROR? Too many files to merge")
+            raise ValueError("Too many files to merge - " "could be a p2-p3 zip thing")
+
         logging.debug("start iterating through file(s)")
         recalc = kwargs.pop("recalc", True)
+        data = None
         for file_name in self.file_names:
             logging.debug("loading raw file:")
             logging.debug(f"{file_name}")
-            new_cells = raw_file_loader(
+            if is_a_file:
+                file_name = OtherPath(file_name)
+                if not file_name.is_file():
+                    raise NoDataFound(f"Could not find the file {file_name}")
+
+            new_data = raw_file_loader(
                 file_name, pre_processor_hook=pre_processor_hook, **kwargs
             )  # list of tests
+
+            if new_data is None:
+                raise IOError(
+                    f"Could not read {file_name}. Loader returned None. Aborting."
+                )
+            if not new_data.has_data:
+                raise IOError(f"Could not read any data from {file_name}. Aborting.")
+
             if post_processor_hook is not None:
                 # REMARK! this needs to be changed if we stop returning the datasets in a list
                 # (i.e. if we chose to remove option for having more than one test pr instance)
-                new_cells = [post_processor_hook(n) for n in new_cells]
+                new_data = post_processor_hook(new_data)
 
-            if new_cells:
+            if data is None:
                 # retrieving the first cell data (e.g. first file)
-                if cells is None:
-                    logging.debug("getting data from first file")
-                    if not new_cells[set_number].has_data:
-                        logging.debug("NO DATA")
-                    else:
-                        cells = new_cells
-
+                logging.debug("getting data from first file")
+                data = new_data
+            else:
                 # appending cell data file to existing
-                else:
-                    logging.debug("continuing reading files...")
-                    _cells = self._append(
-                        cells[set_number], new_cells[set_number], recalc=recalc
-                    )
+                logging.debug("continuing reading files...")
+                data = self._append(data, new_data, recalc=recalc)
 
-                    if not _cells:
-                        logging.warning(f"NO CELLS FOUND: {file_name}")
-                        continue
-
-                    cells[set_number] = _cells
-
-                    # retrieving file info in a for-loop in case of multiple files
-                    # Remark!
-                    #    - the raw_data_files attribute is a list
-                    #    - the raw_data_files_length attribute is a list
-                    # The reason for this choice is not clear anymore, but
-                    # let us keep it like this for now
-                    logging.debug("added the data set - merging file info")
-                    # TODO: include this into prms (and config-file):
-                    max_raw_files_to_merge = 20
-                    # TODO: legacy code - please fix me
-                    for j, raw_data_file in enumerate(
-                        new_cells[set_number].raw_data_files
-                    ):
-                        file_size = new_cells[set_number].raw_data_files_length[j]
-                        cells[set_number].raw_data_files.append(raw_data_file)
-                        cells[set_number].raw_data_files_length.append(file_size)
-                        counter += 1
-                        if counter > max_raw_files_to_merge:
-                            logging.debug("ERROR? Too many files to merge")
-                            raise ValueError(
-                                "Too many files to merge - "
-                                "could be a p2-p3 zip thing"
-                            )
+                # retrieving file info in a for-loop in case of multiple files
+                # Remark!
+                #    - the raw_data_files attribute is a list
+                #    - the raw_data_files_length attribute is a list
 
-            else:
-                logging.debug("NOTHING LOADED")
+                logging.debug("added the data set - merging file info")
+
+                data.raw_data_files.extend(new_data.raw_data_files)
+                data.raw_data_files_length.extend(new_data.raw_data_files_length)
 
         logging.debug("finished loading the raw-files")
 
-        test_exists = False
-        if cells:
-            if not cells[0].has_data:
-                logging.debug(
-                    "the first dataset (or only dataset) loaded from the raw data file is empty"
-                )
-            else:
-                test_exists = True
+        if not prms.Reader.sorted_data:
+            logging.debug("sorting data")
+            data = self._sort_data(data)
+            data.raw_units = self._set_raw_units()
 
-        if test_exists:
-            if not prms.Reader.sorted_data:
-                logging.debug("sorting data")
-                cells[set_number] = self._sort_data(cells[set_number])
-            # REMARK! If you want to allow for more than one cell pr instance, this needs to be replaced (for example using .extend)
-            cells[set_number].raw_units = self._set_raw_units()
-            self.cells.append(cells[set_number])
-        else:
-            logging.warning("No new datasets added!")
-        self.number_of_datasets = len(self.cells)
-        self.status_datasets = self._validate_datasets()
-        self._invent_a_name()
+        self.data = data
+        self._invent_a_session_name()  # TODO (v1.0.0): fix me
         return self
 
-    def from_res(self, filenames=None, check_file_type=True):
-        """Convenience function for loading arbin-type data into the
-        datastructure.
-
-        Args:
-            filenames: ((lists of) list of raw-file names): uses
-                cellpy.file_names if None.
-                If list-of-list, it loads each list into separate datasets.
-                The files in the inner list will be merged.
-            check_file_type (bool): check file type if True
-                (res-, or cellpy-format)
-        """
-        raise DeprecatedFeature
-
-    def _validate_datasets(self, level=0):
+    def _validate_cell(self, level=0):
         logging.debug("validating test")
-        level = 0
         # simple validation for finding empty datasets - should be expanded to
-        # find not-complete datasets, datasets with missing prms etc
-        v = []
+        # find not-complete datasets, datasets with missing parameters etc
+        v = True
         if level == 0:
-            for test in self.cells:
-                # check that it contains all the necessary headers
-                # (and add missing ones)
-                # test = self._clean_up_normal_table(test)
-                # check that the test is not empty
-                v.append(self._is_not_empty_dataset(test))
-            logging.debug(f"validation array: {v}")
+            try:
+                data = self.data
+                return True
+            except NoDataFound:
+                return False
         return v
 
-    def check(self):
-        """Returns False if no datasets exists or if one or more of the datasets
-        are empty"""
-
-        if len(self.status_datasets) == 0:
-            return False
-        if all(self.status_datasets):
-            return True
-        return False
-
-    # TODO: maybe consider being a bit more concice (re-implement)
-    def _is_not_empty_dataset(self, dataset):
-        if dataset is self._empty_dataset():
-            return False
-        else:
-            return True
-
-    # TODO: check if this is useful and if it is rename, if not delete
-    def _clean_up_normal_table(self, test=None, dataset_number=None):
-        # check that test contains all the necessary headers
-        # (and add missing ones)
-        raise NotImplementedError
-
-    # TODO: this is used for the check-datasetnr-thing. Will soon be obsolete?
-    def _report_empty_dataset(self):
-        logging.info("Empty set")
-
-    @staticmethod
-    def _empty_dataset():
-        return None
-
-    def _invent_a_name(self, filename=None, override=False):
-        if filename is None:
-            self.name = "nameless"
-            return
-        if self.name and not override:
-            return
-        path = Path(filename)
-        self.name = path.with_suffix("").name
-
     def partial_load(self, **kwargs):
         """Load only a selected part of the cellpy file."""
         raise NotImplementedError
 
     def link(self, **kwargs):
         """Create a link to a cellpy file.
 
         If the file is very big, it is sometimes better to work with the data
-        out of memory (i.e. on disk). A CellpyData object with a linked file
-        will in most cases work as a normal object. However, some of the methods
+        out of memory (i.e. on disk). A CellpyCell object with a linked file
+        will in most cases work as a normal object. However, some methods
         might be disabled. And it will be slower.
 
         Notes:
             2020.02.08 - maybe this functionality is not needed and can be replaced
                 by using dask or similar?
         """
         raise NotImplementedError
@@ -1391,131 +1188,105 @@
     def load(
         self,
         cellpy_file,
         parent_level=None,
         return_cls=True,
         accept_old=True,
         selector=None,
+        **kwargs,
     ):
         """Loads a cellpy file.
 
         Args:
-            cellpy_file (path, str): Full path to the cellpy file.
+            cellpy_file (OtherPath, str): Full path to the cellpy file.
             parent_level (str, optional): Parent level. Warning! Deprecating this soon!
             return_cls (bool): Return the class.
             accept_old (bool): Accept loading old cellpy-file versions.
                 Instead of raising WrongFileVersion it only issues a warning.
             selector (): under development
 
         Returns:
-            cellpy.CellPyData class if return_cls is True
+            cellpy.CellPyCellpy class if return_cls is True
         """
 
+        # This is what happens:
+        # 1) (this is not implemented yet, using only hdf5) chose what file format to load from
+        # 2) in reader (currently only _load_hdf5): check version and select sub-reader.
+        # 3) in sub-reader: read data
+        # 4) in this method: add data to CellpyCell object (i.e. self)
+        for kwarg in kwargs:
+            logging.debug(f"received (still) un-supported keyword argument {kwarg=}")
+
         try:
             logging.debug("loading cellpy-file (hdf5):")
             logging.debug(cellpy_file)
-
+            logging.debug(f"{type(cellpy_file)=}")
+            cellpy_file = OtherPath(cellpy_file)
             with pickle_protocol(PICKLE_PROTOCOL):
-                new_datasets = self._load_hdf5(
+                logging.debug(f"using pickle protocol {PICKLE_PROTOCOL}")
+                data = self._load_hdf5(
                     cellpy_file, parent_level, accept_old, selector=selector
                 )
             logging.debug("cellpy-file loaded")
 
         except AttributeError:
-            new_datasets = []
+            data = None
             logging.warning(
                 "This cellpy-file version is not supported by"
                 "current reader (try to update cellpy)."
             )
 
-        if new_datasets:
-            for dataset in new_datasets:
-                self.cells.append(dataset)
+        if data:
+            self.data = data
         else:
             # raise LoadError
             logging.warning("Could not load")
             logging.warning(str(cellpy_file))
 
-        self.number_of_datasets = len(self.cells)
-        self.status_datasets = self._validate_datasets()
-        self._invent_a_name(cellpy_file)
+        self._invent_a_session_name(cellpy_file)
         if return_cls:
             return self
 
-    def old_load(
-        self, cellpy_file, parent_level=None, return_cls=True, accept_old=False
-    ):
-        """Loads a cellpy file.
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
+    def _get_cellpy_file_version(self, filename, meta_dir=None, parent_level=None):
+        if meta_dir is None:
+            meta_dir = prms._cellpyfile_common_meta
 
-        Args:
-            cellpy_file (path, str): Full path to the cellpy file.
-            parent_level (str, optional): Parent level. Warning! Deprecating this soon!
-            return_cls (bool): Return the class.
-            accept_old (bool): Accept loading old cellpy-file versions.
-                Instead of raising WrongFileVersion it only issues a warning.
-
-        Returns:
-            cellpy.CellPyData class if return_cls is True
-        """
-
-        try:
-            logging.debug("loading cellpy-file (hdf5):")
-            logging.debug(cellpy_file)
-            with pickle_protocol(PICKLE_PROTOCOL):
-                new_datasets = self._load_hdf5(cellpy_file, parent_level, accept_old)
-            logging.debug("cellpy-file loaded")
-        except AttributeError:
-            new_datasets = []
-            logging.warning(
-                "This cellpy-file version is not supported by"
-                "current reader (try to update cellpy)."
-            )
-
-        if new_datasets:
-            for dataset in new_datasets:
-                self.cells.append(dataset)
-        else:
-            # raise LoadError
-            logging.warning("Could not load")
-            logging.warning(str(cellpy_file))
-
-        self.number_of_datasets = len(self.cells)
-        self.status_datasets = self._validate_datasets()
-        self._invent_a_name(cellpy_file)
-        if return_cls:
-            return self
-
-    def _get_cellpy_file_version(self, filename, meta_dir="/info", parent_level=None):
         if parent_level is None:
             parent_level = prms._cellpyfile_root
 
         with pd.HDFStore(filename) as store:
             try:
                 meta_table = store.select(parent_level + meta_dir)
             except KeyError:
                 raise WrongFileVersion(
                     "This file is VERY old - cannot read file version number"
                 )
         try:
-            cellpy_file_version = self._extract_from_dict(
-                meta_table, "cellpy_file_version"
+            # cellpy_file_version = self._extract_from_dict(
+            #     meta_table, "cellpy_file_version"
+            # )
+            meta_dict = meta_table.to_dict(orient="list")
+            cellpy_file_version = self._extract_from_meta_dictionary(
+                meta_dict, "cellpy_file_version"
             )
         except Exception as e:
             warnings.warn(f"Unhandled exception raised: {e}")
             return 0
 
         return cellpy_file_version
 
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
     def _load_hdf5(self, filename, parent_level=None, accept_old=False, selector=None):
         """Load a cellpy-file.
 
         Args:
             filename (str): Name of the cellpy file.
             parent_level (str) (optional): name of the parent level
-                (defaults to "CellpyData"). DeprecationWarning!
+                (defaults to "CellpyData").
             accept_old (bool): accept old file versions.
             selector (): select specific ranges (under development)
 
         Returns:
             loaded datasets (DataSet-object)
         """
 
@@ -1529,14 +1300,15 @@
 
         if not os.path.isfile(filename):
             logging.info(f"File does not exist: {filename}")
             raise IOError(f"File does not exist: {filename}")
 
         cellpy_file_version = self._get_cellpy_file_version(filename)
         logging.debug(f"Cellpy file version {cellpy_file_version}; selector={selector}")
+
         if cellpy_file_version > CELLPY_FILE_VERSION:
             raise WrongFileVersion(
                 f"File format too new: {filename} :: version: {cellpy_file_version}"
                 f"Reload from raw or upgrade your cellpy!"
             )
 
         elif cellpy_file_version < MINIMUM_CELLPY_FILE_VERSION:
@@ -1564,28 +1336,83 @@
             logging.debug(f"Loading {filename} :: v{cellpy_file_version}")
             new_data = self._load_hdf5_current_version(filename, selector=selector)
 
         # self.__check_loaded_data(new_data)
 
         return new_data
 
-    def _load_hdf5_current_version(
-        self, filename, meta_dir="/info", parent_level=None, selector=None
-    ):
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
+    def _load_hdf5_current_version(self, filename, parent_level=None, selector=None):
         if parent_level is None:
             parent_level = prms._cellpyfile_root
 
         raw_dir = prms._cellpyfile_raw
         step_dir = prms._cellpyfile_step
         summary_dir = prms._cellpyfile_summary
         fid_dir = prms._cellpyfile_fid
+        common_meta_dir = prms._cellpyfile_common_meta
+        test_dependent_meta_dir = prms._cellpyfile_test_dependent_meta
 
         logging.debug(f"filename: {filename}")
         logging.debug(f"selector: {selector}")
         with pd.HDFStore(filename) as store:
+            (
+                data,
+                meta_table,
+                test_dependent_meta_table,
+            ) = self._create_initial_data_set_from_cellpy_file(
+                common_meta_dir,
+                parent_level,
+                store,
+                test_dependent_meta_dir=test_dependent_meta_dir,
+            )
+            self._check_keys_in_cellpy_file(
+                common_meta_dir, parent_level, raw_dir, store, summary_dir
+            )
+            self._extract_summary_from_cellpy_file(
+                data, parent_level, store, summary_dir, selector=selector
+            )
+            self._extract_raw_from_cellpy_file(
+                data, parent_level, raw_dir, store, selector=selector
+            )
+            self._extract_steps_from_cellpy_file(
+                data, parent_level, step_dir, store, selector=selector
+            )
+            fid_table, fid_table_selected = self._extract_fids_from_cellpy_file(
+                fid_dir, parent_level, store
+            )
+
+        self._extract_meta_from_cellpy_file(
+            data, meta_table, test_dependent_meta_table, filename
+        )
+
+        if fid_table_selected:
+            (
+                data.raw_data_files,
+                data.raw_data_files_length,
+            ) = self._convert2fid_list(fid_table)
+        else:
+            data.raw_data_files = []
+            data.raw_data_files_length = []
+        return data
+
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
+    def _load_hdf5_v7(self, filename, selector=None, **kwargs):
+        logging.debug("--- loading v7")
+        meta_dir = "/info"
+        parent_level = kwargs.pop("parent_level", "CellpyData")
+        raw_dir = kwargs.pop("raw_dir", "/raw")
+        step_dir = kwargs.pop("step_dir", "/steps")
+        summary_dir = kwargs.pop("summary_dir", "/summary")
+        fid_dir = kwargs.pop("fid_dir", "/fid")
+
+        logging.debug(f"filename: {filename}")
+        logging.debug(f"selector: {selector}")
+
+        with pd.HDFStore(filename) as store:
             data, meta_table = self._create_initial_data_set_from_cellpy_file(
                 meta_dir, parent_level, store
             )
             self._check_keys_in_cellpy_file(
                 meta_dir, parent_level, raw_dir, store, summary_dir
             )
             self._extract_summary_from_cellpy_file(
@@ -1597,31 +1424,93 @@
             self._extract_steps_from_cellpy_file(
                 data, parent_level, step_dir, store, selector=selector
             )
             fid_table, fid_table_selected = self._extract_fids_from_cellpy_file(
                 fid_dir, parent_level, store
             )
 
-        self._extract_meta_from_cellpy_file(data, meta_table, filename)
+        self._extract_meta_from_old_cellpy_file_max_v7(
+            data, meta_table, filename, upgrade_from_to=(7, CELLPY_FILE_VERSION)
+        )
+
+        if fid_table_selected:
+            (
+                data.raw_data_files,
+                data.raw_data_files_length,
+            ) = self._convert2fid_list(fid_table)
+        else:
+            data.raw_data_files = []
+            data.raw_data_files_length = []
+        return data
+
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
+    def _load_hdf5_v6(self, filename, selector=None):
+        logging.critical("--- loading v6")
+        parent_level = "CellpyData"
+        raw_dir = "/raw"
+        step_dir = "/steps"
+        summary_dir = "/summary"
+        fid_dir = "/fid"
+        meta_dir = "/info"
+
+        with pd.HDFStore(filename) as store:
+            data, meta_table = self._create_initial_data_set_from_cellpy_file(
+                meta_dir,
+                parent_level,
+                store,
+            )
+            self._check_keys_in_cellpy_file(
+                meta_dir, parent_level, raw_dir, store, summary_dir
+            )
+            self._extract_summary_from_cellpy_file(
+                data,
+                parent_level,
+                store,
+                summary_dir,
+                selector=selector,
+                upgrade_from_to=(6, CELLPY_FILE_VERSION),
+            )
+            self._extract_raw_from_cellpy_file(
+                data,
+                parent_level,
+                raw_dir,
+                store,
+                selector=selector,
+                upgrade_from_to=(6, CELLPY_FILE_VERSION),
+            )
+            self._extract_steps_from_cellpy_file(
+                data,
+                parent_level,
+                step_dir,
+                store,
+                selector=selector,
+            )
+            fid_table, fid_table_selected = self._extract_fids_from_cellpy_file(
+                fid_dir, parent_level, store
+            )
+
+        self._extract_meta_from_old_cellpy_file_max_v7(
+            data, meta_table, filename, upgrade_from_to=(6, CELLPY_FILE_VERSION)
+        )
 
         if fid_table_selected:
             (
                 data.raw_data_files,
                 data.raw_data_files_length,
             ) = self._convert2fid_list(fid_table)
         else:
             data.raw_data_files = []
             data.raw_data_files_length = []
-        # this does not yet allow multiple sets
-        new_tests = [
-            data
-        ]  # but cellpy is ready when that time comes (if it ever happens)
-        return new_tests
 
+        logging.debug("loaded new test")
+        return data
+
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
     def _load_hdf5_v5(self, filename, selector=None):
+        logging.critical("--- loading v5")
         parent_level = "CellpyData"
         raw_dir = "/raw"
         step_dir = "/steps"
         summary_dir = "/summary"
         fid_dir = "/fid"
         meta_dir = "/info"
 
@@ -1629,164 +1518,206 @@
             data, meta_table = self._create_initial_data_set_from_cellpy_file(
                 meta_dir, parent_level, store
             )
             self._check_keys_in_cellpy_file(
                 meta_dir, parent_level, raw_dir, store, summary_dir
             )
             self._extract_summary_from_cellpy_file(
-                data, parent_level, store, summary_dir, selector=selector
+                data,
+                parent_level,
+                store,
+                summary_dir,
+                selector=selector,
+                upgrade_from_to=(5, CELLPY_FILE_VERSION),
             )
             self._extract_raw_from_cellpy_file(
-                data, parent_level, raw_dir, store, selector=selector
+                data,
+                parent_level,
+                raw_dir,
+                store,
+                selector=selector,
+                upgrade_from_to=(5, CELLPY_FILE_VERSION),
             )
             self._extract_steps_from_cellpy_file(
                 data, parent_level, step_dir, store, selector=selector
             )
             fid_table, fid_table_selected = self._extract_fids_from_cellpy_file(
                 fid_dir, parent_level, store
             )
 
-        self._extract_meta_from_cellpy_file(data, meta_table, filename)
+        self._extract_meta_from_old_cellpy_file_max_v7(data, meta_table, filename)
 
         if fid_table_selected:
             (
                 data.raw_data_files,
                 data.raw_data_files_length,
             ) = self._convert2fid_list(fid_table)
         else:
             data.raw_data_files = []
             data.raw_data_files_length = []
 
-        # this does not yet allow multiple sets
         logging.debug("loaded new test")
-        new_tests = [
-            data
-        ]  # but cellpy is ready when that time comes (if it ever happens)
-        return new_tests
+        return data
 
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
     def _load_old_hdf5(self, filename, cellpy_file_version):
         if cellpy_file_version < 5:
-            new_data = self._load_old_hdf5_v3_to_v4(filename)
+            data = self._load_old_hdf5_v3_to_v4(filename)
         elif cellpy_file_version == 5:
-            new_data = self._load_hdf5_v5(filename)
+            data = self._load_hdf5_v5(filename)
+        elif cellpy_file_version == 6:
+            data = self._load_hdf5_v6(filename)
+        elif cellpy_file_version == 7:
+            data = self._load_hdf5_v7(filename)
         else:
             raise WrongFileVersion(f"version {cellpy_file_version} is not supported")
 
-        if cellpy_file_version < 6:
-            logging.debug("legacy cellpy file version needs translation")
-            new_data = old_settings.translate_headers(new_data, cellpy_file_version)
-            # self.__check_loaded_data(new_data)
-        return new_data
-
-    def __check_loaded_data(self, new_data):
-        print("Checking loaded data".center(80, "="))
-        print("file names:")
-        print(self.file_names)
-        print("new data sets:")
-        print(len(new_data))
-        print("first data set:")
-        first = new_data[0]
-        print(first)
+        # if cellpy_file_version < 6:
+        #     logging.debug("legacy cellpy file version needs translation")
+        #     # data.raw = cellpy_file_upgrade_settings()
+        #     data.raw = rename_raw_columns(data.raw, old, new)
+        #     # data = old_settings.translate_headers(data, cellpy_file_version)
+        #     # self.__check_loaded_data(data)
+        return data
 
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
     def _load_old_hdf5_v3_to_v4(self, filename):
+        logging.critical("--- loading v < 5")
         parent_level = "CellpyData"
         meta_dir = "/info"
         _raw_dir = "/dfdata"
         _step_dir = "/step_table"
         _summary_dir = "/dfsummary"
         _fid_dir = "/fidtable"
 
         with pd.HDFStore(filename) as store:
             data, meta_table = self._create_initial_data_set_from_cellpy_file(
                 meta_dir, parent_level, store
             )
 
-        self._check_keys_in_cellpy_file(
-            meta_dir, parent_level, _raw_dir, store, _summary_dir
-        )
-        self._extract_summary_from_cellpy_file(data, parent_level, store, _summary_dir)
-        self._extract_raw_from_cellpy_file(data, parent_level, _raw_dir, store)
-        self._extract_steps_from_cellpy_file(data, parent_level, _step_dir, store)
-        fid_table, fid_table_selected = self._extract_fids_from_cellpy_file(
-            _fid_dir, parent_level, store
-        )
-        self._extract_meta_from_cellpy_file(data, meta_table, filename)
+            self._check_keys_in_cellpy_file(
+                meta_dir, parent_level, _raw_dir, store, _summary_dir
+            )
+            self._extract_summary_from_cellpy_file(
+                data,
+                parent_level,
+                store,
+                _summary_dir,
+                upgrade_from_to=(4, CELLPY_FILE_VERSION),
+            )
+            self._extract_raw_from_cellpy_file(
+                data,
+                parent_level,
+                _raw_dir,
+                store,
+                upgrade_from_to=(4, CELLPY_FILE_VERSION),
+            )
+            self._extract_steps_from_cellpy_file(
+                data,
+                parent_level,
+                _step_dir,
+                store,
+                upgrade_from_to=(4, CELLPY_FILE_VERSION),
+            )
+            fid_table, fid_table_selected = self._extract_fids_from_cellpy_file(
+                _fid_dir, parent_level, store
+            )
+        self._extract_meta_from_old_cellpy_file_max_v7(data, meta_table, filename)
         warnings.warn(
-            "Loaded old cellpy-file version (<5). " "Please update and save again."
+            "Loaded old cellpy-file version (<5). Please update and save again."
         )
         if fid_table_selected:
             (
                 data.raw_data_files,
                 data.raw_data_files_length,
             ) = self._convert2fid_list(fid_table)
         else:
             data.raw_data_files = []
             data.raw_data_files_length = []
 
-        new_tests = [data]
-        return new_tests
-
-    def _create_initial_data_set_from_cellpy_file(self, meta_dir, parent_level, store):
+        # new_tests = [data]
+        # return new_tests
+        return data
+
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
+    def _create_initial_data_set_from_cellpy_file(
+        self, meta_dir, parent_level, store, test_dependent_meta_dir=None
+    ):
         # Remark that this function is run before selecting loading method
-        # based on version. If you change the meta_dir prm to something else than
+        # based on version. If you change the common_meta_dir prm to something else than
         # "/info" it will most likely fail.
-        # Remark! Used for versions 3, 4, 5
+        # Remark! Used from versions 3
+        if test_dependent_meta_dir is not None:
+            common_meta_table = store.select(parent_level + meta_dir)
+            test_dependent_meta = store.select(parent_level + test_dependent_meta_dir)
+            data = Data()
+            # data.cellpy_file_version = CELLPY_FILE_VERSION
+            return data, common_meta_table, test_dependent_meta
 
-        data = Cell()
+        data = Data()
         meta_table = None
 
         try:
             meta_table = store.select(parent_level + meta_dir)
         except KeyError as e:
             logging.info("This file is VERY old - no info given here")
             logging.info("You should convert the files to a newer version!")
             logging.debug(e)
             return data, meta_table
 
         try:
-            data.cellpy_file_version = self._extract_from_dict(
-                meta_table, "cellpy_file_version"
-            )
+            meta_dict = meta_table.to_dict(orient="list")
+            # data.cellpy_file_version = self._extract_from_meta_dictionary(
+            #     meta_dict, "cellpy_file_version"
+            # )
         except Exception as e:
-            data.cellpy_file_version = 0
+            # data.cellpy_file_version = 0
             warnings.warn(f"Unhandled exception raised: {e}")
             return data, meta_table
 
-        logging.debug(f"cellpy file version. {data.cellpy_file_version}")
+        # logging.debug(f"cellpy file version. {data.cellpy_file_version}")
         return data, meta_table
 
-    def _check_keys_in_cellpy_file(
-        self, meta_dir, parent_level, raw_dir, store, summary_dir
-    ):
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
+    @staticmethod
+    def _check_keys_in_cellpy_file(meta_dir, parent_level, raw_dir, store, summary_dir):
         required_keys = [raw_dir, summary_dir, meta_dir]
         required_keys = ["/" + parent_level + _ for _ in required_keys]
+
         for key in required_keys:
             if key not in store.keys():
                 logging.info(
                     f"This cellpy-file is not good enough - "
                     f"at least one key is missing: {key}"
                 )
                 raise Exception(
                     f"OH MY GOD! At least one crucial key is missing {key}!"
                 )
         logging.debug(f"Keys in current cellpy-file: {store.keys()}")
 
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
     def _hdf5_locate_data_points_from_max_cycle_number(
         self, table_name, max_cycle, parent_level, store, child_level
     ):
         if table_name == prms._cellpyfile_step:
-            _cycle_header = headers_step_table.cycle
+            _cycle_header = self.headers_step_table.cycle
             table_path = parent_level + child_level
         elif table_name == prms._cellpyfile_raw:
-            _cycle_header = headers_normal.cycle_index_txt
+            _cycle_header = self.headers_normal.cycle_index_txt
             table_path = parent_level + child_level
+        else:
+            raise IOError(
+                f"provided wrong table name: {table_name} "
+                f"(valid options: ({prms._cellpyfile_step}, {prms._cellpyfile_raw}))"
+            )
 
         cycles = store.select(table_path, where="columns=[_cycle_header]")
         return cycles[_cycle_header] <= max_cycle
 
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
     def _hdf5_cycle_filter(self, table=None):
         # this is not the best way to do it
         if max_cycle := self.limit_loaded_cycles:
             if table == "summary":
                 logging.debug(f"limited to cycle_number {max_cycle}")
                 return f"index <= {int(max_cycle)}"
             elif table == "raw":
@@ -1796,323 +1727,394 @@
                 return f"index <= {int(self.limit_data_points)}"
             elif table == "steps":
                 # update this by finding the last data point
                 #  by making a function setting self.limit_data_points
                 logging.debug(f"limited to data_point {self.limit_data_points}")
                 return f"index <= {int(self.limit_data_points)}"
 
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
     def _unpack_selector(self, selector):
         # not implemented yet
         # should be used for trimming the selector so that it is not necessary to parse it individually
         # for all the _extract_xxx_from_cellpy_file methods.
         return selector
 
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
     def _extract_summary_from_cellpy_file(
-        self, data, parent_level, store, summary_dir, selector=None
+        self,
+        data: Data,
+        parent_level: str,
+        store: pd.HDFStore,
+        summary_dir: str,
+        selector: Union[None, str] = None,
+        upgrade_from_to: tuple = None,
     ):
         if selector is not None:
             cycle_filter = []
             if max_cycle := selector.get("max_cycle", None):
                 # self.overwrite_able = False
 
                 cycle_filter.append(f"index <= {int(max_cycle)}")
                 self.limit_loaded_cycles = max_cycle
         else:
             # getting cycle filter by setting attributes:
             self.limit_loaded_cycles = None
             cycle_filter = self._hdf5_cycle_filter("summary")
 
         data.summary = store.select(parent_level + summary_dir, where=cycle_filter)
+        if upgrade_from_to is not None:
+            old, new = upgrade_from_to
+            logging.debug(f"upgrading from {old} to {new}")
+            data.summary = rename_summary_columns(data.summary, old, new)
 
         # TODO: max data point should be an attribute
-        max_data_point = data.summary["data_point"].max()
+        try:
+            max_data_point = data.summary[self.headers_summary.data_point].max()
+        except KeyError as e:
+            raise KeyError(
+                f"You are most likely trying to open a too old cellpy file"
+            ) from e
+
         self.limit_data_points = int(max_data_point)
         logging.debug(f"data-point max limit: {self.limit_data_points}")
 
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
     def _extract_raw_from_cellpy_file(
-        self, data, parent_level, raw_dir, store, selector=None
+        self,
+        data,
+        parent_level,
+        raw_dir,
+        store,
+        selector: Union[None, str] = None,
+        upgrade_from_to: tuple = None,
     ):
         # selector is not implemented yet for only raw data
         # however, selector for max_cycle will still work since
         # the attribute self.limit_data_points is set while reading the summary
         cycle_filter = self._hdf5_cycle_filter(table="raw")
         data.raw = store.select(parent_level + raw_dir, where=cycle_filter)
+        if upgrade_from_to is not None:
+            old, new = upgrade_from_to
+            logging.debug(f"upgrading from {old} to {new}")
+            data.raw = rename_raw_columns(data.raw, old, new)
 
     def _extract_steps_from_cellpy_file(
-        self, data, parent_level, step_dir, store, selector=None
+        self,
+        data,
+        parent_level,
+        step_dir,
+        store,
+        selector: Union[None, str] = None,
+        upgrade_from_to: tuple = None,
     ):
         try:
             data.steps = store.select(parent_level + step_dir)
             if self.limit_data_points:
                 data.steps = data.steps.loc[
                     data.steps["point_last"] <= self.limit_data_points
                 ]
                 logging.debug(f"limited to data_point {self.limit_data_points}")
+            if upgrade_from_to is not None:
+                old, new = upgrade_from_to
+                logging.debug(f"upgrading from {old} to {new}")
+                data.steps = rename_step_columns(data.steps, old, new)
         except Exception as e:
             print(e)
             logging.debug("could not get steps from cellpy-file")
             data.steps = pd.DataFrame()
             warnings.warn(f"Unhandled exception raised: {e}")
 
-    def _extract_fids_from_cellpy_file(self, fid_dir, parent_level, store):
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
+    def _extract_fids_from_cellpy_file(
+        self, fid_dir, parent_level, store, upgrade_from_to: tuple = None
+    ):
         logging.debug(f"Extracting fid table from {fid_dir} in hdf5 store")
         try:
             fid_table = store.select(
                 parent_level + fid_dir
             )  # remark! changed spelling from
             # lower letter to camel-case!
             fid_table_selected = True
+            if upgrade_from_to is not None:
+                old, new = upgrade_from_to
+                logging.debug(f"upgrading from {old} to {new}")
+                fid_table = rename_fid_columns(fid_table, old, new)
         except Exception as e:
             logging.debug(e)
             logging.debug("could not get fid from cellpy-file")
             fid_table = []
             warnings.warn("no fid_table - you should update your cellpy-file")
             fid_table_selected = False
         return fid_table, fid_table_selected
 
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
     def _extract_meta_from_cellpy_file(
-        self, data: Cell, meta_table: pd.DataFrame, filename: Union[Path, str]
+        self,
+        data: Data,
+        meta_table: pd.DataFrame,
+        test_dependent_meta_table: pd.DataFrame,
+        filename: Union[Path, str],
+        upgrade_from_to: tuple = None,
+    ) -> None:
+        if upgrade_from_to is not None:
+            old, new = upgrade_from_to
+            print(f"upgrading meta from {old} to {new}")
+            logging.debug(f"upgrading meta from {old} to {new}")
+            # fid_table = rename_fid_columns(fid_table, old, new)
+
+        data.loaded_from = str(filename)
+        meta_dict = meta_table.to_dict(orient="list")
+
+        # unpacking the raw data limits
+        # remark! stored as scalars (not test dependent)
+        for key in data.raw_limits:
+            h5_key = f"{prms._cellpyfile_raw_limit_pre_id}{key}"
+            try:
+                v = meta_dict.pop(h5_key)
+                data.raw_units[key] = v[0]
+            except KeyError:
+                logging.debug(f"missing key in meta_table: {h5_key}")
+                # warnings.warn("OLD-TYPE: Recommend to save in new format!")
+
+        # unpacking the raw data units
+        # remark! stored as scalars (not test dependent)
+        for key in data.raw_units:
+            h5_key = f"{prms._cellpyfile_raw_unit_pre_id}{key}"
+            try:
+                v = meta_dict.pop(h5_key)
+                data.raw_units[key] = v[0]
+            except KeyError:
+                logging.critical(f"missing key in meta_table: {h5_key}")
+                # warnings.warn("OLD-TYPE: Recommend to save in new format!")
+
+        data.meta_common.update(as_list=False, **meta_dict)
+        test_dependent_meta_dict = test_dependent_meta_table.to_dict(orient="list")
+        data.meta_test_dependent.update(as_list=True, **test_dependent_meta_dict)
+
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
+    def _extract_meta_from_old_cellpy_file_max_v7(
+        self,
+        data: Data,
+        meta_table: pd.DataFrame,
+        filename: Union[Path, str],
+        upgrade_from_to: tuple,
     ) -> None:
         # get attributes from meta table
         # remark! could also utilise the pandas to dictionary method directly
         # for example: meta_table.T.to_dict()
         # Maybe a good task for someone who would like to learn more about
         # how cellpy works.
 
-        for attribute in ATTRS_CELLPYFILE:
-            value = self._extract_from_dict(meta_table, attribute)
-            # some fixes due to errors propagated into the cellpy-files
-            if attribute == "creator":
-                if not isinstance(value, str):
-                    value = "no_name"
-
-            if attribute == "test_no":
-                if not isinstance(value, (int, float)):
-                    value = 0
-
-            setattr(data, attribute, value)
-
-        if data.mass is None:
-            data.mass = 1.0
-        else:
-            data.mass_given = True
-
-        if data.cycle_mode is None:
-            logging.critical("cycle mode not found")
+        old, new = upgrade_from_to
+        logging.debug(f"upgrading meta from {old} to {new}")
+        if old > 7:
+            raise IOError("using this method for processing v>7 is not allowed!")
 
-        data.loaded_from = str(filename)
-
-        # hack to allow the renaming of tests to datasets
-        try:
-            name = self._extract_from_dict_hard(meta_table, "name")
-            if not isinstance(name, str):
-                name = "no_name"
-            data.name = name
-
-        except KeyError:
-            logging.debug(f"missing key in meta table: {name}")
-            # warnings.warn("OLD-TYPE: Recommend to save in new format!")
-            try:
-                name = self._extract_from_dict(meta_table, "test_name")
-            except Exception as e:
-                name = "no_name"
-                logging.debug("name set to 'no_name")
-                # warnings.warn(f"Unhandled exception raised: {e}")
-            data.name = name
+        meta_dict = meta_table.to_dict(orient="list")
 
         # unpacking the raw data limits
-        # TODO: check if they end up at the correct level (cellpydata or cell)
+        # remark! stored as scalars (not test dependent)
         for key in data.raw_limits:
-            h5_key = key
+            h5_key = f"{prms._cellpyfile_raw_limit_pre_id}{key}"
             try:
-                data.raw_limits[key] = self._extract_from_dict_hard(meta_table, h5_key)
+                v = meta_dict.pop(h5_key)
+                data.raw_units[key] = v[0]
             except KeyError:
                 logging.debug(f"missing key in meta_table: {h5_key}")
                 # warnings.warn("OLD-TYPE: Recommend to save in new format!")
 
         # unpacking the raw data units
-        # TODO: check if they end up at the correct level (cellpydata or cell)
+        # remark! stored as scalars (not test dependent)
         for key in data.raw_units:
-            h5_key = f"raw_unit_{key}"
+            h5_key = f"{prms._cellpyfile_raw_unit_pre_id}{key}"
             try:
-                data.raw_units[key] = self._extract_from_dict_hard(meta_table, h5_key)
+                v = meta_dict.pop(h5_key)
+                v = v[0]
+                if not isinstance(v, str):
+                    logging.debug(f"{v} is not of type string")
+                    v = convert_from_simple_unit_label_to_string_unit_label(key, v)
+                data.raw_units[key] = v
             except KeyError:
                 logging.critical(f"missing key in meta_table: {h5_key}")
                 # warnings.warn("OLD-TYPE: Recommend to save in new format!")
 
+        meta_dict = data.meta_common.digest(as_list=False, **meta_dict)
+        data.meta_test_dependent.update(as_list=True, **meta_dict)
+
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
     @staticmethod
-    def _extract_from_dict(t, x, default_value=None):
+    def _extract_from_meta_dictionary(
+        meta_dict, attribute, default_value=None, hard=False
+    ):
         try:
-            value = t[x].values
-            if value:
-                value = value[0]
-        except KeyError:
+            value = meta_dict[attribute][0]
+            if not value:
+                value = None
+        except KeyError as e:
+            if hard:
+                raise KeyError from e
             value = default_value
         return value
 
-    @staticmethod
-    def _extract_from_dict_hard(t, x):
-        value = t[x].values
-        if value:
-            value = value[0]
-        return value
-
-    def _create_infotable(self, dataset_number=None):
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-exporters?):
+    def _create_infotable(self):
         # needed for saving class/DataSet to hdf5
-
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
-
-        cell = self.cell
-
-        infotable = collections.OrderedDict()
-
-        for attribute in ATTRS_CELLPYFILE:
-            value = getattr(cell, attribute)
-            infotable[attribute] = [value]
-
-        infotable["cellpy_file_version"] = [CELLPY_FILE_VERSION]
-        infotable["cycle_mode"] = [self.cycle_mode]
+        cell = self.data
+        new_info_table = asdict(cell.meta_common)
+        new_info_table_test_dependent = asdict(cell.meta_test_dependent)
+        new_info_table["cellpy_file_version"] = CELLPY_FILE_VERSION
 
         limits = cell.raw_limits
         for key in limits:
-            h5_key = key
-            infotable[key] = limits[h5_key]
+            h5_key = f"{prms._cellpyfile_raw_limit_pre_id}{key}"
+            new_info_table[key] = limits[h5_key]
 
         units = cell.raw_units
         for key in units:
-            h5_key = f"raw_unit_{key}"
-            infotable[h5_key] = units[key]
+            h5_key = f"{prms._cellpyfile_raw_unit_pre_id}{key}"
+            value = units[key]
+            if not isinstance(value, str):
+                raise IOError(
+                    f"raw unit for {key} ({value}) must be of type string, not {type(value)}"
+                )
+            new_info_table[h5_key] = value
 
-        infotable = pd.DataFrame(infotable)
+        new_info_table = pd.DataFrame.from_records([new_info_table])
+        new_info_table_test_dependent = pd.DataFrame.from_records(
+            [new_info_table_test_dependent]
+        )
 
-        logging.debug("_create_infotable: fid")
+        fidtable = self._convert2fid_table(cell)
+        fidtable = pd.DataFrame(fidtable)
+        # TODO: test_dependent with several tests (and possibly merge with FID)
+        # TODO: save
+        # TODO: load old
+        # TODO: find out if it is possible to initiate dataclasses with **kwargs (for loading)
+        # TODO: update getters and setters (cell_name etc)
+        return new_info_table, new_info_table_test_dependent, fidtable
+
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-exporters?):
+    @staticmethod
+    def _convert2fid_table(cell):
+        # used when saving cellpy-file
+        logging.debug("converting FileID object to fid-table that can be saved")
         fidtable = collections.OrderedDict()
         fidtable["raw_data_name"] = []
         fidtable["raw_data_full_name"] = []
         fidtable["raw_data_size"] = []
         fidtable["raw_data_last_modified"] = []
         fidtable["raw_data_last_accessed"] = []
         fidtable["raw_data_last_info_changed"] = []
         fidtable["raw_data_location"] = []
+        # TODO: consider deprecating this as we now have implemented last_data_point:
         fidtable["raw_data_files_length"] = []
+
         fidtable["last_data_point"] = []
         fids = cell.raw_data_files
-        fidtable["raw_data_fid"] = fids
         if fids:
             for fid, length in zip(fids, cell.raw_data_files_length):
                 try:
-                    fidtable["raw_data_name"].append(str(Path(fid.name).name))
-                    fidtable["raw_data_full_name"].append(str(Path(fid.full_name)))
+                    fidtable["raw_data_name"].append(fid.name)
+                    fidtable["raw_data_full_name"].append(fid.full_name)
                     fidtable["raw_data_size"].append(fid.size)
                     fidtable["raw_data_last_modified"].append(fid.last_modified)
                     fidtable["raw_data_last_accessed"].append(fid.last_accessed)
                     fidtable["raw_data_last_info_changed"].append(fid.last_info_changed)
-                except Exception:
+                except AttributeError:  # TODO: this is probably not needed anymore
                     logging.debug("this is probably not from a file")
                     fidtable["raw_data_name"].append("db")
                     fidtable["raw_data_full_name"].append("db")
                     fidtable["raw_data_size"].append(fid.size)
                     fidtable["raw_data_last_modified"].append("db")
                     fidtable["raw_data_last_accessed"].append("db")
                     fidtable["raw_data_last_info_changed"].append("db")
 
                 fidtable["raw_data_location"].append(fid.location)
                 fidtable["raw_data_files_length"].append(length)
-                fidtable["last_data_point"].append(fid.last_data_point)
+                fidtable["last_data_point"].append(
+                    fid.last_data_point
+                )  # will most likely be the same as length
         else:
             warnings.warn("seems you lost info about your raw-data (missing fids)")
-        fidtable = pd.DataFrame(fidtable)
-        return infotable, fidtable
+        return fidtable
 
-    def _convert2fid_list(self, tbl):
-        logging.debug("converting loaded fidtable to FileID object")
+    # TODO @jepe: move this to its own module (e.g. as a cellpy-loader in instruments?):
+    @staticmethod
+    def _convert2fid_list(tbl):
+        # used when reading cellpy-file
+        logging.debug("converting loaded fid-table to FileID object")
         fids = []
         lengths = []
         min_amount = 0
         for counter, item in enumerate(tbl["raw_data_name"]):
             fid = FileID()
             try:
-                fid.name = Path(item).name
+                fid.name = OtherPath(item).name
             except NotImplementedError:
-                fid.name = os.path.basename(item)
+                fid.name = item
             fid.full_name = tbl["raw_data_full_name"][counter]
             fid.size = tbl["raw_data_size"][counter]
             fid.last_modified = tbl["raw_data_last_modified"][counter]
             fid.last_accessed = tbl["raw_data_last_accessed"][counter]
             fid.last_info_changed = tbl["raw_data_last_info_changed"][counter]
             fid.location = tbl["raw_data_location"][counter]
             length = tbl["raw_data_files_length"][counter]
             if "last_data_point" in tbl.columns:
                 fid.last_data_point = tbl["last_data_point"][counter]
             else:
                 fid.last_data_point = 0
+            if "is_db" in tbl.columns:
+                fid.is_db = tbl["is_db"][counter]
             fids.append(fid)
             lengths.append(length)
             min_amount = 1
         if min_amount < 1:
             logging.debug("info about raw files missing")
         return fids, lengths
 
-    def merge(self, datasets=None, separate_datasets=False, **kwargs):
+    # TODO @jepe (v.1.0.0): update this to use single data instances (i.e. to cell from cells)
+    def merge(self, datasets: list, **kwargs):
         """This function merges datasets into one set."""
-
         logging.info("Merging")
-        if separate_datasets:
-            warnings.warn(
-                "The option separate_datasets=True is"
-                "not implemented yet. Performing merging, but"
-                "neglecting the option."
-            )
-        else:
-            if datasets is None:
-                datasets = list(range(len(self.cells)))
-            first = True
-            for dataset_number in datasets:
-                if first:
-                    dataset = self.cells[dataset_number]
-                    first = False
-                else:
-                    dataset = self._append(
-                        dataset, self.cells[dataset_number], **kwargs
-                    )
-                    for raw_data_file, file_size in zip(
-                        self.cells[dataset_number].raw_data_files,
-                        self.cells[dataset_number].raw_data_files_length,
-                    ):
-                        dataset.raw_data_files.append(raw_data_file)
-                        dataset.raw_data_files_length.append(file_size)
-            self.cells = [dataset]
-            self.number_of_datasets = 1
+        self.data = datasets.pop(0)
+        for data in datasets:
+            self.data = self._append(self.data, data, **kwargs)
+            for raw_data_file, file_size in zip(
+                data.raw_data_files,
+                data.raw_data_files_length,
+            ):
+                self.data.raw_data_files.append(raw_data_file)
+                self.data.raw_data_files_length.append(file_size)
         return self
 
+    # TODO @jepe (v.1.0.0): update/check this - single data instances (i.e. to cell from cells)
     def _append(self, t1, t2, merge_summary=False, merge_step_table=False, recalc=True):
         logging.debug(
             f"merging two datasets\n(merge summary = {merge_summary})\n"
             f"(merge step table = {merge_step_table})"
         )
         if t1.raw.empty:
             logging.debug("OBS! the first dataset is empty")
             logging.debug(" -> merged contains only second")
             return t2
 
         if t2.raw.empty:
-            t1.merged = True
             logging.debug("OBS! the second dataset was empty")
             logging.debug(" -> merged contains only first")
             return t1
 
+        if not isinstance(t1.loaded_from, (list, tuple)):
+            t1.loaded_from = [t1.loaded_from]
+
         cycle_index_header = self.headers_summary.cycle_index
-        cell = t1
+        data = t1
         if recalc:
             # finding diff of time
-            start_time_1 = t1.start_datetime
-            start_time_2 = t2.start_datetime
+            start_time_1 = t1.meta_common.start_datetime
+            start_time_2 = t2.meta_common.start_datetime
 
             if self.tester in ["arbin_res"]:
                 diff_time = xldate_as_datetime(start_time_2) - xldate_as_datetime(
                     start_time_1
                 )
             else:
                 diff_time = start_time_2 - start_time_1
@@ -2145,16 +2147,16 @@
             test_time_header = self.headers_normal.test_time_txt
             t2.raw[test_time_header] = t2.raw[test_time_header] + diff_time
         else:
             logging.debug("not doing recalc")
         # merging
         logging.debug("performing concat")
         raw = pd.concat([t1.raw, t2.raw], ignore_index=True)
-        cell.raw = raw
-        cell.no_cycles = max(raw[cycle_index_header])
+        data.raw = raw
+        data.loaded_from.append(t2.loaded_from)
         step_table_made = False
 
         if merge_summary:
             # checking if we already have made a summary file of these datasets
             # (to be used if merging summaries (but not properly implemented yet))
             if t1.summary.empty or t2.summary.empty:
                 summary_made = False
@@ -2197,15 +2199,15 @@
 
                     t2.summary[data_point_header] = (
                         t2.summary[data_point_header] + last_data_point
                     )
 
                 summary2 = pd.concat([t1.summary, t2.summary], ignore_index=True)
 
-                cell.summary = summary2
+                data.summary = summary2
             else:
                 logging.debug(
                     "could not merge summary tables "
                     "(non-existing) -"
                     "create them first!"
                 )
 
@@ -2213,69 +2215,34 @@
             if step_table_made:
                 cycle_index_header = self.headers_normal.cycle_index_txt
                 t2.steps[self.headers_step_table.cycle] = (
                     t2.raw[self.headers_step_table.cycle] + last_cycle
                 )
 
                 steps2 = pd.concat([t1.steps, t2.steps], ignore_index=True)
-                cell.steps = steps2
+                data.steps = steps2
             else:
                 logging.debug(
                     "could not merge step tables "
                     "(non-existing) -"
                     "create them first!"
                 )
 
-        cell.merged = True
         logging.debug(" -> merged with new dataset")
         # TODO: @jepe -  update merging for more variables
-        return cell
-
-    # --------------iterate-and-find-in-data-----------------------------------
-    # TODO: make this obsolete (somehow)
-    def _validate_dataset_number(self, n, check_for_empty=True):
-        # Returns dataset_number (or None if empty)
-        # Remark! _is_not_empty_dataset returns True or False
-
-        if not len(self.cells):
-            logging.info(
-                "Can't see any datasets! Are you sure you have " "loaded anything?"
-            )
-            return
-
-        if n is not None:
-            v = n
-        else:
-            if self.selected_cell_number is None:
-                v = 0
-            else:
-                v = self.selected_cell_number
-
-        if check_for_empty:
-            not_empty = self._is_not_empty_dataset(self.cells[v])
-            if not_empty:
-                return v
-            else:
-                return None
-        else:
-            return v
+        return data
 
     # TODO: check if this can be moved to helpers
-    def _validate_step_table(self, dataset_number=None, simple=False):
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
-
+    def _validate_step_table(self, simple=False):
         step_index_header = self.headers_normal.step_index_txt
         logging.debug("-validating step table")
-        d = self.cells[dataset_number].raw
-        s = self.cells[dataset_number].steps
+        d = self.data.raw
+        s = self.data.steps
 
-        if not self.cells[dataset_number].has_steps:
+        if not self.data.has_steps:
             return False
 
         no_cycles_raw = np.amax(d[self.headers_normal.cycle_index_txt])
         headers_step_table = self.headers_step_table
         no_cycles_step_table = np.amax(s[headers_step_table.cycle])
 
         if simple:
@@ -2305,41 +2272,27 @@
                         s.loc[
                             s[headers_step_table.cycle] == cycle_number,
                             headers_step_table.step,
                         ]
                     )
                     if no_steps_raw != no_steps_step_table:
                         validated = False
-                        # txt = ("Error in step table "
-                        #        "(cycle: %i) d: %i, s:%i)" % (
-                        #         cycle_number,
-                        #         no_steps_raw,
-                        #         no_steps_steps
-                        #     )
-                        # )
-                        #
-                        # logging.debug(txt)
             return validated
 
-    def print_steps(self, dataset_number=None):
+    def print_steps(self):
         """Print the step table."""
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
-        st = self.cells[dataset_number].steps
+        st = self.data.steps
         print(st)
 
     def get_step_numbers(
         self,
         steptype="charge",
         allctypes=True,
         pdtype=False,
         cycle_number=None,
-        dataset_number=None,
         trim_taper_steps=None,
         steps_to_skip=None,
         steptable=None,
     ):
         # TODO: @jepe - include sub_steps here
         # TODO: @jepe - include option for not selecting taper steps here
         """Get the step numbers of selected type.
@@ -2347,56 +2300,48 @@
         Returns the selected step_numbers for the selected type of step(s).
 
         Args:
             steptype (string): string identifying type of step.
             allctypes (bool): get all types of charge (or discharge).
             pdtype (bool): return results as pandas.DataFrame
             cycle_number (int): selected cycle, selects all if not set.
-            dataset_number (int): test number (default first)
-                (usually not used).
             trim_taper_steps (integer): number of taper steps to skip (counted
                 from the end, i.e. 1 means skip last step in each cycle).
             steps_to_skip (list): step numbers that should not be included.
             steptable (pandas.DataFrame): optional steptable
 
         Returns:
             A dictionary containing a list of step numbers corresponding
                 to the selected steptype for the cycle(s).
             Returns a pandas.DataFrame instead of a dict of lists if pdtype is
                 set to True. The frame is a sub-set of the step-table frame
                 (i.e. all the same columns, only filtered by rows).
 
         Example:
-            >>> my_charge_steps = CellpyData.get_step_numbers(
+            >>> my_charge_steps = CellpyCell.get_step_numbers(
             >>>    "charge",
             >>>    cycle_number = 3
             >>> )
             >>> print my_charge_steps
             {3: [5,8]}
 
         """
         t0 = time.time()
         # logging.debug("Trying to get step-types")
         if steps_to_skip is None:
             steps_to_skip = []
 
         if steptable is None:
-            dataset_number = self._validate_dataset_number(dataset_number)
-            # logging.debug(f"dt 1: {time.time() - t0}")
-            if dataset_number is None:
-                self._report_empty_dataset()
-                return
-
-            if not self.cells[dataset_number].has_steps:
+            if not self.data.has_steps:
                 logging.debug("steps is not made")
 
                 if self.force_step_table_creation or self.force_all:
                     logging.debug("creating step_table for")
-                    logging.debug(self.cells[dataset_number].loaded_from)
-                    self.make_step_table(dataset_number=dataset_number)
+                    logging.debug(self.data.loaded_from)
+                    self.make_step_table()
 
                 else:
                     logging.info("ERROR! Cannot use get_steps: create step_table first")
                     logging.info("You could use find_step_numbers method instead")
                     logging.info("(but I don't recommend it)")
                     return None
 
@@ -2436,23 +2381,23 @@
             for st in add_these:
                 steptypes.append(st)
 
         # logging.debug("Your steptypes:")
         # logging.debug(steptypes)
 
         if steptable is None:
-            st = self.cells[dataset_number].steps
+            st = self.data.steps
         else:
             st = steptable
         shdr = self.headers_step_table
 
         # retrieving cycle numbers
         # logging.debug(f"dt 3: {time.time() - t0}")
         if cycle_number is None:
-            cycle_numbers = self.get_cycle_numbers(dataset_number, steptable=steptable)
+            cycle_numbers = self.get_cycle_numbers(steptable=steptable)
         else:
             if isinstance(cycle_number, collections.abc.Iterable):
                 cycle_numbers = cycle_number
             else:
                 cycle_numbers = [cycle_number]
 
         if trim_taper_steps is not None:
@@ -2461,15 +2406,15 @@
 
         if pdtype:
             # logging.debug("Return pandas dataframe.")
             if trim_taper_steps:
                 logging.info(
                     "Trimming taper steps is currently not"
                     "possible when returning pd.DataFrame. "
-                    "Do it manually insteaD."
+                    "Do it manually instead."
                 )
             out = st[st[shdr.type].isin(steptypes) & st[shdr.cycle].isin(cycle_numbers)]
             return out
 
         # if not pdtype, return a dict instead
         # logging.debug("out as dict; out[cycle] = [s1,s2,...]")
         # logging.debug("(same behaviour as find_step_numbers)")
@@ -2496,28 +2441,23 @@
 
             if not steplist:
                 steplist = [0]
             out[cycle] = steplist
         # logging.debug(f"dt tot: {time.time() - t0}")
         return out
 
-    def load_step_specifications(self, file_name, short=False, dataset_number=None):
+    def load_step_specifications(self, file_name, short=False):
         """Load a table that contains step-type definitions.
 
         This function loads a file containing a specification for each step or
         for each (cycle_number, step_number) combinations if short==False. The
         step_cycle specifications that are allowed are stored in the variable
         cellreader.list_of_step_types.
         """
 
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
-
         # if short:
         #     # the table only consists of steps (not cycle,step pairs) assuming
         #     # that the step numbers uniquely defines step type (this is true
         #     # for arbin at least).
         #     raise NotImplementedError
 
         step_specs = pd.read_csv(file_name, sep=prms.Reader.sep)
@@ -2561,59 +2501,57 @@
         step_specifications=None,
         short=False,
         profiling=False,
         all_steps=False,
         add_c_rate=True,
         skip_steps=None,
         sort_rows=True,
-        dataset_number=None,
         from_data_point=None,
+        nom_cap_specifics=None,
     ):
-
         """Create a table (v.4) that contains summary information for each step.
 
         This function creates a table containing information about the
         different steps for each cycle and, based on that, decides what type of
         step it is (e.g. charge) for each cycle.
 
         The format of the steps is:
 
             index: cycleno - stepno - sub-step-no - ustep
-            Time info (average, stdev, max, min, start, end, delta) -
-            Logging info (average, stdev, max, min, start, end, delta) -
-            Current info (average, stdev, max, min, start, end, delta) -
-            Voltage info (average,  stdev, max, min, start, end, delta) -
-            Type (from pre-defined list) - SubType -
-            Info
+            Time info: average, stdev, max, min, start, end, delta
+            Logging info: average, stdev, max, min, start, end, delta
+            Current info: average, stdev, max, min, start, end, delta
+            Voltage info: average,  stdev, max, min, start, end, delta
+            Type: (from pre-defined list) - SubType
+            Info: not used.
 
-         Args:
+        Args:
             step_specifications (pandas.DataFrame): step specifications
             short (bool): step specifications in short format
             profiling (bool): turn on profiling
-
             all_steps (bool): investigate all steps including same steps within
                 one cycle (this is useful for e.g. GITT).
             add_c_rate (bool): include a C-rate estimate in the steps
             skip_steps (list of integers): list of step numbers that should not
                 be processed (future feature - not used yet).
             sort_rows (bool): sort the rows after processing.
-            dataset_number: defaults to self.dataset_number
-            from_data_point (int): first data point to use
+            from_data_point (int): first data point to use.
+            nom_cap_specifics (str): "gravimetric", "areal", or "absolute".
 
         Returns:
             None
+
         """
         # TODO: @jepe - include option for omitting steps
         # TODO: @jepe  - make it is possible to update only new data
 
         time_00 = time.time()
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
+
+        if nom_cap_specifics is None:
+            nom_cap_specifics = prms.Materials.default_nom_cap_specifics
 
         if profiling:
             print("PROFILING MAKE_STEP_TABLE".center(80, "="))
 
         def first(x):
             return x.iloc[0]
 
@@ -2629,19 +2567,19 @@
 
             return difference
 
         nhdr = self.headers_normal
         shdr = self.headers_step_table
 
         if from_data_point is not None:
-            df = self.cells[dataset_number].raw.loc[
-                self.cells[dataset_number].raw[nhdr.data_point_txt] >= from_data_point
+            df = self.data.raw.loc[
+                self.data.raw[nhdr.data_point_txt] >= from_data_point
             ]
         else:
-            df = self.cells[dataset_number].raw
+            df = self.data.raw
         # df[shdr.internal_resistance_change] = \
         #     df[nhdr.internal_resistance_txt].pct_change()
 
         # selecting only the most important columns from raw:
         keep = [
             nhdr.data_point_txt,
             nhdr.test_time_txt,
@@ -2690,44 +2628,48 @@
             df[shdr.ustep] = self._ustep(df[shdr.step])
 
         logging.debug(f"groupby: {by}")
 
         if profiling:
             time_01 = time.time()
 
-        # TODO: make sure that all columns are nummeric
+        # TODO: make sure that all columns are numeric
 
         gf = df.groupby(by=by)
         df_steps = gf.agg(
             [np.mean, np.std, np.amin, np.amax, first, last, delta]
         ).rename(columns={"amin": "min", "amax": "max", "mean": "avr"})
 
         df_steps = df_steps.reset_index()
 
         if profiling:
             print(f"*** groupby-agg: {time.time() - time_01} s")
             time_01 = time.time()
 
-        # new cols
-
         # column with C-rates:
         if add_c_rate:
-            nom_cap = self.cells[dataset_number].nom_cap
-            mass = self.cells[dataset_number].mass
-            spec_conv_factor = self.get_converter_to_specific()
-            logging.debug(f"c-rate: nom_cap={nom_cap} spec_conv={spec_conv_factor}")
+            logging.debug("adding c-rates")
+            nom_cap = self.data.nom_cap
+            if nom_cap_specifics == "gravimetric":
+                mass = self.data.mass
+                nom_cap = self.nominal_capacity_as_absolute(
+                    nom_cap, mass, nom_cap_specifics
+                )
 
+            elif nom_cap_specifics == "areal":
+                area = self.data.active_electrode_area
+                nom_cap = self.nominal_capacity_as_absolute(
+                    nom_cap, area, nom_cap_specifics
+                )
             df_steps[shdr.rate_avr] = abs(
                 round(
-                    df_steps.loc[:, (shdr.current, "avr")]
-                    / (nom_cap / spec_conv_factor),
-                    2,
+                    df_steps.loc[:, (shdr.current, "avr")] / nom_cap,
+                    3,
                 )
             )
-
         df_steps[shdr.type] = np.nan
         df_steps[shdr.sub_type] = np.nan
         df_steps[shdr.info] = np.nan
 
         if step_specifications is None:
             current_limit_value_hard = self.raw_limits["current_hard"]
             current_limit_value_soft = self.raw_limits["current_soft"]
@@ -2916,28 +2858,24 @@
             print(f"*** flattening: {time.time() - time_01} s")
 
         logging.debug(f"(dt: {(time.time() - time_00):4.2f}s)")
 
         if from_data_point is not None:
             return df_steps
         else:
-            self.cells[dataset_number].steps = df_steps
+            self.data.steps = df_steps
             return self
 
-    def select_steps(self, step_dict, append_df=False, dataset_number=None):
+    def select_steps(self, step_dict, append_df=False):
         """Select steps (not documented yet)."""
         raise DeprecatedFeature
 
-    def _select_step(self, cycle, step, dataset_number=None):
+    def _select_step(self, cycle, step):
         # TODO: @jepe - insert sub_step here
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
-        test = self.cells[dataset_number]
+        test = self.data
 
         # check if columns exist
         c_txt = self.headers_normal.cycle_index_txt
         s_txt = self.headers_normal.step_index_txt
         y_txt = self.headers_normal.voltage_txt
         x_txt = self.headers_normal.discharge_capacity_txt  # jepe fix
 
@@ -2950,28 +2888,27 @@
         if not any(test.raw.columns == s_txt):
             logging.info("ERROR - cannot find %s" % s_txt)
             sys.exit(-1)
 
         # logging.debug(f"selecting cycle {cycle} step {step}")
         v = test.raw[(test.raw[c_txt] == cycle) & (test.raw[s_txt] == step)]
 
-        if self.is_empty(v):
+        if self._is_empty_array(v):
             logging.debug("empty dataframe")
             return None
         else:
             return v
 
-    def populate_step_dict(self, step, dataset_number=None):
+    def populate_step_dict(self, step):
         """Returns a dict with cycle numbers as keys
         and corresponding steps (list) as values."""
         raise DeprecatedFeature
 
     def _export_cycles(
         self,
-        dataset_number,
         setname=None,
         sep=None,
         outname=None,
         shifted=False,
         method=None,
         shift=0.0,
         last_cycle=None,
@@ -2984,15 +2921,15 @@
         if sep is None:
             sep = self.sep
         if outname is None:
             outname = setname + lastname
 
         logging.debug(f"outname: {outname}")
 
-        list_of_cycles = self.get_cycle_numbers(dataset_number=dataset_number)
+        list_of_cycles = self.get_cycle_numbers()
         if last_cycle is not None:
             list_of_cycles = [c for c in list_of_cycles if c <= int(last_cycle)]
             logging.debug(f"only processing up to cycle {last_cycle}")
             logging.debug(f"you have {len(list_of_cycles)}" f"cycles to process")
         out_data = []
         c = None
         if not method:
@@ -3003,17 +2940,15 @@
             _last = 0.0
         logging.debug(f"number of cycles: {len(list_of_cycles)}")
         for cycle in list_of_cycles:
             try:
                 if shifted and c is not None:
                     shift = _last
                     # print(f"shifted = {shift}, first={_first}")
-                df = self.get_cap(
-                    cycle, dataset_number=dataset_number, method=method, shift=shift
-                )
+                df = self.get_cap(cycle, method=method, shift=shift)
                 if df.empty:
                     logging.debug("NoneType from get_cap")
                 else:
                     c = df["capacity"]
                     v = df["voltage"]
 
                     _last = c.iat[-1]
@@ -3045,15 +2980,14 @@
         logging.info(f"The file {outname} was created")
         logging.debug(f"(dt: {(time.time() - time_00):4.2f}s)")
         logging.debug("END exporting cycles")
 
     # TODO: remove this
     def _export_cycles_old(
         self,
-        dataset_number,
         setname=None,
         sep=None,
         outname=None,
         shifted=False,
         method=None,
         shift=0.0,
         last_cycle=None,
@@ -3063,15 +2997,15 @@
         logging.debug("*** OLD EXPORT-CYCLES METHOD***")
         lastname = "_cycles.csv"
         if sep is None:
             sep = self.sep
         if outname is None:
             outname = setname + lastname
 
-        list_of_cycles = self.get_cycle_numbers(dataset_number=dataset_number)
+        list_of_cycles = self.get_cycle_numbers()
         logging.debug(f"you have {len(list_of_cycles)} cycles")
         if last_cycle is not None:
             list_of_cycles = [c for c in list_of_cycles if c <= int(last_cycle)]
             logging.debug(f"only processing up to cycle {last_cycle}")
             logging.debug(f"you have {len(list_of_cycles)}" f"cycles to process")
         out_data = []
         c = None
@@ -3083,17 +3017,15 @@
             _last = 0.0
 
         for cycle in list_of_cycles:
             try:
                 if shifted and c is not None:
                     shift = _last
                     # print(f"shifted = {shift}, first={_first}")
-                c, v = self.get_cap(
-                    cycle, dataset_number=dataset_number, method=method, shift=shift
-                )
+                c, v = self.get_cap(cycle, method=method, shift=shift)
                 if c is None:
                     logging.debug("NoneType from get_cap")
                 else:
                     _last = c.iat[-1]
                     _first = c.iat[0]
 
                     c = c.tolist()
@@ -3188,260 +3120,288 @@
     ):
         """Saves the data as .csv file(s).
 
         Args:
             datadir: folder where to save the data (uses current folder if not
                 given).
             sep: the separator to use in the csv file
-                (defaults to CellpyData.sep).
+                (defaults to CellpyCell.sep).
             cycles: (bool) export voltage-capacity curves if True.
             raw: (bool) export raw-data if True.
             summary: (bool) export summary if True.
             shifted (bool): export with cumulated shift.
-            method (string): how the curves are given
-                "back-and-forth" - standard back and forth; discharge
-                    (or charge) reversed from where charge (or
-                    discharge) ends.
+            method (string): how the curves are given::
+
+                "back-and-forth" - standard back and forth; discharge (or charge)
+                    reversed from where charge (or discharge) ends.
+
                 "forth" - discharge (or charge) continues along x-axis.
-                "forth-and-forth" - discharge (or charge) also starts at 0 (or
-                    shift if not shift=0.0)
+
+                "forth-and-forth" - discharge (or charge) also starts at 0
+                    (or shift if not shift=0.0)
+
             shift: start-value for charge (or discharge)
             last_cycle: process only up to this cycle (if not None).
 
-        Returns: Nothing
+        Returns:
+            Nothing
 
         """
 
         if sep is None:
             sep = self.sep
 
         logging.debug("saving to csv")
 
-        dataset_number = -1
-        for data in self.cells:
-            dataset_number += 1
-            if not self._is_not_empty_dataset(data):
-                logging.info("to_csv -")
-                logging.info("empty test [%i]" % dataset_number)
-                logging.info("not saved!")
+        try:
+            data = self.data
+        except NoDataFound:
+            logging.info("to_csv -")
+            logging.info("NoDataFound: not saved!")
+            return
+
+        if isinstance(data.loaded_from, (list, tuple)):
+            txt = "merged file"
+            txt += "using first file as basename"
+            logging.debug(txt)
+            no_merged_sets = len(data.loaded_from)
+            no_merged_sets = "_merged_" + str(no_merged_sets).zfill(3)
+            filename = data.loaded_from[0]
+        else:
+            filename = data.loaded_from
+            no_merged_sets = ""
+
+        firstname, extension = os.path.splitext(filename)
+        firstname += no_merged_sets
+        if datadir:
+            firstname = os.path.join(datadir, os.path.basename(firstname))
+
+        if raw:
+            outname_normal = firstname + "_normal.csv"
+            self._export_normal(data, outname=outname_normal, sep=sep)
+            if data.has_steps is True:
+                outname_steps = firstname + "_steps.csv"
+                self._export_steptable(data, outname=outname_steps, sep=sep)
             else:
-                if isinstance(data.loaded_from, (list, tuple)):
-                    txt = "merged file"
-                    txt += "using first file as basename"
-                    logging.debug(txt)
-                    no_merged_sets = len(data.loaded_from)
-                    no_merged_sets = "_merged_" + str(no_merged_sets).zfill(3)
-                    filename = data.loaded_from[0]
-                else:
-                    filename = data.loaded_from
-                    no_merged_sets = ""
-                firstname, extension = os.path.splitext(filename)
-                firstname += no_merged_sets
-                if datadir:
-                    firstname = os.path.join(datadir, os.path.basename(firstname))
-
-                if raw:
-                    outname_normal = firstname + "_normal.csv"
-                    self._export_normal(data, outname=outname_normal, sep=sep)
-                    if data.has_steps is True:
-                        outname_steps = firstname + "_steps.csv"
-                        self._export_steptable(data, outname=outname_steps, sep=sep)
-                    else:
-                        logging.debug("steps_made is not True")
+                logging.debug("steps_made is not True")
 
-                if summary:
-                    outname_stats = firstname + "_stats.csv"
-                    self._export_stats(data, outname=outname_stats, sep=sep)
-
-                if cycles:
-                    outname_cycles = firstname + "_cycles.csv"
-                    self._export_cycles(
-                        outname=outname_cycles,
-                        dataset_number=dataset_number,
-                        sep=sep,
-                        shifted=shifted,
-                        method=method,
-                        shift=shift,
-                        last_cycle=last_cycle,
-                    )
+        if summary:
+            outname_stats = firstname + "_stats.csv"
+            self._export_stats(data, outname=outname_stats, sep=sep)
+
+        if cycles:
+            outname_cycles = firstname + "_cycles.csv"
+            self._export_cycles(
+                outname=outname_cycles,
+                sep=sep,
+                shifted=shifted,
+                method=method,
+                shift=shift,
+                last_cycle=last_cycle,
+            )
 
     def save(
         self,
         filename,
-        dataset_number=None,
         force=False,
         overwrite=None,
         extension="h5",
         ensure_step_table=None,
+        ensure_summary_table=None,
     ):
         """Save the data structure to cellpy-format.
 
         Args:
             filename: (str or pathlib.Path) the name you want to give the file
-            dataset_number: (int) if you have several datasets, chose the one
-                you want (probably leave this untouched)
             force: (bool) save a file even if the summary is not made yet
                 (not recommended)
             overwrite: (bool) save the new version of the file even if old one
                 exists.
             extension: (str) filename extension.
             ensure_step_table: (bool) make step-table if missing.
+            ensure_summary_table: (bool) make summary-table if missing.
 
         Returns: Nothing at all.
         """
         logging.debug(f"Trying to save cellpy-file to {filename}")
         logging.info(f" -> {filename}")
 
+        cellpy_file_format = "hdf5"
+
+        # some checks to find out what you want
         if overwrite is None:
             overwrite = self.overwrite_able
 
         if ensure_step_table is None:
             ensure_step_table = self.ensure_step_table
 
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            logging.info("Saving test failed!")
-            self._report_empty_dataset()
-            return
+        if ensure_summary_table is None:
+            ensure_summary_table = self.ensure_summary_table
 
-        test = self.cell
-        summary_made = test.has_summary
-
-        if not summary_made and not force:
+        my_data = self.data
+        summary_made = my_data.has_summary
+        if not summary_made and not force and not ensure_summary_table:
+            logging.info("File not saved!")
             logging.info("You should not save datasets without making a summary first!")
             logging.info("If you really want to do it, use save with force=True")
             return
 
-        step_table_made = test.has_steps
+        step_table_made = my_data.has_steps
         if not step_table_made and not force and not ensure_step_table:
             logging.info(
+                "File not saved!"
                 "You should not save datasets without making a step-table first!"
             )
             logging.info("If you really want to do it, use save with force=True")
             return
 
-        outfile_all = Path(filename)
+        outfile_all = OtherPath(filename)
         if not outfile_all.suffix:
+            logging.debug("No suffix given - adding one")
             outfile_all = outfile_all.with_suffix(f".{extension}")
 
-        if os.path.isfile(outfile_all):
+        if outfile_all.is_file():
             logging.debug("Outfile exists")
             if overwrite:
                 logging.debug("overwrite = True")
                 try:
                     os.remove(outfile_all)
                 except PermissionError as e:
                     logging.critical("Could not over write old file")
                     logging.info(e)
                     return
             else:
-                logging.critical("Save (hdf5): file exist - did not save", end=" ")
+                logging.critical("File exists - did not save")
                 logging.info(outfile_all)
                 return
 
         if ensure_step_table:
             logging.debug("ensure_step_table is on")
-            if not test.has_steps:
+            if not my_data.has_steps:
                 logging.debug("save: creating step table")
-                self.make_step_table(dataset_number=dataset_number)
+                self.make_step_table()
 
-        # This method can probably be updated using pandas transpose trick
-        logging.debug("trying to make infotable")
-        infotbl, fidtbl = self._create_infotable(dataset_number=dataset_number)
+        if ensure_summary_table:
+            logging.debug("ensure_summary_table is on")
+            if not my_data.has_summary:
+                logging.debug("save: creating summary table")
+                self.make_summary_table()
 
-        root = prms._cellpyfile_root
+        logging.debug("trying to make infotable")
+        (
+            common_meta_table,
+            test_dependent_meta_table,
+            fid_table,
+        ) = self._create_infotable()
+
+        logging.debug(f"trying to save to file: {outfile_all}")
+        if cellpy_file_format == "hdf5":
+            # --- saving to hdf5 -----------------------------------
+            root = prms._cellpyfile_root  # noqa
+            raw_dir = prms._cellpyfile_raw  # noqa
+            step_dir = prms._cellpyfile_step  # noqa
+            summary_dir = prms._cellpyfile_summary  # noqa
+            common_meta_dir = prms._cellpyfile_common_meta  # noqa
+            fid_dir = prms._cellpyfile_fid  # noqa
+            test_dependent_meta_dir = prms._cellpyfile_test_dependent_meta  # noqa
+            warnings.simplefilter("ignore", PerformanceWarning)
+            try:
+                with pickle_protocol(PICKLE_PROTOCOL):
+                    store = self._save_to_hdf5(
+                        fid_dir,
+                        fid_table,
+                        common_meta_table,
+                        common_meta_dir,
+                        test_dependent_meta_table,
+                        test_dependent_meta_dir,
+                        my_data,
+                        outfile_all,
+                        raw_dir,
+                        root,
+                        step_dir,
+                        summary_dir,
+                    )
+            finally:
+                store.close()
+            logging.debug(" all -> hdf5 OK")
+            warnings.simplefilter("default", PerformanceWarning)
+            # del store
+        # --- finished saving to hdf5 -------------------------------
 
-        if CELLPY_FILE_VERSION > 4:
-            raw_dir = prms._cellpyfile_raw
-            step_dir = prms._cellpyfile_step
-            summary_dir = prms._cellpyfile_summary
-            meta_dir = "/info"
-            fid_dir = prms._cellpyfile_fid
-
-        else:
-            raw_dir = "/raw"
-            step_dir = "/step_table"
-            summary_dir = "/dfsummary"
-            meta_dir = "/info"
-            fid_dir = "/fidtable"
+    def _save_to_hdf5(
+        self,
+        fid_dir,
+        fid_table,
+        infotbl,
+        meta_dir,
+        test_dependent_meta_table,
+        test_dependent_meta_dir,
+        my_data,
+        outfile_all,
+        raw_dir,
+        root,
+        step_dir,
+        summary_dir,
+    ):
+        store = pd.HDFStore(
+            outfile_all,
+            complib=prms._cellpyfile_complib,
+            complevel=prms._cellpyfile_complevel,
+        )
+        logging.debug("trying to put raw data")
+        logging.debug(" - lets set Data_Point as index")
+        hdr_data_point = self.headers_normal.data_point_txt
+        if my_data.raw.index.name != hdr_data_point:
+            my_data.raw = my_data.raw.set_index(hdr_data_point, drop=False)
+        store.put(root + raw_dir, my_data.raw, format=prms._cellpyfile_raw_format)
+        logging.debug(" raw -> hdf5 OK")
+        logging.debug("trying to put summary")
+        store.put(
+            root + summary_dir,
+            my_data.summary,
+            format=prms._cellpyfile_summary_format,
+        )
+        logging.debug(" summary -> hdf5 OK")
 
-        logging.debug("trying to save to hdf5")
-        txt = "\nHDF5 file: %s" % outfile_all
-        logging.debug(txt)
+        logging.debug("trying to put meta data")
+        store.put(root + meta_dir, infotbl, format=prms._cellpyfile_infotable_format)
+        logging.debug(" common meta -> hdf5 OK")
+        store.put(
+            root + test_dependent_meta_dir,
+            test_dependent_meta_table,
+            format=prms._cellpyfile_infotable_format,
+        )
+        logging.debug(" test dependent meta -> hdf5 OK")
 
-        warnings.simplefilter("ignore", PerformanceWarning)
+        logging.debug("trying to put fidtable")
+        store.put(root + fid_dir, fid_table, format=prms._cellpyfile_fidtable_format)
+        logging.debug(" fid -> hdf5 OK")
+        logging.debug("trying to put step")
         try:
-            with pickle_protocol(4):
-                store = pd.HDFStore(
-                    outfile_all,
-                    complib=prms._cellpyfile_complib,
-                    complevel=prms._cellpyfile_complevel,
-                )
-
-                logging.debug("trying to put raw data")
-
-                logging.debug(" - lets set Data_Point as index")
-
-                hdr_data_point = self.headers_normal.data_point_txt
-
-                if test.raw.index.name != hdr_data_point:
-                    test.raw = test.raw.set_index(hdr_data_point, drop=False)
-
-                store.put(root + raw_dir, test.raw, format=prms._cellpyfile_raw_format)
-                logging.debug(" raw -> hdf5 OK")
-
-                logging.debug("trying to put summary")
-                store.put(
-                    root + summary_dir,
-                    test.summary,
-                    format=prms._cellpyfile_summary_format,
-                )
-                logging.debug(" summary -> hdf5 OK")
-
-                logging.debug("trying to put meta data")
-                store.put(
-                    root + meta_dir, infotbl, format=prms._cellpyfile_infotable_format
-                )
-                logging.debug(" meta -> hdf5 OK")
-
-                logging.debug("trying to put fidtable")
-                store.put(
-                    root + fid_dir, fidtbl, format=prms._cellpyfile_fidtable_format
-                )
-                logging.debug(" fid -> hdf5 OK")
-
-                logging.debug("trying to put step")
-                try:
-                    store.put(
-                        root + step_dir,
-                        test.steps,
-                        format=prms._cellpyfile_stepdata_format,
-                    )
-                    logging.debug(" step -> hdf5 OK")
-                except TypeError:
-                    test = self._fix_dtype_step_table(test)
-                    store.put(
-                        root + step_dir,
-                        test.steps,
-                        format=prms._cellpyfile_stepdata_format,
-                    )
-                    logging.debug(" fixed step -> hdf5 OK")
-
-                # creating indexes
-                # hdr_data_point = self.headers_normal.data_point_txt
-                # hdr_cycle_steptable = self.headers_step_table.cycle
-                # hdr_cycle_normal = self.headers_normal.cycle_index_txt
-
-                # store.create_table_index(root + "/raw", columns=[hdr_data_point],
-                #                          optlevel=9, kind='full')
-        finally:
-            store.close()
-        logging.debug(" all -> hdf5 OK")
-        warnings.simplefilter("default", PerformanceWarning)
-        # del store
+            store.put(
+                root + step_dir,
+                my_data.steps,
+                format=prms._cellpyfile_stepdata_format,
+            )
+            logging.debug(" step -> hdf5 OK")
+        except TypeError:
+            my_data = self._fix_dtype_step_table(my_data)
+            store.put(
+                root + step_dir,
+                my_data.steps,
+                format=prms._cellpyfile_stepdata_format,
+            )
+            logging.debug(" fixed step -> hdf5 OK")
+        # creating indexes
+        # hdr_data_point = self.headers_normal.data_point_txt
+        # hdr_cycle_steptable = self.headers_step_table.cycle
+        # hdr_cycle_normal = self.headers_normal.cycle_index_txt
+        # store.create_table_index(root + "/raw", columns=[hdr_data_point],
+        #                          optlevel=9, kind='full')
+        return store
 
     # --------------helper-functions--------------------------------------------
     def _fix_dtype_step_table(self, dataset):
         hst = get_headers_step_table()
         try:
             cols = dataset.steps.columns
         except AttributeError:
@@ -3462,63 +3422,53 @@
         charge_title = self.headers_normal.charge_capacity_txt
         chargecap = 0.0
         dischargecap = 0.0
 
         # TODO: @jepe - use pd.loc[row,column]
 
         if capacity_modifier == "reset":
-
             for index, row in summary.iterrows():
                 dischargecap_2 = row[discharge_title]
                 summary.loc[index, discharge_title] = dischargecap_2 - dischargecap
                 dischargecap = dischargecap_2
                 chargecap_2 = row[charge_title]
                 summary.loc[index, charge_title] = chargecap_2 - chargecap
                 chargecap = chargecap_2
         else:
             raise NotImplementedError
 
         logging.debug(f"(dt: {(time.time() - time_00):4.2f}s)")
         return summary
 
     # TODO: check if this is useful and if it is rename, if not delete
-    def _cap_mod_normal(
-        self, dataset_number=None, capacity_modifier="reset", allctypes=True
-    ):
+    def _cap_mod_normal(self, capacity_modifier="reset", allctypes=True):
         # modifies the normal table
         time_00 = time.time()
         logging.debug("Not properly checked yet! Use with caution!")
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
         cycle_index_header = self.headers_normal.cycle_index_txt
         step_index_header = self.headers_normal.step_index_txt
         discharge_index_header = self.headers_normal.discharge_capacity_txt
         discharge_energy_index_header = self.headers_normal.discharge_energy_txt
         charge_index_header = self.headers_normal.charge_capacity_txt
         charge_energy_index_header = self.headers_normal.charge_energy_txt
 
-        raw = self.cells[dataset_number].raw
+        raw = self.data.raw
 
         chargecap = 0.0
         dischargecap = 0.0
 
         if capacity_modifier == "reset":
             # discharge cycles
             no_cycles = np.amax(raw[cycle_index_header])
             for j in range(1, no_cycles + 1):
                 cap_type = "discharge"
                 e_header = discharge_energy_index_header
                 cap_header = discharge_index_header
                 discharge_cycles = self.get_step_numbers(
-                    steptype=cap_type,
-                    allctypes=allctypes,
-                    cycle_number=j,
-                    dataset_number=dataset_number,
+                    steptype=cap_type, allctypes=allctypes, cycle_number=j
                 )
 
                 steps = discharge_cycles[j]
                 txt = "Cycle  %i (discharge):  " % j
                 logging.debug(txt)
                 # TODO: @jepe - use pd.loc[row,column] e.g. pd.loc[:,"charge_cap"]
                 # for col or pd.loc[(pd.["step"]==1),"x"]
@@ -3530,18 +3480,15 @@
                 raw.loc[selection, cap_header] = raw.loc[selection, cap_header] - c0
                 raw.loc[selection, e_header] = raw.loc[selection, e_header] - e0
 
                 cap_type = "charge"
                 e_header = charge_energy_index_header
                 cap_header = charge_index_header
                 charge_cycles = self.get_step_numbers(
-                    steptype=cap_type,
-                    allctypes=allctypes,
-                    cycle_number=j,
-                    dataset_number=dataset_number,
+                    steptype=cap_type, allctypes=allctypes, cycle_number=j
                 )
                 steps = charge_cycles[j]
                 txt = "Cycle  %i (charge):  " % j
                 logging.debug(txt)
 
                 selection = (raw[cycle_index_header] == j) & (
                     raw[step_index_header].isin(steps)
@@ -3550,183 +3497,147 @@
                 if any(selection):
                     c0 = raw[selection].iloc[0][cap_header]
                     e0 = raw[selection].iloc[0][e_header]
                     raw.loc[selection, cap_header] = raw.loc[selection, cap_header] - c0
                     raw.loc[selection, e_header] = raw.loc[selection, e_header] - e0
         logging.debug(f"(dt: {(time.time() - time_00):4.2f}s)")
 
-    def get_number_of_tests(self):
-        return self.number_of_datasets
+    def get_mass(self):
+        return self.data.meta_common.mass
 
-    def get_mass(self, set_number=None):
-        set_number = self._validate_dataset_number(set_number)
-        if set_number is None:
-            self._report_empty_dataset()
-            return
-        if not self.cells[set_number].mass_given:
-            logging.info("No mass")
-        return self.cells[set_number].mass
-
-    def get_cell(self, n=0):
-        # TODO: remove me
-        return self.cells[n]
-
-    def sget_voltage(self, cycle, step, dataset_number=None):
+    def sget_voltage(self, cycle, step):
         """Returns voltage for cycle, step.
 
         Convenience function; same as issuing
            raw[(raw[cycle_index_header] == cycle) &
                  (raw[step_index_header] == step)][voltage_header]
 
         Args:
             cycle: cycle number
             step: step number
-            dataset_number: the dataset number (automatic selection if None)
 
         Returns:
             pandas.Series or None if empty
         """
         header = self.headers_normal.voltage_txt
-        return self._sget(
-            cycle, step, header, usteps=False, dataset_number=dataset_number
-        )
+        return self._sget(cycle, step, header, usteps=False)
 
-    def sget_current(self, cycle, step, dataset_number=None):
+    def sget_current(self, cycle, step):
         """Returns current for cycle, step.
 
         Convenience function; same as issuing
            raw[(raw[cycle_index_header] == cycle) &
                  (raw[step_index_header] == step)][current_header]
 
         Args:
             cycle: cycle number
             step: step number
-            dataset_number: the dataset number (automatic selection if None)
 
         Returns:
             pandas.Series or None if empty
         """
         header = self.headers_normal.current_txt
-        return self._sget(
-            cycle, step, header, usteps=False, dataset_number=dataset_number
-        )
+        return self._sget(cycle, step, header, usteps=False)
 
-    def get_voltage(self, cycle=None, dataset_number=None, full=True):
+    def get_voltage(self, cycle=None, full=True):
         """Returns voltage (in V).
 
         Args:
             cycle: cycle number (all cycles if None)
-            dataset_number: first dataset if None
             full: valid only for cycle=None (i.e. all cycles), returns the full
                pandas.Series if True, else a list of pandas.Series
 
         Returns:
             pandas.Series (or list of pandas.Series if cycle=None og full=False)
         """
 
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
         cycle_index_header = self.headers_normal.cycle_index_txt
         voltage_header = self.headers_normal.voltage_txt
         # step_index_header  = self.headers_normal.step_index_txt
 
-        test = self.cells[dataset_number].raw
+        data = self.data.raw
         if cycle:
             logging.debug("getting voltage curve for cycle")
-            c = test[(test[cycle_index_header] == cycle)]
-            if not self.is_empty(c):
+            c = data[(data[cycle_index_header] == cycle)]
+            if not self._is_empty_array(c):
                 v = c[voltage_header]
                 return v
         else:
             if not full:
                 logging.debug("getting list of voltage-curves for all cycles")
                 v = []
-                no_cycles = np.amax(test[cycle_index_header])
+                no_cycles = np.amax(data[cycle_index_header])
                 for j in range(1, no_cycles + 1):
                     txt = "Cycle  %i:  " % j
                     logging.debug(txt)
-                    c = test[(test[cycle_index_header] == j)]
+                    c = data[(data[cycle_index_header] == j)]
                     v.append(c[voltage_header])
             else:
                 logging.debug("getting frame of all voltage-curves")
-                v = test[voltage_header]
+                v = data[voltage_header]
             return v
 
-    def get_current(self, cycle=None, dataset_number=None, full=True):
+    def get_current(self, cycle=None, full=True):
         """Returns current (in mA).
 
         Args:
             cycle: cycle number (all cycles if None)
-            dataset_number: first dataset if None
             full: valid only for cycle=None (i.e. all cycles), returns the full
                pandas.Series if True, else a list of pandas.Series
 
         Returns:
             pandas.Series (or list of pandas.Series if cycle=None og full=False)
         """
 
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
         cycle_index_header = self.headers_normal.cycle_index_txt
         current_header = self.headers_normal.current_txt
         # step_index_header  = self.headers_normal.step_index_txt
 
-        test = self.cells[dataset_number].raw
+        data = self.data.raw
         if cycle:
             logging.debug(f"getting current for cycle {cycle}")
-            c = test[(test[cycle_index_header] == cycle)]
-            if not self.is_empty(c):
+            c = data[(data[cycle_index_header] == cycle)]
+            if not self._is_empty_array(c):
                 v = c[current_header]
                 return v
         else:
             if not full:
                 logging.debug("getting a list of current-curves for all cycles")
                 v = []
-                no_cycles = np.amax(test[cycle_index_header])
+                no_cycles = np.amax(data[cycle_index_header])
                 for j in range(1, no_cycles + 1):
                     txt = "Cycle  %i:  " % j
                     logging.debug(txt)
-                    c = test[(test[cycle_index_header] == j)]
+                    c = data[(data[cycle_index_header] == j)]
                     v.append(c[current_header])
             else:
                 logging.debug("getting all current-curves ")
-                v = test[current_header]
+                v = data[current_header]
             return v
 
-    def sget_steptime(self, cycle, step, dataset_number=None):
+    def sget_steptime(self, cycle, step):
         """Returns step time for cycle, step.
 
         Convenience function; same as issuing
            raw[(raw[cycle_index_header] == cycle) &
                  (raw[step_index_header] == step)][step_time_header]
 
         Args:
             cycle: cycle number
             step: step number
-            dataset_number: the dataset number (automatic selection if None)
 
         Returns:
             pandas.Series or None if empty
         """
 
         header = self.headers_normal.step_time_txt
-        return self._sget(
-            cycle, step, header, usteps=False, dataset_number=dataset_number
-        )
+        return self._sget(cycle, step, header, usteps=False)
 
-    def _sget(self, cycle, step, header, usteps=False, dataset_number=None):
-        dataset_number = self._validate_dataset_number(dataset_number)
+    def _sget(self, cycle, step, header, usteps=False):
         logging.debug(f"searching for {header}")
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
 
         cycle_index_header = self.headers_normal.cycle_index_txt
         step_index_header = self.headers_normal.step_index_txt
 
         if usteps:
             print("Using sget for usteps is not supported yet.")
             print("I encourage you to work with the DataFrames directly instead.")
@@ -3735,80 +3646,69 @@
             print(" - look up the start and end 'data_point' in the raw DataFrame")
             print("")
             print(
                 "(Just remember to run make_step_table with the all_steps set to True before you do it)"
             )
             return
 
-        test = self.cells[dataset_number].raw
+        test = self.data.raw
 
         if not isinstance(step, (list, tuple)):
             step = [step]
 
         return test.loc[
             (test[cycle_index_header] == cycle) & (test[step_index_header].isin(step)),
             header,
         ].reset_index(drop=True)
 
-    def sget_timestamp(self, cycle, step, dataset_number=None):
+    def sget_timestamp(self, cycle, step):
         """Returns timestamp for cycle, step.
 
         Convenience function; same as issuing
            raw[(raw[cycle_index_header] == cycle) &
                  (raw[step_index_header] == step)][timestamp_header]
 
         Args:
             cycle: cycle number
             step: step number (can be a list of several step numbers)
-            dataset_number: the dataset number (automatic selection if None)
 
         Returns:
             pandas.Series
         """
 
         header = self.headers_normal.test_time_txt
-        return self._sget(
-            cycle, step, header, usteps=False, dataset_number=dataset_number
-        )
+        return self._sget(cycle, step, header, usteps=False)
 
-    def sget_step_numbers(self, cycle, step, dataset_number=None):
+    def sget_step_numbers(self, cycle, step):
         """Returns step number for cycle, step.
 
         Convenience function; same as issuing
            raw[(raw[cycle_index_header] == cycle) &
                  (raw[step_index_header] == step)][step_index_header]
 
         Args:
             cycle: cycle number
             step: step number (can be a list of several step numbers)
-            dataset_number: the dataset number (automatic selection if None)
 
         Returns:
             pandas.Series
         """
 
         header = self.headers_normal.step_index_txt
-        return self._sget(
-            cycle, step, header, usteps=False, dataset_number=dataset_number
-        )
-
-    def get_datetime(self, cycle=None, dataset_number=None, full=True):
+        return self._sget(cycle, step, header, usteps=False)
 
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
+    def get_datetime(self, cycle=None, full=True):
         cycle_index_header = self.headers_normal.cycle_index_txt
         datetime_header = self.headers_normal.datetime_txt
 
         v = pd.Series()
-        test = self.cells[dataset_number].raw
+        test = self.data.raw
         if cycle:
             c = test[(test[cycle_index_header] == cycle)]
-            if not self.is_empty(c):
+            if not self._is_empty_array(c):
                 v = c[datetime_header]
 
         else:
             if not full:
                 logging.debug("getting datetime for all cycles")
                 v = []
                 cycles = self.get_cycle_numbers()
@@ -3818,42 +3718,35 @@
                     c = test[(test[cycle_index_header] == j)]
                     v.append(c[datetime_header])
             else:
                 logging.debug("returning full datetime col")
                 v = test[datetime_header]
         return v
 
-    def get_timestamp(
-        self, cycle=None, dataset_number=None, in_minutes=False, full=True
-    ):
+    def get_timestamp(self, cycle=None, in_minutes=False, full=True):
         """Returns timestamps (in sec or minutes (if in_minutes==True)).
 
         Args:
             cycle: cycle number (all if None)
-            dataset_number: first dataset if None
             in_minutes: return values in minutes instead of seconds if True
             full: valid only for cycle=None (i.e. all cycles), returns the full
                pandas.Series if True, else a list of pandas.Series
 
         Returns:
             pandas.Series (or list of pandas.Series if cycle=None og full=False)
         """
 
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
         cycle_index_header = self.headers_normal.cycle_index_txt
         timestamp_header = self.headers_normal.test_time_txt
 
         v = pd.Series()
-        test = self.cells[dataset_number].raw
+        test = self.data.raw
         if cycle:
             c = test[(test[cycle_index_header] == cycle)]
-            if not self.is_empty(c):
+            if not self._is_empty_array(c):
                 v = c[timestamp_header]
 
         else:
             if not full:
                 logging.debug("getting timestapm for all cycles")
                 v = []
                 cycles = self.get_cycle_numbers()
@@ -3867,57 +3760,81 @@
                 v = test[timestamp_header]
                 if in_minutes and v is not None:
                     v /= 60.0
         if in_minutes and v is not None:
             v /= 60.0
         return v
 
-    def get_dcap(self, cycle=None, dataset_number=None, converter=None, **kwargs):
-        """Returns discharge_capacity (in mAh/g), and voltage."""
+    def get_dcap(
+        self,
+        cycle=None,
+        converter=None,
+        mode="gravimetric",
+        return_dataframe=False,
+        **kwargs,
+    ):
+        """Returns discharge_capacity and voltage for the selected cycle
+        Args:
+            cycle (int): cycle number.
+            converter (string): defaults to None.
+            mode (string): defaults to "gravimetric".
+            return_dataframe (bool): if True: returns pd.DataFrame instead of capacity, voltage series.
+        Returns:
+            discharge_capacity, voltage (pd.Series).
 
+        """
+        # TODO: update docstring on purpose of "converter" and "mode"
         #  TODO - jepe: should return a DataFrame as default
         #   but remark that we then have to update e.g. batch_helpers.py
-        #  TODO - jepe: change needed: should not use
-        #   dataset_number as parameter
 
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
         if converter is None:
-            converter = self.get_converter_to_specific()
+            converter = self.get_converter_to_specific(mode=mode)
 
-        dc, v = self._get_cap(
-            cycle, dataset_number, "discharge", converter=converter, **kwargs
-        )
-        return dc, v
+        dc, v = self._get_cap(cycle, "discharge", converter=converter, **kwargs)
+        if return_dataframe:
+            cycle_df = pd.concat([v, dc], axis=1)
+            return cycle_df
+        else:
+            return dc, v
 
-    def get_ccap(self, cycle=None, dataset_number=None, converter=None, **kwargs):
-        """Returns charge_capacity (in mAh/g), and voltage."""
+    def get_ccap(
+        self,
+        cycle=None,
+        converter=None,
+        mode="gravimetric",
+        return_dataframe=False,
+        **kwargs,
+    ):
+        """Returns charge_capacity and voltage for the selected cycle.
+        Args:
+            cycle (int): cycle number.
+            converter (string): defaults to None.
+            mode (string): defaults to "gravimetric".
+            return_dataframe (bool): if True: returns pd.DataFrame instead of capacity, voltage series.
+        Returns:
+            charge_capacity, voltage (pd.Series).
 
-        #  TODO - jepe: should return a DataFrame as default
+        """
+        # TODO: update docstring on purpose of "converter" and "mode"
+        # TODO - jepe: should return a DataFrame as default
         #   (but remark that we then have to update e.g. batch_helpers.py)
-        #  TODO - jepe: change needed: should not use
-        #   dataset_number as parameter
 
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
         if converter is None:
-            converter = self.get_converter_to_specific()
-        cc, v = self._get_cap(
-            cycle, dataset_number, "charge", converter=converter, **kwargs
-        )
-        return cc, v
+            converter = self.get_converter_to_specific(mode=mode)
+        cc, v = self._get_cap(cycle, "charge", converter=converter, **kwargs)
+
+        if return_dataframe:
+            cycle_df = pd.concat([v, cc], axis=1)
+            return cycle_df
+        else:
+            return cc, v
 
     def get_cap(
         self,
         cycle=None,
-        dataset_number=None,
         method="back-and-forth",
         insert_nan=None,
         shift=0.0,
         categorical_column=False,
         label_cycle_number=False,
         split=False,
         interpolated=False,
@@ -3931,26 +3848,24 @@
     ):
         """Gets the capacity for the run.
 
         Args:
             cycle (int): cycle number.
             method (string): how the curves are given
                 "back-and-forth" - standard back and forth; discharge
-                    (or charge) reversed from where charge (or discharge) ends.
+                (or charge) reversed from where charge (or discharge) ends.
                 "forth" - discharge (or charge) continues along x-axis.
                 "forth-and-forth" - discharge (or charge) also starts at 0
-                    (or shift if not shift=0.0)
+                (or shift if not shift=0.0)
             insert_nan (bool): insert a np.nan between the charge and discharge curves.
                 Defaults to True for "forth-and-forth", else False
             shift: start-value for charge (or discharge) (typically used when
                 plotting shifted-capacity).
             categorical_column: add a categorical column showing if it is
                 charge or discharge.
-            dataset_number (int): test number (default first)
-                (usually not used).
             label_cycle_number (bool): add column for cycle number
                 (tidy format).
             split (bool): return a list of c and v instead of the default
                 that is to return them combined in a DataFrame. This is only
                 possible for some specific combinations of options (neither
                 categorical_column=True or label_cycle_number=True are
                 allowed).
@@ -3966,26 +3881,21 @@
             inter_cycle_shift (bool): cumulative shifts between consecutive
                 cycles. Defaults to True.
             interpolate_along_cap (bool): interpolate along capacity axis instead
                 of along the voltage axis. Defaults to False.
 
         Returns:
             pandas.DataFrame ((cycle) voltage, capacity, (direction (-1, 1)))
-                unless split is explicitly set to True. Then it returns a tuple
-                with capacity (mAh/g) and voltage.
+            unless split is explicitly set to True. Then it returns a tuple
+            with capacity (mAh/g) and voltage.
         """
 
         # TODO: allow for fixing the interpolation range (so that it is possible
         #   to run the function on several cells and have a common x-axis
 
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
-
         # if cycle is not given, then this function should
         # iterate through cycles
         if cycle is None:
             cycle = self.get_cycle_numbers()
 
         if not isinstance(cycle, collections.abc.Iterable):
             cycle = [cycle]
@@ -4016,21 +3926,19 @@
 
         initial = True
         for current_cycle in cycle:
             error = False
             try:
                 cc, cv = self.get_ccap(
                     current_cycle,
-                    dataset_number,
                     converter=specific_converter,
                     **kwargs,
                 )
                 dc, dv = self.get_dcap(
                     current_cycle,
-                    dataset_number,
                     converter=specific_converter,
                     **kwargs,
                 )
 
             except NullData as e:
                 error = True
                 logging.debug(e)
@@ -4124,15 +4032,14 @@
                         _first_df = pd.DataFrame(
                             {
                                 "voltage": _first_step_v,
                                 "capacity": _first_step_c,
                             }
                         )
                         if interpolated:
-
                             _first_df = interpolate_y_on_x(
                                 _first_df,
                                 y=y_col,
                                 x=x_col,
                                 dx=dx,
                                 number_of_points=number_of_points,
                                 direction=first_interpolation_direction,
@@ -4197,58 +4104,57 @@
             return cycle_df
         else:
             return capacity, voltage
 
     def _get_cap(
         self,
         cycle=None,
-        dataset_number=None,
         cap_type="charge",
         trim_taper_steps=None,
         steps_to_skip=None,
         steptable=None,
         converter=None,
+        usteps=False,
     ):
+        if usteps:
+            print(
+                "Unfortunately, the ustep functionality is not implemented in this version of cellpy"
+            )
+            raise NotImplementedError("ustep == True not allowed!")
         # used when extracting capacities (get_ccap, get_dcap)
         # TODO: @jepe - does not allow for constant voltage yet?
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
-        test = self.cells[
-            dataset_number
-        ]  # not used anymore - will be removed when we skip several cells option
+
+        test = self.data
 
         if cap_type == "charge_capacity":
             cap_type = "charge"
         elif cap_type == "discharge_capacity":
             cap_type = "discharge"
 
         cycles = self.get_step_numbers(
             steptype=cap_type,
             allctypes=False,
             cycle_number=cycle,
-            dataset_number=dataset_number,
             trim_taper_steps=trim_taper_steps,
             steps_to_skip=steps_to_skip,
             steptable=steptable,
         )
-
         if cap_type == "charge":
             column_txt = self.headers_normal.charge_capacity_txt
         else:
             column_txt = self.headers_normal.discharge_capacity_txt
         if cycle:
             steps = cycles[cycle]
             _v = []
             _c = []
-
+            if len(set(steps)) < len(steps) and not usteps:
+                raise ValueError(f"You have duplicate step numbers!")
             for step in sorted(steps):
-                selected_step = self._select_step(cycle, step, dataset_number)
-                if not self.is_empty(selected_step):
+                selected_step = self._select_step(cycle, step)
+                if not self._is_empty_array(selected_step):
                     _v.append(selected_step[self.headers_normal.voltage_txt])
                     _c.append(selected_step[column_txt] * converter)
             try:
                 voltage = pd.concat(_v, axis=0)
                 cap = pd.concat(_c, axis=0)
             except Exception:
                 logging.debug("could not find any steps for this cycle")
@@ -4257,65 +4163,63 @@
             # get all the discharge cycles
             # this is a dataframe filtered on step and cycle
             # This functionality is not crucial since get_cap (that uses this method) has it
             # (but it might be nice to improve performance)
             raise NotImplementedError(
                 "Not yet possible to extract without giving cycle numbers (use get_cap instead)"
             )
-
         return cap, voltage
 
     def get_ocv(
         self,
         cycles=None,
         direction="up",
         remove_first=False,
         interpolated=False,
         dx=None,
         number_of_points=None,
     ):
-
         """get the open circuit voltage relaxation curves.
 
         Args:
             cycles (list of ints or None): the cycles to extract from
                 (selects all if not given).
             direction ("up", "down", or "both"): extract only relaxations that
                 is performed during discharge for "up" (because then the
                 voltage relaxes upwards) etc.
             remove_first: remove the first relaxation curve (typically,
                 the first curve is from the initial rest period between
-                assembling the cell to the actual testing/cycling starts)
+                assembling the data to the actual testing/cycling starts)
             interpolated (bool): set to True if you want the data to be
                 interpolated (e.g. for creating smaller files)
             dx (float): the step used when interpolating.
             number_of_points (int): number of points to use (over-rides dx)
                 for interpolation (i.e. the length of the interpolated data).
 
         Returns:
             A pandas.DataFrame with cycle-number, step-number, step-time, and
                 voltage columns.
         """
-
+        # TODO: use proper column header pickers
         if cycles is None:
             cycles = self.get_cycle_numbers()
         else:
             if not isinstance(cycles, (list, tuple, np.ndarray)):
                 cycles = [cycles]
             else:
                 remove_first = False
 
         ocv_rlx_id = "ocvrlx"
         if direction == "up":
             ocv_rlx_id += "_up"
         elif direction == "down":
             ocv_rlx_id += "_down"
 
-        steps = self.cell.steps
-        raw = self.cell.raw
+        steps = self.data.steps
+        raw = self.data.raw
 
         ocv_steps = steps.loc[steps["cycle"].isin(cycles), :]
 
         ocv_steps = ocv_steps.loc[
             ocv_steps.type.str.startswith(ocv_rlx_id, na=False), :
         ]
 
@@ -4323,21 +4227,20 @@
             ocv_steps = ocv_steps.iloc[1:, :]
 
         step_time_label = self.headers_normal.step_time_txt
         voltage_label = self.headers_normal.voltage_txt
         cycle_label = self.headers_normal.cycle_index_txt
         step_label = self.headers_normal.step_index_txt
 
-        selected_df = raw.where(
-            raw[cycle_label].isin(ocv_steps.cycle)
-            & raw[step_label].isin(ocv_steps.step)
-        ).dropna()
-
-        selected_df = selected_df.loc[
-            :, [cycle_label, step_label, step_time_label, voltage_label]
+        selected_df = raw.loc[
+            (
+                raw[cycle_label].isin(ocv_steps.cycle)
+                & raw[step_label].isin(ocv_steps.step)
+            ),
+            [cycle_label, step_label, step_time_label, voltage_label],
         ]
 
         if interpolated:
             if dx is None and number_of_points is None:
                 dx = prms.Reader.time_interpolation_step
             new_dfs = list()
             groupby_list = [cycle_label, step_label]
@@ -4355,46 +4258,37 @@
                     new_group[i] = j
                 new_dfs.append(new_group)
 
             selected_df = pd.concat(new_dfs)
 
         return selected_df
 
-    def get_number_of_cycles(self, dataset_number=None, steptable=None):
+    def get_number_of_cycles(self, steptable=None):
         """Get the number of cycles in the test."""
         if steptable is None:
-            dataset_number = self._validate_dataset_number(dataset_number)
-            if dataset_number is None:
-                self._report_empty_dataset()
-                return
-            d = self.cells[dataset_number].raw
+            d = self.data.raw
             no_cycles = np.amax(d[self.headers_normal.cycle_index_txt])
         else:
             no_cycles = np.amax(steptable[self.headers_step_table.cycle])
         return no_cycles
 
-    def get_cycle_numbers_old(self, dataset_number=None, steptable=None):
+    def get_cycle_numbers_old(self, steptable=None):
         """Get a list containing all the cycle numbers in the test."""
         logging.debug("getting cycle numbers")
         if steptable is None:
-            dataset_number = self._validate_dataset_number(dataset_number)
-            if dataset_number is None:
-                self._report_empty_dataset()
-                return
-            d = self.cells[dataset_number].raw
+            d = self.data.raw
             cycles = d[self.headers_normal.cycle_index_txt].dropna().unique()
         else:
             logging.debug("steptable is not none")
             cycles = steptable[self.headers_step_table.cycle].dropna().unique()
         logging.debug(f"got {len(cycles)} cycle numbers")
         return cycles
 
     def get_cycle_numbers(
         self,
-        dataset_number=None,
         steptable=None,
         rate=None,
         rate_on=None,
         rate_std=None,
         rate_column=None,
         inverse=False,
     ):
@@ -4412,21 +4306,17 @@
 
         Returns:
             numpy.ndarray of cycle numbers.
         """
 
         logging.debug("getting cycle numbers")
         if steptable is None:
-            dataset_number = self._validate_dataset_number(dataset_number)
-            if dataset_number is None:
-                self._report_empty_dataset()
-                return
-            d = self.cells[dataset_number].raw
+            d = self.data.raw
             cycles = d[self.headers_normal.cycle_index_txt].dropna().unique()
-            steptable = self.cells[dataset_number].steps
+            steptable = self.data.steps
         else:
             logging.debug("steptable is given as input parameter")
             cycles = steptable[self.headers_step_table.cycle].dropna().unique()
 
         if rate is None:
             return cycles
 
@@ -4464,146 +4354,274 @@
             cycles_mask = ~cycles_mask
 
         filtered_step_table = steptable[cycles_mask]
         filtered_cycles = filtered_step_table[self.headers_step_table["cycle"]].unique()
 
         return filtered_cycles
 
-    def get_ir(self, dataset_number=None):
+    def get_ir(self):
         """Get the IR data (Deprecated)."""
         raise DeprecatedFeature
 
-    def get_converter_to_specific(
-        self, dataset=None, mass=None, to_unit=None, from_unit=None
+    def nominal_capacity_as_absolute(
+        self,
+        value=None,
+        specific=None,
+        nom_cap_specifics=None,
+        convert_charge_units=False,
     ):
-        """get the conversion value to use when calculating specific values (e.g mAh/g).
+        # TODO: implement handling of edge-cases (i.e. the raw capacity is not in absolute values)
+        if nom_cap_specifics is None:
+            nom_cap_specifics = "gravimetric"
+
+        if specific is None:
+            if nom_cap_specifics == "gravimetric":
+                specific = self.data.mass
+            elif nom_cap_specifics == "areal":
+                specific = self.data.active_electrode_area
+
+        if value is None:
+            value = self.data.nom_cap
+        value = Q(value, self.cellpy_units["nominal_capacity"])
+
+        if nom_cap_specifics == "gravimetric":
+            specific = Q(specific, self.cellpy_units["mass"])
+        elif nom_cap_specifics == "areal":
+            specific = Q(specific, self.cellpy_units["area"])
+
+        if convert_charge_units:
+            conversion_factor_charge = Q(1, self.cellpy_units["charge"]) / Q(
+                1, self.data.raw_units["charge"]
+            )
+        else:
+            conversion_factor_charge = 1.0
+
+        absolute_value = (
+            (value * conversion_factor_charge * specific).to_reduced_units().to("Ah")
+        )
+        return absolute_value.m
+
+    def with_cellpy_unit(self, parameter, as_str=False):
+        """Return quantity as `pint.Quantity` object."""
+        _look_up = {
+            "nom_cap": "nominal_capacity",
+            "active_electrode_area": "area",
+        }
+        _parameter = parameter
+        if parameter in _look_up.keys():
+            _parameter = _look_up[parameter]
+
+        try:
+            _unit = self.cellpy_units[_parameter]
+        except KeyError:
+            print(f"Did not find any units registered for {parameter}")
+            return
+
+        try:
+            _value = getattr(self.data, parameter)
+        except AttributeError:
+            print(
+                f"{parameter} is not a valid cellpy data attribute (but the unit is {_unit})"
+            )
+            return
+
+        if as_str:
+            return f"{_value} {_unit}"
+
+        return Q(_value, _unit)
+
+    def to_cellpy_unit(self, value, physical_property):
+        """Convert value to cellpy units.
 
         Args:
-            dataset: DataSet object
-            mass: mass of electrode (for example active material in mg)
-            to_unit: (float) unit of input, f.ex. if unit of charge
-              is mAh and unit of mass is g, then to_unit for charge/mass
-              will be 0.001 * 1.0 = 0.001
-            from_unit: (float) unit of output, f.ex. if unit of charge
-              is mAh and unit of mass is g, then to_unit for charge/mass
-              will be 1.0 / 0.001 = 1000.0
+            value (numeric, pint.Quantity or str): what you want to convert from
+            physical_property (str): What this value is a measure of
+                (must correspond to one of the keys in the CellpyUnits class).
 
-        Returns:
-            multiplier (float) from_unit / to_unit / mass
+        Returns (numeric):
+            the value in cellpy units
+        """
+        logging.debug(f"value {value} is numeric? {isinstance(value, numbers.Number)}")
+        logging.debug(
+            f"value {value} is a pint quantity? {isinstance(value, Quantity)}"
+        )
+
+        if not isinstance(value, Quantity):
+            if isinstance(value, numbers.Number):
+                try:
+                    value = Q(value, self.data.raw_units[physical_property])
+                    logging.debug(f"With unit from raw-units: {value}")
+                except NoDataFound:
+                    raise NoDataFound(
+                        "If you dont have any cells you cannot convert"
+                        " values to cellpy units without providing what"
+                        " unit to convert from!"
+                    )
+                except KeyError as e:
+                    raise KeyError(
+                        "You have to provide a valid physical_property"
+                    ) from e
+            elif isinstance(value, tuple):
+                value = Q(*value)
+            else:
+                value = Q(value)
+
+        value = value.to(self.cellpy_units[physical_property])
+
+        return value.m
+
+    def get_converter_to_specific(
+        self,
+        dataset: Data = None,
+        value: float = None,
+        from_units: CellpyUnits = None,
+        to_units: CellpyUnits = None,
+        mode: str = "gravimetric",
+    ) -> float:
+        """Convert from absolute units to specific (areal or gravimetric).
+
+        Args:
+            dataset: data instance
+            value: value used to scale on.
+            from_units: defaults to data.raw_units.
+            to_units: defaults to cellpy_units.
+            mode (str): gravimetric, areal or absolute
 
+        Returns:
+            conversion factor (multiply with this to get your values into specific values).
         """
-        if not dataset:
-            dataset_number = self._validate_dataset_number(None)
-            if dataset_number is None:
-                self._report_empty_dataset()
-                return
-            dataset = self.cells[dataset_number]
+        # TODO @jepe: implement handling of edge-cases
+        # TODO @jepe: fix all the instrument readers (replace floats in raw_units with strings)
+        if dataset is None:
+            dataset = self.data
 
-        if not mass:
-            mass = dataset.mass
+        new_units = to_units or self.cellpy_units
+        old_units = from_units or dataset.raw_units
 
-        if not to_unit:
-            to_unit_cap = self.cellpy_units["charge"]
-            to_unit_mass = self.cellpy_units["specific"]
-            to_unit = to_unit_cap / to_unit_mass
-        if not from_unit:
-            from_unit_cap = self.raw_units["charge"]
-            from_unit_mass = self.cellpy_units["mass"]
-            from_unit = from_unit_cap / from_unit_mass
-        logging.debug(f"from-unit: {from_unit}")
-        logging.debug(f"to-unit: {to_unit}")
-        logging.debug(f"mass: {mass}")
-        conversion_factor = from_unit / to_unit / mass
-        logging.debug(f"conversion factor: {conversion_factor}")
+        if mode == "gravimetric":
+            value = value or dataset.mass
+            value = Q(value, new_units["mass"])
+
+            to_unit_specific = Q(1.0, new_units["specific_gravimetric"])
+
+        elif mode == "areal":
+            value = value or dataset.active_electrode_area
+            value = Q(value, new_units["area"])
+            to_unit_specific = Q(1.0, new_units["specific_areal"])
+
+        elif mode == "absolute":
+            value = Q(1.0, None)
+            to_unit_specific = Q(1.0, None)
 
-        return conversion_factor
+        elif mode is None:
+            return 1.0
 
-    def get_diagnostics_plots(self, dataset_number=None, scaled=False):
+        else:
+            logging.debug(f"mode={mode} not supported!")
+            return 1.0
+
+        from_unit_cap = Q(1.0, old_units["charge"])
+        to_unit_cap = Q(1.0, new_units["charge"])
+
+        # from unit is always in absolute values:
+        from_unit = from_unit_cap
+
+        to_unit = to_unit_cap / to_unit_specific
+
+        conversion_factor = (from_unit / to_unit / value).to_reduced_units()
+        logging.debug(f"conversion factor: {conversion_factor}")
+        return conversion_factor.m
+
+    def get_diagnostics_plots(self, scaled=False):
         raise DeprecatedFeature(
             "This feature is deprecated. "
             "Extract diagnostics from the summary instead."
         )
 
-    def _set_mass(self, dataset_number, value):
+    def _set_mass(self, value):
+        # TODO: replace with setter
         try:
-            self.cells[dataset_number].mass = value
-            self.cells[dataset_number].mass_given = True
+            self.data.meta_common.mass = value
         except AttributeError as e:
             logging.info("This test is empty")
             logging.info(e)
 
-    def _set_tot_mass(self, dataset_number, value):
+    def _set_tot_mass(self, value):
+        # TODO: replace with setter
         try:
-            self.cells[dataset_number].tot_mass = value
+            self.data.meta_common.tot_mass = value
         except AttributeError as e:
             logging.info("This test is empty")
             logging.info(e)
 
-    def _set_nom_cap(self, dataset_number, value):
+    def _set_nom_cap(self, value):
+        # TODO: replace with setter
         try:
-            self.cells[dataset_number].nom_cap = value
+            self.data.meta_common.nom_cap = value
         except AttributeError as e:
             logging.info("This test is empty")
             logging.info(e)
 
-    def _set_run_attribute(self, attr, vals, dataset_number=None, validated=None):
+    def _set_run_attribute(self, attr, val, validated=None):
         # Sets the val (vals) for the test (datasets).
         # Remark! This is left-over code from old ages when we thought we needed
         #   to have data-sets with multiple cells. And before we learned about
         #   setters and getters in Python. Feel free to refactor it.
 
+        # TODO: deprecate it
+
         if attr == "mass":
             setter = self._set_mass
         elif attr == "tot_mass":
             setter = self._set_tot_mass
         elif attr == "nom_cap":
             setter = self._set_nom_cap
 
-        number_of_tests = len(self.cells)
-        if not number_of_tests:
+        if not self.data:
             logging.info("No datasets have been loaded yet")
             logging.info(f"Cannot set {attr} before loading datasets")
             sys.exit(-1)
 
-        if not dataset_number:
-            dataset_number = list(range(len(self.cells)))
-
-        if not self._is_listtype(dataset_number):
-            dataset_number = [dataset_number]
-
-        if not self._is_listtype(vals):
-            vals = [vals]
         if validated is None:
-            for t, m in zip(dataset_number, vals):
-                setter(t, m)
+            setter(val)
         else:
-            for t, m, v in zip(dataset_number, vals, validated):
-                if v:
-                    setter(t, m)
-                else:
-                    logging.debug("_set_run_attribute: this set is empty")
+            if validated:
+                setter(val)
+            else:
+                logging.debug("_set_run_attribute: this set is empty")
 
-    def set_mass(self, masses, dataset_number=None, validated=None):
+    def set_mass(self, mass, validated=None):
         """Sets the mass (masses) for the test (datasets)."""
-        self._set_run_attribute(
-            "mass", masses, dataset_number=dataset_number, validated=validated
+
+        warnings.warn(
+            "This function is deprecated. " "Use the setter instead (mass = value).",
+            DeprecationWarning,
+            stacklevel=2,
         )
+        self._set_run_attribute("mass", mass, validated=validated)
 
-    def set_tot_mass(self, masses, dataset_number=None, validated=None):
-        """Sets the mass (masses) for the test (datasets)."""
-        self._set_run_attribute(
-            "tot_mass", masses, dataset_number=dataset_number, validated=validated
+    def set_tot_mass(self, mass, validated=None):
+        warnings.warn(
+            "This function is deprecated. "
+            "Use the setter instead (tot_mass = value).",
+            DeprecationWarning,
+            stacklevel=2,
         )
 
-    def set_nom_cap(self, nom_caps, dataset_number=None, validated=None):
-        """Sets the mass (masses) for the test (datasets)."""
-        self._set_run_attribute(
-            "nom_cap", nom_caps, dataset_number=dataset_number, validated=validated
+        self._set_run_attribute("tot_mass", mass, validated=validated)
+
+    def set_nom_cap(self, nom_cap, validated=None):
+        warnings.warn(
+            "This function is deprecated. " "Use the setter instead (nom_cap = value).",
+            DeprecationWarning,
+            stacklevel=2,
         )
 
+        self._set_run_attribute("nom_cap", nom_cap, validated=validated)
+
     @staticmethod
     def set_col_first(df, col_names):
         """set selected columns first in a pandas.DataFrame.
 
         This function sets cols with names given in  col_names (a list) first in
         the DataFrame. The last col in col_name will come first (processed last)
         """
@@ -4616,96 +4634,45 @@
                 column_headings.pop(column_headings.index(col_name))
                 column_headings.insert(0, col_name)
 
         finally:
             df = df.reindex(columns=column_headings)
             return df
 
-    def set_dataset_number_force(self, dataset_number=0):
-        """Force to set testnumber.
-
-        Sets the DataSet number default (all functions with prm dataset_number
-        will then be run assuming the default set dataset_number)
-        """
-        self.selected_cell_number = dataset_number
-
-    def set_cellnumber(self, dataset_number):
-        """Set the cell number.
-
-        Set the cell number that will be used
-        (CellpyData.selected_dataset_number).
-        The class can save several datasets (but its not a frequently used
-        feature), the datasets are stored in a list and dataset_number is the
-        selected index in the list.
-
-        Several options are available:
-              n - int in range 0..(len-1) (python uses offset as index, i.e.
-                  starts with 0)
-              last, end, newest - last (index set to -1)
-              first, zero, beginning, default - first (index set to 0)
-        """
-        warnings.warn("Deprecated", DeprecationWarning)
-        logging.debug("***set_testnumber(n)")
-        if not isinstance(dataset_number, int):
-            dataset_number_txt = dataset_number
-            try:
-                if dataset_number_txt.lower() in ["last", "end", "newest"]:
-                    dataset_number = -1
-                elif dataset_number_txt.lower() in [
-                    "first",
-                    "zero",
-                    "beginning",
-                    "default",
-                ]:
-                    dataset_number = 0
-            except Exception as e:
-                logging.debug("assuming numeric")
-                warnings.warn(f"Unhandled exception raised: {e}")
-
-        number_of_tests = len(self.cells)
-        if dataset_number >= number_of_tests:
-            dataset_number = -1
-            logging.debug("you dont have that many datasets, setting to last test")
-        elif dataset_number < -1:
-            logging.debug("not a valid option, setting to first test")
-            dataset_number = 0
-        self.selected_cell_number = dataset_number
-
-    # TODO: deprecate this
-    def get_summary(self, dataset_number=None, use_summary_made=False):
+    def get_summary(self, use_summary_made=False):
         """Retrieve summary returned as a pandas DataFrame."""
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return None
-
-        cell = self.cell
+        cell = self.data
 
         # This is a bit convoluted; in the old days, we used an attribute
         # called summary_made,
         # that was set to True when the summary was made successfully.
         # It is most likely never
         # used anymore. And will most probably be deleted.
+
+        warnings.warn(
+            "get_summary is deprecated. Use the CellpyCell.data.summary property instead.",
+            DeprecationWarning,
+        )
+
         if use_summary_made:
             summary_made = cell.has_summary
         else:
             summary_made = True
 
         if not summary_made:
             warnings.warn("Summary is not made yet")
             return None
         else:
             logging.info("Returning datasets[test_no].summary")
             return cell.summary
 
     # -----------internal-helpers-----------------------------------------------
 
-    # TODO: clean it up a bit
     @staticmethod
-    def is_empty(v):
+    def _is_empty_array(v):
         try:
             if not v:
                 return True
             else:
                 return False
         except Exception:
             try:
@@ -4723,25 +4690,14 @@
     def _is_listtype(x):
         if isinstance(x, (list, tuple)):
             return True
         else:
             return False
 
     @staticmethod
-    def _check_file_type(filename):
-        warnings.warn(DeprecationWarning("this method will be removed " "in v.0.4.0"))
-        extension = os.path.splitext(filename)[-1]
-        filetype = "res"
-        if extension.lower() == ".res":
-            filetype = "res"
-        elif extension.lower() == ".h5":
-            filetype = "h5"
-        return filetype
-
-    @staticmethod
     def _bounds(x):
         return np.amin(x), np.amax(x)
 
     @staticmethod
     def _roundup(x):
         n = 1000.0
         x = np.ceil(x * n)
@@ -4785,729 +4741,604 @@
             else:
                 last_item = max(raw.loc[raw[c_txt] == j + 1, d_txt])
                 steps.append(last_item)
 
         last_items = raw[d_txt].isin(steps)
         return last_items
 
-    # TODO: find out what this is for and probably delete it
-    def _modify_cycle_number_using_cycle_step(
-        self, from_tuple=None, to_cycle=44, dataset_number=None
-    ):
-        # modify step-cycle tuple to new step-cycle tuple
-        # from_tuple = [old cycle_number, old step_number]
-        # to_cycle    = new cycle_number
-
-        if from_tuple is None:
-            from_tuple = [1, 4]
-        logging.debug("**- _modify_cycle_step")
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
-
-        cycle_index_header = self.headers_normal.cycle_index_txt
-        step_index_header = self.headers_normal.step_index_txt
-
-        step_table_txt_cycle = self.headers_step_table.cycle
-        step_table_txt_step = self.headers_step_table.step
-
-        # modifying steps
-        st = self.cells[dataset_number].steps
-        st[step_table_txt_cycle][
-            (st[step_table_txt_cycle] == from_tuple[0])
-            & (st[step_table_txt_step] == from_tuple[1])
-        ] = to_cycle
-        # modifying normal_table
-        nt = self.cells[dataset_number].raw
-        nt[cycle_index_header][
-            (nt[cycle_index_header] == from_tuple[0])
-            & (nt[step_index_header] == from_tuple[1])
-        ] = to_cycle
-        # modifying summary_table
-        # not implemented yet
-
     # ----------making-summary------------------------------------------------------
     def make_summary(
         self,
-        find_ocv=False,
+        # find_ocv=False,
         find_ir=False,
         find_end_voltage=True,
         use_cellpy_stat_file=None,
-        all_tests=True,
-        dataset_number=0,
+        # all_tests=True,
         ensure_step_table=True,
-        add_normalized_cycle_index=True,
-        add_c_rate=True,
+        # add_c_rate=True,
         normalization_cycles=None,
         nom_cap=None,
-        from_cycle=None,
+        nom_cap_specifics="gravimetric",
+        # from_cycle=None,
     ):
         """Convenience function that makes a summary of the cycling data."""
-
         # TODO: @jepe - include option for omitting steps
         # TODO: @jepe  - make it is possible to update only new data by implementing
         #  from_cycle (only calculate summary from a given cycle number).
         #  Probably best to keep the old summary and make
         #  a new one for the rest, then use pandas.concat to merge them.
-        #  Might have to create the culumative cols etc after merging?
+        #  Might have to create the cumulative cols etc after merging?
 
         # first - check if we need some "instrument-specific" prms
-        dataset_number = self._validate_dataset_number(dataset_number)
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
-
         if ensure_step_table is None:
             ensure_step_table = self.ensure_step_table
 
         if use_cellpy_stat_file is None:
             use_cellpy_stat_file = prms.Reader.use_cellpy_stat_file
             logging.debug("using use_cellpy_stat_file from prms")
             logging.debug(f"use_cellpy_stat_file: {use_cellpy_stat_file}")
 
-        if all_tests is True:
-            for j in range(len(self.cells)):
-                txt = "creating summary for file "
-                test = self.cells[j]
-                if not self._is_not_empty_dataset(test):
-                    logging.info(f"Empty test {j})")
-                    return
-                if isinstance(test.loaded_from, (list, tuple)):
-                    for f in test.loaded_from:
-                        txt += f"{f}\n"
-                else:
-                    txt += str(test.loaded_from)
-
-                if not test.mass_given:
-                    txt += f" mass for test {j} is not given"
-                    txt += f" setting it to {test.mass} mg"
-                logging.debug(txt)
+        txt = "creating summary for file "
+        try:
+            test = self.data
+        except NoDataFound:
+            logging.info(f"Empty test {test})")
+            return
 
-                self._make_summary(
-                    j,
-                    find_ocv=find_ocv,
-                    find_ir=find_ir,
-                    find_end_voltage=find_end_voltage,
-                    use_cellpy_stat_file=use_cellpy_stat_file,
-                    ensure_step_table=ensure_step_table,
-                    add_normalized_cycle_index=add_normalized_cycle_index,
-                    add_c_rate=add_c_rate,
-                    normalization_cycles=normalization_cycles,
-                    nom_cap=nom_cap,
-                )
+        if isinstance(test.loaded_from, (list, tuple)):
+            for f in test.loaded_from:
+                txt += f"{f}\n"
         else:
-            logging.debug("creating summary for only one test")
-            dataset_number = self._validate_dataset_number(dataset_number)
-            if dataset_number is None:
-                self._report_empty_dataset()
-                return
-            self._make_summary(
-                dataset_number,
-                find_ocv=find_ocv,
-                find_ir=find_ir,
-                find_end_voltage=find_end_voltage,
-                use_cellpy_stat_file=use_cellpy_stat_file,
-                ensure_step_table=ensure_step_table,
-                add_normalized_cycle_index=add_normalized_cycle_index,
-                add_c_rate=add_c_rate,
-                normalization_cycles=normalization_cycles,
-                nom_cap=nom_cap,
-            )
+            txt += str(test.loaded_from)
+
+        logging.debug(txt)
+
+        self._make_summary(
+            # find_ocv=find_ocv,
+            find_ir=find_ir,
+            find_end_voltage=find_end_voltage,
+            use_cellpy_stat_file=use_cellpy_stat_file,
+            ensure_step_table=ensure_step_table,
+            # add_c_rate=add_c_rate,
+            normalization_cycles=normalization_cycles,
+            nom_cap=nom_cap,
+            nom_cap_specifics=nom_cap_specifics,
+        )
+        # else:
+        #     logging.debug("creating summary for only one test")
+        #     self._make_summary(
+        #         find_ocv=find_ocv,
+        #         find_ir=find_ir,
+        #         find_end_voltage=find_end_voltage,
+        #         use_cellpy_stat_file=use_cellpy_stat_file,
+        #         ensure_step_table=ensure_step_table,
+        #         add_c_rate=add_c_rate,
+        #         normalization_cycles=normalization_cycles,
+        #         nom_cap=nom_cap,
+        #         nom_cap_specifics="gravimetric",
+        #     )
         return self
 
     def _make_summary(
         self,
-        dataset_number=None,
         mass=None,
         update_it=False,
         select_columns=True,
-        find_ocv=False,
-        find_ir=False,
+        # find_ocv=False,  # deprecated
+        find_ir=True,
         find_end_voltage=False,
         ensure_step_table=True,
-        # TODO: @jepe - include option for omitting steps
+        # TODO @jepe: - include option for omitting steps
         sort_my_columns=True,
         use_cellpy_stat_file=False,
-        add_normalized_cycle_index=True,
-        add_c_rate=False,
+        # add_c_rate=True,  # deprecated
         normalization_cycles=None,
         nom_cap=None,
+        nom_cap_specifics="gravimetric",
+        add_daniel_columns=False,  # deprecated
         # capacity_modifier = None,
         # test=None
     ):
-        cycle_index_as_index = True
+        # ---------------- discharge loss --------------------------------------
+        # Assume that both charge and discharge is defined as positive.
+        # The gain for cycle n (compared to cycle n-1)
+        # is then cap[n] - cap[n-1]. The loss is the negative of gain.
+        # discharge loss = discharge_cap[n-1] - discharge_cap[n]
 
-        time_00 = time.time()
+        # ---------------- charge loss -----------------------------------------
+        # charge loss = charge_cap[n-1] - charge_cap[n]
 
-        dataset_number = self._validate_dataset_number(dataset_number)
+        # --------------- D.L. -------------------------------------------------
+        # NH_n: high level at cycle n. The slope NHn=f(n) is linked to SEI loss
+        # NB_n: low level (summation of irreversible capacities) at cycle n
+        # Ref_n: sum[i=1 to ref](Q_charge_i - Q_discharge_i) + Q_charge_ref
+        # Typically, ref should be a number where the electrode has become
+        # stable (i.e. 5).
+        # NBn/100 = sum[i=1 to n](Q_charge_i - Q_discharge_i) / Ref_n
+        # NHn/100 = Q_charge_n + sum[i=1 to n-1](Q_charge_i - Q_discharge_i)
+        #  / Ref_n
+        # NH = 100%  ok if NH<120 at n=200
+        # NB = 20% stable (or less)
+
+        # --------- shifted capacities ------------------------------------------
+        #  as defined by J. Dahn et al.
+        # Note! Should double-check this (including checking
+        # if it is valid in cathode mode).
+
+        # --------- relative irreversible capacities -----------------------------
+        #  as defined by Gauthier et al.
+        # RIC = discharge_cap[n-1] - charge_cap[n] /  charge_cap[n-1]
+        # RIC_SEI = discharge_cap[n] - charge_cap[n-1] / charge_cap[n-1]
+        # RIC_disconnect = charge_cap[n-1] - charge_cap[n] / charge_cap[n-1]
+
+        # --------- notes --------------------------------------------------------
+        # @jepe 2022.09.11: trying to use .assign from now on
+        #   as it is recommended (but this will likely increase memory usage)
 
+        # TODO: add this to arguments and possible prms:
+        specifics = ["gravimetric", "areal"]
+        cycle_index_as_index = True
+        time_00 = time.time()
         logging.debug("start making summary")
-        if dataset_number is None:
-            self._report_empty_dataset()
-            return
-        dataset = self.cells[dataset_number]
 
+        cell = self.data
         if not mass:
-            mass = dataset.mass or 1.0
+            mass = cell.mass or 1.0
         else:
             if update_it:
-                dataset.mass = mass
+                cell.mass = mass
+
+        if not use_cellpy_stat_file:
+            logging.debug("not using cellpy statfile")
 
-        if ensure_step_table and not self.load_only_summary:
+        if nom_cap is None:
+            nom_cap = cell.nom_cap
+
+        logging.info(f"Using the following nominal capacity: {nom_cap}")
+
+        # cellpy has historically assumed that the nominal capacity (nom_cap) is specific gravimetric
+        # (i.e. in units of for example mAh/g), but now we need it in absolute units (e.g. Ah). The plan
+        # is to set stuff like this during initiation of the cell (but not yet):
+
+        # generating absolute nominal capacity:
+        if nom_cap_specifics == "gravimetric":
+            nom_cap_abs = self.nominal_capacity_as_absolute(
+                nom_cap, mass, nom_cap_specifics
+            )
+        elif nom_cap_specifics == "areal":
+            nom_cap_abs = self.nominal_capacity_as_absolute(
+                nom_cap, cell.active_electrode_area, nom_cap_specifics
+            )
+
+        # ensuring that a step table exists:
+        if ensure_step_table:
             logging.debug("ensuring existence of step-table")
-            if not dataset.has_steps:
+            if not cell.has_steps:
                 logging.debug("dataset.step_table_made is not True")
                 logging.info("running make_step_table")
-                if nom_cap is not None:
-                    dataset.nom_cap = nom_cap
-                self.make_step_table(dataset_number=dataset_number)
-
-        # Retrieve the converters etc.
-        specific_converter = self.get_converter_to_specific(dataset=dataset, mass=mass)
-
-        hdr_normal = self.headers_normal
-
-        # TODO @jepe: don't "unpack" these (replace all occurrences with the unpacked ones)
-        #  (even though that will slow the code a little bit)
-
-        # TODO @jepe: rename from xxx_txt to xxx in the internal_settings
-        #  (careful - this needs to be done step-by-step combined with frequent testing)
-        dt_txt = hdr_normal.datetime_txt
-        tt_txt = hdr_normal.test_time_txt
-        st_txt = hdr_normal.step_time_txt
-        c_txt = hdr_normal.cycle_index_txt
-        d_txt = hdr_normal.data_point_txt
-        s_txt = hdr_normal.step_index_txt
-        voltage_header = hdr_normal.voltage_txt
-        charge_txt = hdr_normal.charge_capacity_txt
-        discharge_txt = hdr_normal.discharge_capacity_txt
-        ir_txt = hdr_normal.internal_resistance_txt
-        test_id_txt = hdr_normal.test_id_txt
-        i_txt = hdr_normal.current_txt
-
-        hdr_summary = self.headers_summary
-        discharge_title = hdr_summary.discharge_capacity
-        charge_title = hdr_summary.charge_capacity
-        cumcharge_title = hdr_summary.cumulated_charge_capacity
-        cumdischarge_title = hdr_summary.cumulated_discharge_capacity
-        coulomb_title = hdr_summary.coulombic_efficiency
-        cumcoulomb_title = hdr_summary.cumulated_coulombic_efficiency
-        coulomb_diff_title = hdr_summary.coulombic_difference
-        cumcoulomb_diff_title = hdr_summary.cumulated_coulombic_difference
-        col_discharge_loss_title = hdr_summary.discharge_capacity_loss
-        col_charge_loss_title = hdr_summary.charge_capacity_loss
-        dcloss_cumsum_title = hdr_summary.cumulated_discharge_capacity_loss
-        closs_cumsum_title = hdr_summary.cumulated_charge_capacity_loss
-        endv_charge_title = hdr_summary.end_voltage_charge
-        endv_discharge_title = hdr_summary.end_voltage_discharge
-        ocv_1_v_min_title = hdr_summary.ocv_first_min
-        ocv_1_v_max_title = hdr_summary.ocv_first_max
-        ocv_2_v_min_title = hdr_summary.ocv_second_min
-        ocv_2_v_max_title = hdr_summary.ocv_second_max
-        ir_discharge_title = hdr_summary.ir_discharge
-        ir_charge_title = hdr_summary.ir_charge
-
-        ric_disconnect_title = hdr_summary.cumulated_ric_disconnect
-        ric_sei_title = hdr_summary.cumulated_ric_sei
-        ric_title = hdr_summary.cumulated_ric
-        high_level_at_cycle_n_txt = hdr_summary.high_level
-        low_level_at_cycle_n_txt = hdr_summary.low_level
-        shifted_charge_capacity_title = hdr_summary.shifted_charge_capacity
-        shifted_discharge_capacity_title = hdr_summary.shifted_discharge_capacity
-
-        h_normalized_cycle = hdr_summary.normalized_cycle_index
-
-        hdr_steps = self.headers_step_table
-
-        # Here are the two main DataFrames for the test
-        # (raw-data and summary-data)
-        summary_df = dataset.summary
-        if not self.load_only_summary:
-            # Can't find summary from raw data if raw data is not loaded.
-            raw = dataset.raw
-            if use_cellpy_stat_file:
-                # This should work even if raw does not
-                # contain all data from the test
-                try:
-                    summary_requirment = raw[d_txt].isin(summary_df[d_txt])
-                except KeyError:
-                    logging.info("Error in stat_file (?) - using _select_last")
-                    summary_requirment = self._select_last(raw)
-            else:
-                summary_requirment = self._select_last(raw)
-            summary = raw[summary_requirment].copy()
-        else:
-            # summary_requirment = self._reloadrows_raw(summary_df[d_txt])
-            summary = summary_df
-            dataset.summary = summary
-            logging.warning("not implemented yet")
-            return
 
+                # update nom_cap in case it is given as argument to make_summary:
+                cell.nom_cap = nom_cap
+                self.make_step_table()
+
+        summary_df = cell.summary
+
+        raw = cell.raw
+        if use_cellpy_stat_file:
+            try:
+                summary_requirement = raw[self.headers_normal.data_point_txt].isin(
+                    summary_df[self.headers_normal.data_point_txt]
+                )
+            except KeyError:
+                logging.info("Error in stat_file (?) - using _select_last")
+                summary_requirement = self._select_last(raw)
+        else:
+            summary_requirement = self._select_last(raw)
+        summary = raw[summary_requirement].copy()
         column_names = summary.columns
+        # TODO @jepe: use pandas.DataFrame properties instead (.len, .reset_index), but maybe first
+        #  figure out if this is really needed and why it was implemented in the first place.
         summary_length = len(summary[column_names[0]])
         summary.index = list(range(summary_length))
-        # could also index based on Cycle_Index
-        # indexes = summary.index
 
         if select_columns:
-            columns_to_keep = [charge_txt, c_txt, d_txt, dt_txt, discharge_txt, tt_txt]
+            logging.debug("keeping only selected set of columns")
+            columns_to_keep = [
+                self.headers_normal.charge_capacity_txt,
+                self.headers_normal.cycle_index_txt,
+                self.headers_normal.data_point_txt,
+                self.headers_normal.datetime_txt,
+                self.headers_normal.discharge_capacity_txt,
+                self.headers_normal.test_time_txt,
+            ]
             for cn in column_names:
                 if not columns_to_keep.count(cn):
                     summary.pop(cn)
 
-        if not use_cellpy_stat_file:
-            logging.debug("not using cellpy statfile")
-            # logging.debug("Values obtained from raw:")
-            # logging.debug(summary.head(20))
+        cell.summary = summary
 
         if self.cycle_mode == "anode":
             logging.info(
-                "Assuming cycling in anode half-cell (discharge before charge) mode"
+                "Assuming cycling in anode half-data (discharge before charge) mode"
             )
-            _first_step_txt = discharge_title
-            _second_step_txt = charge_title
+            _first_step_txt = self.headers_summary.discharge_capacity
+            _second_step_txt = self.headers_summary.charge_capacity
         else:
-            logging.info("Assuming cycling in full-cell / cathode mode")
-            _first_step_txt = charge_title
-            _second_step_txt = discharge_title
-
-        # TODO @jepe: replace these with DataFrame.assign:
-        # logging.debug("Creates summary: specific discharge ('%s')"
-        #                   % discharge_title)
-        summary[discharge_title] = summary[discharge_txt] * specific_converter
-
-        # logging.debug("Creates summary: specific scharge ('%s')" %
-        #                   charge_title)
-        summary[charge_title] = summary[charge_txt] * specific_converter
-
-        # logging.debug("Creates summary: cumulated specific charge ('%s')" %
-        #                   cumdischarge_title)
-        summary[cumdischarge_title] = summary[discharge_title].cumsum()
-
-        # logging.debug("Creates summary: cumulated specific charge ('%s')" %
-        #                   cumcharge_title)
-        summary[cumcharge_title] = summary[charge_title].cumsum()
+            logging.info("Assuming cycling in full-data / cathode mode")
+            _first_step_txt = self.headers_summary.charge_capacity
+            _second_step_txt = self.headers_summary.discharge_capacity
 
-        summary[coulomb_title] = (
-            100.0 * summary[_second_step_txt] / summary[_first_step_txt]
-        )
+        # ---------------- absolute -------------------------------
 
-        summary[coulomb_diff_title] = (
-            summary[_first_step_txt] - summary[_second_step_txt]
+        cell = self._generate_absolute_summary_columns(
+            cell, _first_step_txt, _second_step_txt
+        )
+        cell = self._equivalent_cycles_to_summary(
+            cell, _first_step_txt, _second_step_txt, nom_cap_abs, normalization_cycles
         )
 
-        summary[cumcoulomb_title] = summary[coulomb_title].cumsum()
+        # getting the C-rates, using values from step-table (so it will not be changed
+        # even though you provide make_summary with a new nom_cap unfortunately):
+        cell = self._c_rates_to_summary(cell)
+
+        # ----------------- specifics ----------------------------------------
+        specific_columns = self.headers_summary.specific_columns
+        for mode in specifics:
+            cell = self._generate_specific_summary_columns(cell, mode, specific_columns)
 
-        summary[cumcoulomb_diff_title] = summary[coulomb_diff_title].cumsum()
+        if add_daniel_columns:
+            warnings.warn(
+                "Adding daniel columns is deprecated.", DeprecationWarning, stacklevel=2
+            )
 
-        # ---------------- discharge loss ---------------------
-        # Assume that both charge and discharge is defined as positive.
-        # The gain for cycle n (compared to cycle n-1)
-        # is then cap[n] - cap[n-1]. The loss is the negative of gain.
-        # discharge loss = discharge_cap[n-1] - discharge_cap[n]
-        # logging.debug("Creates summary: calculates DL")
-        summary[col_discharge_loss_title] = (
-            summary[discharge_title].shift(1) - summary[discharge_title]
-        )
+        # TODO @jepe: refactor this to method:
+        if find_end_voltage:
+            cell = self._end_voltage_to_summary(cell)
 
-        summary[dcloss_cumsum_title] = summary[col_discharge_loss_title].cumsum()
+        if find_ir and (
+            self.headers_normal.internal_resistance_txt in cell.raw.columns
+        ):
+            cell = self._ir_to_summary(cell)
 
-        # ---------------- charge loss ------------------------
-        # charge loss = charge_cap[n-1] - charge_cap[n]
-        summary[col_charge_loss_title] = (
-            summary[charge_title].shift(1) - summary[charge_title]
-        )
+        if sort_my_columns:
+            logging.debug("sorting columns")
+            new_first_col_list = [
+                self.headers_normal.datetime_txt,
+                self.headers_normal.test_time_txt,
+                self.headers_normal.data_point_txt,
+                self.headers_normal.cycle_index_txt,
+            ]
+            cell.summary = self.set_col_first(cell.summary, new_first_col_list)
+
+        if cycle_index_as_index:
+            index_col = self.headers_summary.cycle_index
+            try:
+                cell.summary.set_index(index_col, inplace=True)
+            except KeyError:
+                logging.debug("Setting cycle_index as index failed")
 
-        summary[closs_cumsum_title] = summary[col_charge_loss_title].cumsum()
+        logging.debug(f"(dt: {(time.time() - time_00):4.2f}s)")
 
-        # --------------- D.L. --------------------------------
-        # NH_n: high level at cycle n. The slope NHn=f(n) is linked to SEI loss
-        # NB_n: low level (summation of irreversible capacities) at cycle n
-        # Ref_n: sum[i=1 to ref](Q_charge_i - Q_discharge_i) + Q_charge_ref
-        # Typically, ref should be a number where the electrode has become
-        # stable (i.e. 5).
-        # NBn/100 = sum[i=1 to n](Q_charge_i - Q_discharge_i) / Ref_n
-        # NHn/100 = Q_charge_n + sum[i=1 to n-1](Q_charge_i - Q_discharge_i)
-        #  / Ref_n
-        # NH = 100%  ok if NH<120 at n=200
-        # NB = 20% stable (or less)
+    def _generate_absolute_summary_columns(
+        self, data, _first_step_txt, _second_step_txt
+    ) -> Data:
+        summary = data.summary
+        summary[self.headers_summary.coulombic_efficiency] = (
+            100 * summary[_second_step_txt] / summary[_first_step_txt]
+        )
+        summary[self.headers_summary.cumulated_coulombic_efficiency] = summary[
+            self.headers_summary.coulombic_efficiency
+        ].cumsum()
+
+        capacity_columns = {
+            self.headers_summary.charge_capacity: summary[
+                self.headers_normal.charge_capacity_txt
+            ],
+            self.headers_summary.discharge_capacity: summary[
+                self.headers_normal.discharge_capacity_txt
+            ],
+        }
+        summary = summary.assign(**capacity_columns)
 
-        # TODO @jepe: refactor this to method:
-        n = self.daniel_number
-        cap_ref = summary.loc[summary[c_txt] == n, _first_step_txt]
-        if not cap_ref.empty:
-            cap_ref = cap_ref.values[0]
+        calculated_from_capacity_columns = {
+            self.headers_summary.cumulated_charge_capacity: summary[
+                self.headers_summary.charge_capacity
+            ].cumsum(),
+            self.headers_summary.cumulated_discharge_capacity: summary[
+                self.headers_summary.discharge_capacity
+            ].cumsum(),
+            self.headers_summary.discharge_capacity_loss: (
+                summary[self.headers_summary.discharge_capacity].shift(1)
+                - summary[self.headers_summary.discharge_capacity]
+            ),
+            self.headers_summary.charge_capacity_loss: (
+                summary[self.headers_summary.charge_capacity].shift(1)
+                - summary[self.headers_summary.charge_capacity]
+            ),
+            self.headers_summary.coulombic_difference: (
+                summary[_first_step_txt] - summary[_second_step_txt]
+            ),
+        }
 
-            ref = (
-                summary.loc[summary[c_txt] < n, _second_step_txt].sum()
-                + summary.loc[summary[c_txt] < n, _first_step_txt].sum()
-                + cap_ref
-            )
+        summary = summary.assign(**calculated_from_capacity_columns)
 
-            summary[low_level_at_cycle_n_txt] = (100 / ref) * (
-                summary[_first_step_txt].cumsum() - summary[_second_step_txt].cumsum()
-            )
+        calculated_from_coulombic_efficiency_columns = {
+            self.headers_summary.cumulated_coulombic_difference: summary[
+                self.headers_summary.coulombic_difference
+            ].cumsum(),
+        }
 
-            summary[high_level_at_cycle_n_txt] = (100 / ref) * (
-                summary[_first_step_txt]
-                + summary[_first_step_txt].cumsum()
-                - summary[_second_step_txt].cumsum()
-            )
-        else:
-            txt = f"ref cycle number: {n}"
-            logging.info(
-                "could not extract low-high levels (ref cycle number does not exist)"
-            )
-            # logging.info(txt)
-            summary[low_level_at_cycle_n_txt] = np.nan
-            summary[high_level_at_cycle_n_txt] = np.nan
+        summary = summary.assign(**calculated_from_coulombic_efficiency_columns)
+        calculated_from_capacity_loss_columns = {
+            self.headers_summary.cumulated_discharge_capacity_loss: summary[
+                self.headers_summary.discharge_capacity_loss
+            ].cumsum(),
+            self.headers_summary.cumulated_charge_capacity_loss: summary[
+                self.headers_summary.charge_capacity_loss
+            ].cumsum(),
+        }
+        summary = summary.assign(**calculated_from_capacity_loss_columns)
 
-        # --------------relative irreversible capacities
-        #  as defined by Gauthier et al.---
-        # RIC = discharge_cap[n-1] - charge_cap[n] /  charge_cap[n-1]
-        RIC = (summary[_first_step_txt].shift(1) - summary[_second_step_txt]) / summary[
+        individual_edge_movement = summary[_first_step_txt] - summary[_second_step_txt]
+        shifted_charge_capacity_column = {
+            self.headers_summary.shifted_charge_capacity: individual_edge_movement.cumsum(),
+        }
+        summary = summary.assign(**shifted_charge_capacity_column)
+
+        shifted_discharge_capacity_column = {
+            self.headers_summary.shifted_discharge_capacity: summary[
+                self.headers_summary.shifted_charge_capacity
+            ]
+            + summary[_first_step_txt],
+        }
+        summary = summary.assign(**shifted_discharge_capacity_column)
+        ric = (summary[_first_step_txt].shift(1) - summary[_second_step_txt]) / summary[
             _second_step_txt
         ].shift(1)
-        summary[ric_title] = RIC.cumsum()
-
-        # RIC_SEI = discharge_cap[n] - charge_cap[n-1] / charge_cap[n-1]
-        RIC_SEI = (
+        ric_column = {self.headers_summary.cumulated_ric: ric.cumsum()}
+        summary = summary.assign(**ric_column)
+        summary[self.headers_summary.cumulated_ric] = ric.cumsum()
+        ric_sei = (
             summary[_first_step_txt] - summary[_second_step_txt].shift(1)
         ) / summary[_second_step_txt].shift(1)
-        summary[ric_sei_title] = RIC_SEI.cumsum()
-
-        # RIC_disconnect = charge_cap[n-1] - charge_cap[n] / charge_cap[n-1]
-        RIC_disconnect = (
+        ric_sei_column = {self.headers_summary.cumulated_ric_sei: ric_sei.cumsum()}
+        summary = summary.assign(**ric_sei_column)
+        ric_disconnect = (
             summary[_second_step_txt].shift(1) - summary[_second_step_txt]
         ) / summary[_second_step_txt].shift(1)
-        summary[ric_disconnect_title] = RIC_disconnect.cumsum()
+        ric_disconnect_column = {
+            self.headers_summary.cumulated_ric_disconnect: ric_disconnect.cumsum()
+        }
+        data.summary = summary.assign(**ric_disconnect_column)
 
-        # -------------- shifted capacities as defined by J. Dahn et al. -----
-        # need to double check this (including checking
-        # if it is valid in cathode mode).
-        individual_edge_movement = summary[_first_step_txt] - summary[_second_step_txt]
+        return data
 
-        summary[shifted_charge_capacity_title] = individual_edge_movement.cumsum()
-        summary[shifted_discharge_capacity_title] = (
-            summary[shifted_charge_capacity_title] + summary[_first_step_txt]
+    def _generate_specific_summary_columns(
+        self, data: Data, mode: str, specific_columns: Sequence
+    ) -> Data:
+        specific_converter = self.get_converter_to_specific(dataset=data, mode=mode)
+        summary = data.summary
+        for col in specific_columns:
+            logging.debug(f"generating specific column {col}_{mode}")
+            summary[f"{col}_{mode}"] = specific_converter * summary[col]
+        data.summary = summary
+        return data
+
+    def _c_rates_to_summary(self, data: Data) -> Data:
+        logging.debug("Extracting C-rates")
+        summary = data.summary
+        steps = self.data.steps
+
+        charge_steps = steps.loc[
+            steps.type == "charge",
+            [self.headers_step_table.cycle, self.headers_step_table.rate_avr],
+        ].rename(
+            columns={
+                self.headers_step_table.rate_avr: self.headers_summary.charge_c_rate
+            }
         )
+        summary = summary.merge(
+            charge_steps.drop_duplicates(
+                subset=[self.headers_step_table.cycle], keep="first"
+            ),
+            left_on=self.headers_summary.cycle_index,
+            right_on=self.headers_step_table.cycle,
+            how="left",
+        ).drop(columns=self.headers_step_table.cycle)
+
+        discharge_steps = steps.loc[
+            steps.type == "discharge",
+            [self.headers_step_table.cycle, self.headers_step_table.rate_avr],
+        ].rename(
+            columns={
+                self.headers_step_table.rate_avr: self.headers_summary.discharge_c_rate
+            }
+        )
+        summary = summary.merge(
+            discharge_steps.drop_duplicates(
+                subset=[self.headers_step_table.cycle], keep="first"
+            ),
+            left_on=self.headers_summary.cycle_index,
+            right_on=self.headers_step_table.cycle,
+            how="left",
+        ).drop(columns=self.headers_step_table.cycle)
+        data.summary = summary
+        return data
+
+    def _equivalent_cycles_to_summary(
+        self,
+        data: Data,
+        _first_step_txt: str,
+        _second_step_txt: str,
+        nom_cap: float,
+        normalization_cycles: Union[Sequence, int, None],
+    ) -> Data:
+        # The method currently uses the charge capacity for calculating equivalent cycles. This
+        # can be easily extended to also allow for choosing the discharge capacity later on if
+        # it turns out that to be needed.
 
-        # if convert_date:
-        #     # TODO: should move this to the instrument reader procedure
-        #     logging.debug("converting date from xls-type")
-        #     summary[date_time_txt_title] = \
-        #         summary[dt_txt].apply(xldate_as_datetime)  # , option="to_string")
+        summary = data.summary
 
-        # TODO @jepe: refactor this to method:
-        if find_ocv and not self.load_only_summary:
-            warnings.warn(DeprecationWarning("this option will be removed in v.0.4.0"))
-            # should remove this option
-            logging.info("CONGRATULATIONS")
-            logging.info("-thought this would never be run!")
-            logging.info("-find_ocv in make_summary")
+        if normalization_cycles is not None:
             logging.info(
-                "  this is a stupid routine that can be implemented much better!"
+                f"Using these cycles for finding the nominal capacity: {normalization_cycles}"
             )
-            do_ocv_1 = True
-            do_ocv_2 = True
-
-            ocv1_type = "ocvrlx_up"
-            ocv2_type = "ocvrlx_down"
+            if not isinstance(normalization_cycles, (list, tuple)):
+                normalization_cycles = [normalization_cycles]
 
-            if not self.cycle_mode == "anode":
-                ocv2_type = "ocvrlx_up"
-                ocv1_type = "ocvrlx_down"
-
-            ocv_1 = self._get_ocv(
-                ocv_steps=dataset.ocv_steps,
-                ocv_type=ocv1_type,
-                dataset_number=dataset_number,
-            )
-
-            ocv_2 = self._get_ocv(
-                ocv_steps=dataset.ocv_steps,
-                ocv_type=ocv2_type,
-                dataset_number=dataset_number,
-            )
-
-            if do_ocv_1:
-                only_zeros = summary[discharge_txt] * 0.0
-                ocv_1_indexes = []
-                ocv_1_v_min = []
-                ocv_1_v_max = []
-                ocvcol_min = only_zeros.copy()
-                ocvcol_max = only_zeros.copy()
-
-                for j in ocv_1:
-                    cycle = j["Cycle_Index"].values[0]  # jepe fix
-                    # try to find inxed
-                    index = summary[(summary[c_txt] == cycle)].index
-                    # print cycle, index,
-                    v_min = j["Voltage"].min()  # jepe fix
-                    v_max = j["Voltage"].max()  # jepe fix
-                    # print v_min,v_max
-                    dv = v_max - v_min
-                    ocvcol_min.iloc[index] = v_min
-                    ocvcol_max.iloc[index] = v_max
-
-                summary.insert(0, column=ocv_1_v_min_title, value=ocvcol_min)
-                summary.insert(0, column=ocv_1_v_max_title, value=ocvcol_max)
-
-            if do_ocv_2:
-                only_zeros = summary[discharge_txt] * 0.0
-                ocv_2_indexes = []
-                ocv_2_v_min = []
-                ocv_2_v_max = []
-                ocvcol_min = only_zeros.copy()
-                ocvcol_max = only_zeros.copy()
-
-                for j in ocv_2:
-                    cycle = j["Cycle_Index"].values[0]  # jepe fix
-                    # try to find inxed
-                    index = summary[(summary[c_txt] == cycle)].index
-                    v_min = j["Voltage"].min()  # jepe fix
-                    v_max = j["Voltage"].max()  # jepe fix
-                    dv = v_max - v_min
-                    ocvcol_min.iloc[index] = v_min
-                    ocvcol_max.iloc[index] = v_max
-                summary.insert(0, column=ocv_2_v_min_title, value=ocvcol_min)
-                summary.insert(0, column=ocv_2_v_max_title, value=ocvcol_max)
-
-        # TODO @jepe: refactor this to method:
-        if find_end_voltage and not self.load_only_summary:
-            # needs to be fixed so that end-voltage also can be extracted
-            # from the summary
-            ev_t0 = time.time()
-            logging.debug("finding end-voltage")
-            logging.debug(f"dt: {time.time() - ev_t0}")
-            only_zeros_discharge = summary[discharge_txt] * 0.0
-            only_zeros_charge = summary[charge_txt] * 0.0
-            if not dataset.discharge_steps:
-                logging.debug("need to collect discharge steps")
-                discharge_steps = self.get_step_numbers(
-                    steptype="discharge", allctypes=False, dataset_number=dataset_number
-                )
-                logging.debug(f"dt: {time.time() - ev_t0}")
-            else:
-                discharge_steps = dataset.discharge_steps
-                logging.debug("  already have discharge_steps")
-            if not dataset.charge_steps:
-                logging.debug("need to collect charge steps")
-                charge_steps = self.get_step_numbers(
-                    steptype="charge", allctypes=False, dataset_number=dataset_number
-                )
-                logging.debug(f"dt: {time.time() - ev_t0}")
+            cap_ref = summary.loc[
+                summary[self.headers_normal.cycle_index_txt].isin(normalization_cycles),
+                _first_step_txt,
+            ]
+            if not cap_ref.empty:
+                nom_cap = cap_ref.mean()
             else:
-                charge_steps = dataset.charge_steps
-                logging.debug("  already have charge_steps")
+                logging.info(f"Empty reference cycle(s)")
 
-            endv_indexes = []
-            endv_values_dc = []
-            endv_values_c = []
-            # logging.debug("trying to find end voltage for")
-            # logging.debug(dataset.loaded_from)
-            # logging.debug("Using the following chargesteps")
-            # logging.debug(charge_steps)
-            # logging.debug("Using the following dischargesteps")
-            # logging.debug(discharge_steps)
-            logging.debug("starting iterating through the index")
-            for i in summary.index:
-                # txt = "index in summary.index: %i" % i
-                # logging.debug(txt)
-                # selecting the appropriate cycle
-                cycle = summary.iloc[i][c_txt]
-                # txt = "cycle: %i" % cycle
-                # logging.debug(txt)
-                step = discharge_steps[cycle]
-
-                # finding end voltage for discharge
-                if step[-1]:  # selecting last
-                    # TODO: @jepe - use pd.loc[row,column]
-                    # for col or pd.loc[(pd.["step"]==1),"x"]
-                    end_voltage_dc = raw[
-                        (raw[c_txt] == cycle) & (dataset.raw[s_txt] == step[-1])
-                    ][voltage_header]
-                    # This will not work if there are more than one item in step
-                    end_voltage_dc = end_voltage_dc.values[-1]  # selecting
-                    # last (could also select amax)
-                else:
-                    end_voltage_dc = 0  # could also use numpy.nan
-
-                # finding end voltage for charge
-                step2 = charge_steps[cycle]
-                if step2[-1]:
-                    end_voltage_c = raw[
-                        (raw[c_txt] == cycle) & (dataset.raw[s_txt] == step2[-1])
-                    ][voltage_header]
-                    end_voltage_c = end_voltage_c.values[-1]
-                    # end_voltage_c = np.amax(end_voltage_c)
-                else:
-                    end_voltage_c = 0
-                endv_indexes.append(i)
-                endv_values_dc.append(end_voltage_dc)
-                endv_values_c.append(end_voltage_c)
-            logging.debug("finished iterating")
-            logging.debug(f"find end V took: {time.time() - ev_t0} s")
-            ir_frame_dc = only_zeros_discharge + endv_values_dc
-            ir_frame_c = only_zeros_charge + endv_values_c
-            summary.insert(0, column=endv_discharge_title, value=ir_frame_dc)
-            summary.insert(0, column=endv_charge_title, value=ir_frame_c)
-
-        # TODO @jepe: refactor this to method:
-        if find_ir and (not self.load_only_summary) and (ir_txt in dataset.raw.columns):
-            # should check:  test.charge_steps = None,
-            # test.discharge_steps = None
-            # THIS DOES NOT WORK PROPERLY!!!!
-            # Found a file where it writes IR for cycle n on cycle n+1
-            # This only picks out the data on the last IR step before
-            logging.debug("finding ir")
-            only_zeros = summary[discharge_txt] * 0.0
-            if not dataset.discharge_steps:
-                discharge_steps = self.get_step_numbers(
-                    steptype="discharge", allctypes=False, dataset_number=dataset_number
-                )
+        normalized_cycle_index_column = {
+            self.headers_summary.normalized_cycle_index: summary[
+                self.headers_summary.cumulated_charge_capacity
+            ]
+            / nom_cap
+        }
+        summary = summary.assign(**normalized_cycle_index_column)
+        data.summary = summary
+        return data
+
+    def _ir_to_summary(self, data):
+        # should check:  test.charge_steps = None,
+        # test.discharge_steps = None
+        # THIS DOES NOT WORK PROPERLY!!!!
+        # Found a file where it writes IR for cycle n on cycle n+1
+        # This only picks out the data on the last IR step before
+        summary = data.summary
+        raw = data.raw
+
+        logging.debug("finding ir")
+        only_zeros = summary[self.headers_normal.discharge_capacity_txt] * 0.0
+        discharge_steps = self.get_step_numbers(
+            steptype="discharge",
+            allctypes=False,
+        )
+        charge_steps = self.get_step_numbers(
+            steptype="charge",
+            allctypes=False,
+        )
+        ir_indexes = []
+        ir_values = []
+        ir_values2 = []
+        for i in summary.index:
+            # selecting the appropriate cycle
+            cycle = summary.iloc[i][self.headers_normal.cycle_index_txt]
+            step = discharge_steps[cycle]
+            if step[0]:
+                ir = raw.loc[
+                    (raw[self.headers_normal.cycle_index_txt] == cycle)
+                    & (data.raw[self.headers_normal.step_index_txt] == step[0]),
+                    self.headers_normal.internal_resistance_txt,
+                ]
+                # This will not work if there are more than one item in step
+                ir = ir.values[0]
             else:
-                discharge_steps = dataset.discharge_steps
-                logging.debug("  already have discharge_steps")
-            if not dataset.charge_steps:
-                charge_steps = self.get_step_numbers(
-                    steptype="charge", allctypes=False, dataset_number=dataset_number
-                )
+                ir = 0
+            step2 = charge_steps[cycle]
+            if step2[0]:
+                ir2 = raw[
+                    (raw[self.headers_normal.cycle_index_txt] == cycle)
+                    & (data.raw[self.headers_normal.step_index_txt] == step2[0])
+                ][self.headers_normal.internal_resistance_txt].values[0]
             else:
-                charge_steps = dataset.charge_steps
-                logging.debug("  already have charge_steps")
-
-            ir_indexes = []
-            ir_values = []
-            ir_values2 = []
-            # logging.debug("trying to find ir for")
-            # logging.debug(dataset.loaded_from)
-            # logging.debug("Using the following charge_steps")
-            # logging.debug(charge_steps)
-            # logging.debug("Using the following discharge_steps")
-            # logging.debug(discharge_steps)
-
-            for i in summary.index:
-                # txt = "index in summary.index: %i" % i
-                # logging.debug(txt)
-                # selecting the appropriate cycle
-                cycle = summary.iloc[i][c_txt]  # "Cycle_Index" = i + 1
-                # txt = "cycle: %i" % cycle
-                # logging.debug(txt)
-                step = discharge_steps[cycle]
-                if step[0]:
-                    ir = raw.loc[
-                        (raw[c_txt] == cycle) & (dataset.raw[s_txt] == step[0]), ir_txt
-                    ]
-                    # This will not work if there are more than one item in step
-                    ir = ir.values[0]
-                else:
-                    ir = 0
-                step2 = charge_steps[cycle]
-                if step2[0]:
-
-                    ir2 = raw[(raw[c_txt] == cycle) & (dataset.raw[s_txt] == step2[0])][
-                        ir_txt
-                    ].values[0]
-                else:
-                    ir2 = 0
-                ir_indexes.append(i)
-                ir_values.append(ir)
-                ir_values2.append(ir2)
-
-            ir_frame = only_zeros + ir_values
-            ir_frame2 = only_zeros + ir_values2
-            summary.insert(0, column=ir_discharge_title, value=ir_frame)
-            summary.insert(0, column=ir_charge_title, value=ir_frame2)
-
-        # TODO @jepe: refactor this to method:
-        if add_normalized_cycle_index:
-            if normalization_cycles is not None:
-                logging.info(
-                    f"Using these cycles for finding the nominal capacity: {normalization_cycles}"
-                )
-                if not isinstance(normalization_cycles, (list, tuple)):
-                    normalization_cycles = [normalization_cycles]
-
-                cap_ref = summary.loc[
-                    summary[c_txt].isin(normalization_cycles), _first_step_txt
-                ]
-                if not cap_ref.empty:
-                    nom_cap = cap_ref.mean()
-                else:
-                    logging.info(f"Empty reference cycle(s)")
-
-            if nom_cap is None:
-                logging.debug(f"No nom_cap given")
-                nom_cap = self.cell.nom_cap
-            logging.info(f"Using the following nominal capacity: {nom_cap}")
-            summary[h_normalized_cycle] = summary[cumcharge_title] / nom_cap
-
-        # TODO @jepe: refactor this to method:
-        if add_c_rate:
-            logging.debug("Extracting C-rates")
-            steps = self.cell.steps
-
-            # if hdr_summary.cycle_index not in summary.columns:
-            #     summary = summary.reset_index()
-
-            charge_steps = steps.loc[
-                steps.type == "charge", [hdr_steps.cycle, "rate_avr"]
-            ].rename(columns={"rate_avr": hdr_summary.charge_c_rate})
-
-            summary = summary.merge(
-                charge_steps.drop_duplicates(subset=[hdr_steps.cycle], keep="first"),
-                left_on=hdr_summary.cycle_index,
-                right_on=hdr_steps.cycle,
-                how="left",
-            ).drop(columns=hdr_steps.cycle)
-
-            discharge_steps = steps.loc[
-                steps.type == "discharge", [hdr_steps.cycle, "rate_avr"]
-            ].rename(columns={"rate_avr": hdr_summary.discharge_c_rate})
-
-            summary = summary.merge(
-                discharge_steps.drop_duplicates(subset=[hdr_steps.cycle], keep="first"),
-                left_on=hdr_summary.cycle_index,
-                right_on=hdr_steps.cycle,
-                how="left",
-            ).drop(columns=hdr_steps.cycle)
-
-        if sort_my_columns:
-            logging.debug("sorting columns")
-            new_first_col_list = [dt_txt, tt_txt, d_txt, c_txt]
-            summary = self.set_col_first(summary, new_first_col_list)
+                ir2 = 0
+            ir_indexes.append(i)
+            ir_values.append(ir)
+            ir_values2.append(ir2)
+        ir_frame = only_zeros + ir_values
+        ir_frame2 = only_zeros + ir_values2
+        summary.insert(0, column=self.headers_summary.ir_discharge, value=ir_frame)
+        summary.insert(0, column=self.headers_summary.ir_charge, value=ir_frame2)
+        data.summary = summary
+        return data
+
+    def _end_voltage_to_summary(self, data):
+        # needs to be fixed so that end-voltage also can be extracted
+        # from the summary
+        ev_t0 = time.time()
+        raw = data.raw
+        summary = data.summary
+
+        logging.debug("finding end-voltage")
+        logging.debug(f"dt: {time.time() - ev_t0}")
+        only_zeros_discharge = summary[self.headers_normal.discharge_capacity_txt] * 0.0
+        only_zeros_charge = summary[self.headers_normal.charge_capacity_txt] * 0.0
+        logging.debug("need to collect discharge steps")
+        discharge_steps = self.get_step_numbers(steptype="discharge", allctypes=False)
+        logging.debug(f"dt: {time.time() - ev_t0}")
+        logging.debug("need to collect charge steps")
+        charge_steps = self.get_step_numbers(steptype="charge", allctypes=False)
+        logging.debug(f"dt: {time.time() - ev_t0}")
+        endv_indexes = []
+        endv_values_dc = []
+        endv_values_c = []
+        logging.debug("starting iterating through the index")
+        for i in summary.index:
+            cycle = summary.iloc[i][self.headers_normal.cycle_index_txt]
+            step = discharge_steps[cycle]
+
+            # finding end voltage for discharge
+            if step[-1]:  # selecting last
+                end_voltage_dc = raw[
+                    (raw[self.headers_normal.cycle_index_txt] == cycle)
+                    & (data.raw[self.headers_normal.step_index_txt] == step[-1])
+                ][self.headers_normal.voltage_txt]
+                # This will not work if there are more than one item in step
+                end_voltage_dc = end_voltage_dc.values[-1]  # selecting
+            else:
+                end_voltage_dc = 0  # could also use numpy.nan
 
-        if cycle_index_as_index:
-            index_col = hdr_summary.cycle_index
-            try:
-                summary.set_index(index_col, inplace=True)
-            except KeyError:
-                logging.debug("Setting cycle_index as index failed")
+            # finding end voltage for charge
+            step2 = charge_steps[cycle]
+            if step2[-1]:
+                end_voltage_c = raw[
+                    (raw[self.headers_normal.cycle_index_txt] == cycle)
+                    & (data.raw[self.headers_normal.step_index_txt] == step2[-1])
+                ][self.headers_normal.voltage_txt]
+                end_voltage_c = end_voltage_c.values[-1]
+            else:
+                end_voltage_c = 0
+            endv_indexes.append(i)
+            endv_values_dc.append(end_voltage_dc)
+            endv_values_c.append(end_voltage_c)
+
+        ir_frame_dc = only_zeros_discharge + endv_values_dc
+        ir_frame_c = only_zeros_charge + endv_values_c
+        data.summary.insert(
+            0, column=self.headers_summary.end_voltage_discharge, value=ir_frame_dc
+        )
+        data.summary.insert(
+            0, column=self.headers_summary.end_voltage_charge, value=ir_frame_c
+        )
 
-        dataset.summary = summary
-        logging.debug(f"(dt: {(time.time() - time_00):4.2f}s)")
+        return data
 
     def inspect_nominal_capacity(self, cycles=None):
         """Method for estimating the nominal capacity
 
         Args:
-            cycles (list of ints): the cycles where it is assumed that the cell reaches nominal capacity.
+            cycles (list of ints): the cycles where it is assumed that the data reaches nominal capacity.
 
         Returns:
             Nominal capacity (float).
         """
         logging.debug("inspecting: nominal capacity")
         print("Sorry! This method is still under development.")
         print("Maybe you can plot your data and find the nominal capacity yourself?")
         if cycles is None:
             cycles = [1, 2, 3]
 
-        summary = self.cell.summary
+        summary = self.data.summary
 
         try:
             nc = summary.loc[
                 summary[self.headers_normal.cycle_index_txt].isin(cycles),
                 self.headers_summary.discharge_capacity,
             ].mean()
             print("All I can say for now is that the average discharge capacity")
@@ -5516,190 +5347,576 @@
 
         except ZeroDivisionError:
             print("zero division error")
             nc = None
 
         return nc
 
+    # ================ Experimental features =================
+
+    # ---------------------- update --------------------------
+
+    def dev_update(self, file_names=None, **kwargs):
+        """Experimental method for updating a cellpy-file with new raw-files."""
+
+        print("NOT FINISHED YET!")
+        if len(self.data.raw_data_files) != 1:
+            logging.warning("Merged data. But can only update based on the last file")
+            print(self.data.raw_data_files)
+            for fid in self.data.raw_data_files:
+                print(fid)
+        last = self.data.raw_data_files[0].last_data_point
+
+        self.dev_update_from_raw(
+            file_names=file_names, data_points=[last, None], **kwargs
+        )
+        print("lets try to merge")
+        self.data = self.dev_update_merge()
+        print("now it is time to update the step table")
+        self.dev_update_make_steps()
+        print("and finally, lets update the summary")
+        self.dev_update_make_summary()
+
+    def dev_update_loadcell(
+        self,
+        raw_files,
+        cellpy_file=None,
+        mass=None,
+        summary_on_raw=False,
+        summary_ir=True,
+        summary_end_v=True,
+        force_raw=False,
+        use_cellpy_stat_file=None,
+        nom_cap=None,
+        selector=None,
+        **kwargs,
+    ):
+        """Load cell from raw-files or cellpy-file.
+
+        This is an experimental method. It is not finished yet and the logics
+        in this method will most likely be moved to other methods since
+        new versions of cellpy is now based on the get method (it implements
+        similar logic as loadcell, but is more flexible and easier to use).
+
+
+
+        """
+        logging.info("Started cellpy.cellreader.dev_update_loadcell")
+
+        if cellpy_file is None or force_raw:
+            similar = None
+        else:
+            similar = self.check_file_ids(raw_files, cellpy_file, detailed=True)
+
+        logging.debug("checked if the files were similar")
+
+        if similar is None:
+            # forcing to load only raw_files
+            self.from_raw(raw_files, **kwargs)
+            if self._validate_cell():
+                if mass:
+                    self.set_mass(mass)
+                if summary_on_raw:
+                    self.make_summary(
+                        find_ir=summary_ir,
+                        find_end_voltage=summary_end_v,
+                        use_cellpy_stat_file=use_cellpy_stat_file,
+                        nom_cap=nom_cap,
+                    )
+            else:
+                logging.warning("Empty run!")
+            return self
+
+        self.load(cellpy_file, selector=selector)
+        if mass:
+            self.set_mass(mass)
+
+        if all(similar.values()):
+            logging.info("Everything is up to date")
+            return
+
+        start_file = True
+        for i, f in enumerate(raw_files):
+            # TODO: -> OtherPath?
+            f = Path(f)
+            if not similar[f.name] and start_file:
+                try:
+                    last_data_point = self.data.raw_data_files[i].last_data_point
+                except IndexError:
+                    last_data_point = 0
+
+                self.dev_update_from_raw(
+                    file_names=f, data_points=[last_data_point, None]
+                )
+                self.data = self.dev_update_merge()
+
+            elif not similar[f.name]:
+                try:
+                    last_data_point = self.data.raw_data_files[i].last_data_point
+                except IndexError:
+                    last_data_point = 0
+
+                self.dev_update_from_raw(
+                    file_names=f, data_points=[last_data_point, None]
+                )
+                self.merge()
+
+            start_file = False
+
+        self.dev_update_make_steps()
+        self.dev_update_make_summary(
+            # all_tests=False,
+            # find_ocv=summary_ocv,
+            find_ir=summary_ir,
+            find_end_voltage=summary_end_v,
+            use_cellpy_stat_file=use_cellpy_stat_file,
+        )
+        return self
+
+    # TODO @jepe (v.1.0.0): update this to use single data instances (i.e. to cell from cells)
+    def dev_update_merge(self, t1, t2):
+        print("NOT FINISHED YET - but very close")
+
+        if t1.raw.empty:
+            logging.debug("OBS! the first dataset is empty")
+
+        if t2.raw.empty:
+            logging.debug("the second dataset was empty")
+            logging.debug(" -> merged contains only first")
+            return t1
+        test = t1
+
+        cycle_index_header = self.headers_normal.cycle_index_txt
+
+        if not t1.raw.empty:
+            t1.raw = t1.raw.iloc[:-1]
+            raw2 = pd.concat([t1.raw, t2.raw], ignore_index=True)
+            test.raw = raw2
+        else:
+            test = t2
+        logging.debug(" -> merged with new dataset")
+
+        return test
+
+    def dev_update_make_steps(self, **kwargs):
+        old_steps = self.data.steps.iloc[:-1]
+        # Note! hard-coding header name (might fail if changing default headers)
+        from_data_point = self.data.steps.iloc[-1].point_first
+        new_steps = self.make_step_table(from_data_point=from_data_point, **kwargs)
+        merged_steps = pd.concat([old_steps, new_steps]).reset_index(drop=True)
+        self.data.steps = merged_steps
+
+    def dev_update_make_summary(self, **kwargs):
+        print("NOT FINISHED YET - but not critical")
+        # Update not implemented yet, running full summary calculations for now.
+        # For later:
+        # old_summary = self.data.summary.iloc[:-1]
+        cycle_index_header = self.headers_summary.cycle_index
+        from_cycle = self.data.summary.iloc[-1][cycle_index_header]
+        self.make_summary(from_cycle=from_cycle, **kwargs)
+        # For later:
+        # (Remark! need to solve how to merge cumulated columns)
+        # new_summary = self.make_summary(from_cycle=from_cycle)
+        # merged_summary = pd.concat([old_summary, new_summary]).reset_index(drop=True)
+        # self.data.summary = merged_summary
+
+    def dev_update_from_raw(self, file_names=None, data_points=None, **kwargs):
+        """This method is under development. Using this to develop updating files
+        with only new data.
+        """
+        print("NOT FINISHED YET")
+        # TODO @jepe: implement changes from original from_raw method introduced after this one was last edited.
+        if file_names:
+            self.file_names = file_names
+
+        if file_names is None:
+            logging.info(
+                "No filename given and no stored in the file_names "
+                "attribute. Returning None"
+            )
+            return None
+
+        if not isinstance(self.file_names, (list, tuple)):
+            self.file_names = [file_names]
+
+        raw_file_loader = self.loader
+
+        set_number = 0
+        cell = None
+
+        logging.debug("start iterating through file(s)")
+
+        for f in self.file_names:
+            logging.debug("loading raw file:")
+            logging.debug(f"{f}")
+
+            # get a list of cellpy.readers.core.Data objects
+            # cell = raw_file_loader(f, data_points=data_points, **kwargs)
+            # remark that the bounds are included (i.e. the first datapoint
+            # is 5000.
+
+            logging.debug(
+                "added the data set - merging file info  - oh no; I am not implemented yet"
+            )
+
+            # raw_data_file = copy.deepcopy(test[set_number].raw_data_files[0])
+            # file_size = test[set_number].raw_data_files_length[0]
+
+            # test[set_number].raw_data_files.append(raw_data_file)
+            # test[set_number].raw_data_files_length.append(file_size)
+            # return test
+        # cell[set_number].raw_units = self._set_raw_units()
+        # self.cells.append(cell[set_number])
+        # self.status_dataset = self._validate_cell()
+        # self._invent_a_session_name()
+        return self
+
 
 def get(
     filename=None,
-    mass=None,
     instrument=None,
     instrument_file=None,
+    cellpy_file=None,
+    cycle_mode=None,
+    mass=None,
     nominal_capacity=None,
+    loading=None,
+    area=None,
+    estimate_area=True,
     logging_mode=None,
-    cycle_mode=None,
+    auto_pick_cellpy_format=True,
     auto_summary=True,
+    units=None,
+    step_kwargs=None,
+    summary_kwargs=None,
+    selector=None,
     testing=False,
     **kwargs,
 ):
-    """Create a CellpyData object
+    """Create a CellpyCell object
 
     Args:
-        filename (str, os.PathLike, or list of raw-file names): path to file(s)
-        mass (float): mass of active material (mg) (defaults to mass given in cellpy-file or 1.0)
+        filename (str, os.PathLike, OtherPath, or list of raw-file names): path to file(s) to load
         instrument (str): instrument to use (defaults to the one in your cellpy config file)
         instrument_file (str or path): yaml file for custom file type
-        nominal_capacity (float): nominal capacity for the cell (e.g. used for finding C-rates)
+        cellpy_file (str, os.PathLike, or OtherPath): if both filename (a raw-file) and cellpy_file (a cellpy file)
+            is provided, cellpy will try to check if the raw-file is has been updated since the
+            creation of the cellpy-file and select this instead of the raw file if cellpy thinks
+            they are similar (use with care!).
         logging_mode (str): "INFO" or "DEBUG"
         cycle_mode (str): the cycle mode (e.g. "anode" or "full_cell")
+        mass (float): mass of active material (mg) (defaults to mass given in cellpy-file or 1.0)
+        nominal_capacity (float): nominal capacity for the cell (e.g. used for finding C-rates)
+        loading (float): loading in units [mass] / [area]
+        area (float): active electrode area (e.g. used for finding the areal capacity)
+        estimate_area (bool): calculate area from loading if given (defaults to True)
+        auto_pick_cellpy_format (bool): decide if it is a cellpy-file based on suffix.
         auto_summary (bool): (re-) create summary.
+        units (dict): update cellpy units (used after the file is loaded, e.g. when creating summary).
+        step_kwargs (dict): sent to make_steps
+        summary_kwargs (dict): sent to make_summary
+        selector (dict): passed to load (when loading cellpy-files).
         testing (bool): set to True if testing (will for example prevent making .log files)
         **kwargs: sent to the loader
 
     Returns:
-        CellpyData object (if successful, None if not)
+        CellpyCell object (if successful, None if not)
 
-    """
+    Examples:
+        >>> # read an arbin .res file and create a cellpy object with
+        >>> # populated summary and step-table:
+        >>> c = cellpy.get("my_data.res", instrument="arbin_res", mass=1.14, area=2.12, loading=1.2, nom_cap=155.2)
+        >>>
+        >>> # load a cellpy-file:
+        >>> c = cellpy.get("my_cellpy_file.clp")
+        >>>
+        >>> # load a txt-file exported from Maccor:
+        >>> c = cellpy.get("my_data.txt", instrument="maccor_txt", model="one")
+        >>>
+        >>> # load a raw-file if it is newer than the corresponding cellpy-file,
+        >>> # if not, load the cellpy-file:
+        >>> c = cellpy.get("my_data.res", cellpy_file="my_data.clp")
+        >>>
+        >>> # load a file with a custom file-description:
+        >>> c = cellpy.get("my_file.csv", instrument_file="my_instrument.yaml")
+        >>>
+        >>> # load three subsequent raw-files (of one cell) and merge them:
+        >>> c = cellpy.get(["my_data_01.res", "my_data_02.res", "my_data_03.res"])
+        >>>
+        >>> # load a data set and get the summary charge and discharge capacities
+        >>> # in Ah/g:
+        >>> c = cellpy.get("my_data.res", units=dict(capacity="Ah"))
 
+    """
     from cellpy import log
 
+    db_readers = ["arbin_sql"]
+    instruments_with_colliding_file_suffix = ["arbin_sql_h5"]
+
+    step_kwargs = step_kwargs or {}
+    summary_kwargs = summary_kwargs or {}
+    load_cellpy_file = False
+    logging_mode = "DEBUG" if testing else logging_mode
     log.setup_logging(default_level=logging_mode, testing=testing)
     logging.debug("-------running-get--------")
-    cellpy_instance = CellpyData()
-    logging.debug(f"created CellpyData instance")
-    db_readers = ["arbin_sql"]
+    cellpy_instance = CellpyCell()
+    logging.debug(f"created CellpyCell instance")
+
+    logging.debug(f"{cellpy_file=}")
+    logging.debug(f"{filename=}")
+
+    if filename is None:
+        if cellpy_file is None:
+            logging.info("Running cellpy.get without a filename")
+            logging.info("Returning an empty CellpyCell object.")
+            cellpy_instance = _update_meta(
+                cellpy_instance,
+                cycle_mode=cycle_mode,
+                mass=mass,
+                nominal_capacity=nominal_capacity,
+                area=area,
+                loading=loading,
+                estimate_area=estimate_area,
+                units=units,
+            )
+            return cellpy_instance
+
+        else:
+            load_cellpy_file = True
+            filename = OtherPath(cellpy_file)
+
+    if isinstance(filename, (list, tuple)):
+        logging.debug("got a list or tuple of names")
+        load_cellpy_file = False
+    else:
+        logging.debug("got a single name")
+        logging.debug(f"{filename=}")
+        filename = OtherPath(filename)
+        if (
+            auto_pick_cellpy_format
+            and instrument not in instruments_with_colliding_file_suffix
+            and filename.suffix in [".h5", ".hdf5", ".cellpy", ".cpy"]
+        ):
+            load_cellpy_file = True
+
+    if filename and cellpy_file and not load_cellpy_file:
+        try:
+            similar = cellpy_instance.check_file_ids(filename, cellpy_file)
+            logging.debug(f"checked if the files were similar")
+            if similar:
+                load_cellpy_file = True
+                filename = OtherPath(cellpy_file)
+        except Exception as e:
+            logging.debug(f"Error during checking if similar: {e}")
+            logging.debug("Setting load_cellpy_file to False")
+
+    if load_cellpy_file:
+        logging.info(f"Loading cellpy-file: {filename}")
+        if kwargs.pop("post_processor_hook", None) is not None:
+            logging.warning(
+                "post_processor_hook is not allowed when loading cellpy-files"
+            )
+        cellpy_instance.load(filename, selector=selector, **kwargs)
+        return cellpy_instance
 
+    logging.debug("Prepare for loading raw-file(s)")
     logging.debug(f"checking instrument and instrument_file")
     if instrument_file is not None:
         logging.debug(f"got instrument file {instrument_file=}")
-        # TODO: make tests for this
-        # TODO: align this with the new format
         cellpy_instance.set_instrument(
             instrument="custom", instrument_file=instrument_file
         )
-        # prms.Instruments.custom_instrument_definitions_file = instrument_file
 
     elif instrument is not None:
         logging.debug(f"got instrument in stead of instrument file, {instrument=}")
         model = kwargs.pop("model", None)
-        cellpy_instance.set_instrument(instrument=instrument, model=model)
-
-    if cellpy_instance.tester in db_readers:
-        file_needed = False
-    else:
-        file_needed = True
+        cellpy_instance.set_instrument(instrument=instrument, model=model, **kwargs)
 
-    if cycle_mode is not None:
-        logging.debug("Setting cycle mode")
-        cellpy_instance.cycle_mode = cycle_mode
+    is_a_file = True
+    if cellpy_instance.tester not in db_readers:
+        is_a_file = False
+
+    logging.info(f"Loading raw-file: {filename}")
+    cellpy_instance.from_raw(filename, is_a_file=is_a_file, **kwargs)
+
+    if not cellpy_instance:
+        print("Could not load file: check log!")
+        print("Returning None")
+        return
 
-    if filename is not None:
-        logging.debug(f"{filename=}")
-        if file_needed:
-            if not isinstance(filename, (list, tuple)):
-                filename = Path(filename)
-
-                if not filename.is_file():
-                    print(f"Could not find {filename}")
-                    print("Returning None")
-                    return
+    cellpy_instance = _update_meta(
+        cellpy_instance,
+        cycle_mode=cycle_mode,
+        mass=mass,
+        nominal_capacity=nominal_capacity,
+        area=area,
+        loading=loading,
+        estimate_area=estimate_area,
+        units=units,
+    )
+
+    if auto_summary:
+        logging.info("Creating step table")
+        cellpy_instance.make_step_table(**step_kwargs)
+        logging.info("Creating summary data")
+        cellpy_instance.make_summary(**summary_kwargs)
 
-                if filename.suffix in [".h5", ".hdf5", ".cellpy", ".cpy"]:
-                    logging.info(f"Loading cellpy-file: {filename}")
-                    if kwargs.pop("post_processor_hook", None) is not None:
-                        logging.warning(
-                            "post_processor_hook is not allowed when loading cellpy-files"
-                        )
-                    cellpy_instance.load(filename, **kwargs)
-
-                    # in case the user wants to give another mass to the cell:
-                    if mass is not None:
-                        logging.info(f"Setting mass: {mass}")
-                        cellpy_instance.set_mass(mass)
-                        if auto_summary:
-                            logging.info("Creating step table")
-                            cellpy_instance.make_step_table()
-                            logging.info("Creating summary data")
-                            cellpy_instance.make_summary()
-                    logging.info("Created CellpyData object")
-                    return cellpy_instance
-
-        # raw file
-        logging.info(f"Loading raw-file: {filename}")
-        cellpy_instance.from_raw(filename, **kwargs)
-
-        if not cellpy_instance:
-            print("Could not load file: check log!")
-            print("Returning None")
-            return
+    logging.info("Created CellpyCell object")
+    return cellpy_instance
 
-        logging.debug("raw:")
-        logging.debug(cellpy_instance.cell.raw.head())
 
-        if mass is not None:
-            logging.info(f"Setting mass: {mass}")
-            cellpy_instance.set_mass(mass)
+def _update_meta(
+    cellpy_instance,
+    cycle_mode=None,
+    mass=None,
+    nominal_capacity=None,
+    area=None,
+    loading=None,
+    estimate_area=None,
+    units=None,
+):
+    # TODO: make this a method on CellpyCell
+    if cycle_mode is not None:
+        logging.debug("Setting cycle mode")
+        cellpy_instance.cycle_mode = cycle_mode
 
-        if nominal_capacity is not None:
-            logging.info(f"Setting nominal capacity: {nominal_capacity}")
-            cellpy_instance.set_nom_cap(nominal_capacity)
-
-        if auto_summary:
-            logging.info("Creating step table")
-            cellpy_instance.make_step_table()
-            logging.info("Creating summary data")
-            cellpy_instance.make_summary()
+    if units is not None:
+        logging.debug(f"Updating units: {units}")
+        cellpy_instance.cellpy_units.update(units)
+
+    if mass is not None:
+        logging.info(f"Setting mass: {mass}")
+        cellpy_instance.data.mass = mass
+
+    if nominal_capacity is not None:
+        logging.info(f"Setting nominal capacity: {nominal_capacity}")
+        cellpy_instance.data.nom_cap = nominal_capacity
+
+    if area is not None:
+        logging.debug(f"got area: {area}")
+        cellpy_instance.data.meta_common.active_electrode_area = area
+
+    elif loading and estimate_area:
+        logging.debug("-------------AREA-CALC----------------")
+        logging.debug(f"got loading: {logging}")
+        area = cellpy_instance.data.mass / loading
+        logging.debug(
+            f"calculating area from loading ({loading}) and mass ({cellpy_instance.data.mass}): {area}"
+        )
+        cellpy_instance.data.meta_common.active_electrode_area = area
     else:
-        if mass:
-            prms.Materials.default_mass = mass
-            prms.Materials.default_mass = mass
-        if nominal_capacity:
-            prms.DataSet.nom_cap = nominal_capacity
+        logging.debug("using default area")
 
-    logging.info("Created CellpyData object")
     return cellpy_instance
 
 
+# ============== Internal tests =================
+
+
 def check_raw():
+    import cellpy
     from cellpy.utils import example_data
 
-    cellpy_data_instance = CellpyData()
+    cellpy_data_instance = CellpyCell()
     res_file_path = example_data.arbin_file_path()
-    cellpy_data_instance.loadcell(res_file_path)
-    run_number = 0
+    cellpy.get(res_file_path)
+
     data_point = 2283
     step_time = 1500.05
     sum_discharge_time = 362198.12
-    my_test = cellpy_data_instance.cells[run_number]
+    my_test = cellpy_data_instance.data
 
     summary = my_test.summary
     raw = my_test.raw
     print(summary.columns)
     print(summary.index)
     print(summary.head())
     print(summary.iloc[1, 1])
     print(summary.loc[:, "Data_Point"])
     print(summary.loc[1, "Data_Point"])
 
     print(raw.columns)
     # assert my_test.summary.loc["1", "data_point"] == data_point
-    assert my_test.cell_no == run_number
 
 
 def check_cellpy_file():
     print("running", end=" ")
     print(sys.argv[0])
-    import logging
 
     from cellpy import log
 
     log.setup_logging(default_level="DEBUG")
 
     from cellpy.utils import example_data
 
     f = example_data.cellpy_file_path()
     print(f)
     print(f.is_file())
-    c = CellpyData()
+    c = CellpyCell()
     c.dev_load(f, accept_old=True)
     c.make_step_table()
     c.make_summary()
     print("Here we have it")
-    print(c.cell.summary.columns)
-    print(c.cell.steps.columns)
-    print(c.cell.raw.columns)
+    print(c.data.summary.columns)
+    print(c.data.steps.columns)
+    print(c.data.raw.columns)
+
+
+def save_and_load_cellpy_file():
+    # check to see if updating to new cellpy file version works
+    """
+    # How to update the cellpy file version
+
+    ## Top level
+
+    ## Metadata
+
+    ## Summary, Raw, and Step headers
+
+    update_headers.py
+
+    """
+
+    f00 = Path("../../testdata/hdf5/20160805_test001_45_cc.h5")
+    f04 = Path("../../testdata/hdf5/20160805_test001_45_cc_v4.h5")
+    f05 = Path("../../testdata/hdf5/20160805_test001_45_cc_v5.h5")
+    f06 = Path("../../testdata/hdf5/20160805_test001_45_cc_v6.h5")
+    f07 = Path("../../testdata/hdf5/20160805_test001_45_cc_v7.h5")
+    f08 = Path("../../testdata/hdf5/20160805_test001_45_cc_v8.h5")
+    f_tmp = Path("../../tmp/v1.h5")
+
+    old = f08
+    out = f_tmp
+
+    print("LOADING ORIGINAL".center(80, "*"))
+    c = get(old)
+    for a in dir(c.data):
+        if not a.startswith("__"):
+            if a not in ["raw", "summary", "steps"]:
+                v = getattr(c.data, a)
+                print(f"{a}: {v}")
+
+    print("SAVING".center(80, "*"))
+    c.save(out)
+
+    print("LOADING NEW".center(80, "*"))
+    c = get(out)  # , logging_mode="DEBUG"
+    meta_test_dependent = c.data.meta_test_dependent
+    meta_common = c.data.meta_common
+    print(f"{meta_test_dependent=}")
+    print(f"{meta_common=}")
+    print(f"{c.data.raw_limits=}")
+    print(f"{c.data.raw_units=}")
+
+    for a in dir(c.data):
+        if not a.startswith("__"):
+            if a not in ["raw", "summary", "steps"]:
+                v = getattr(c.data, a)
+                print(f"{a}: {v}")
+    # print("Here we have it")
+    # print(c.data.summary.columns)
+    # print(c.data.steps.columns)
+    # print(c.data.raw.columns)
 
 
 if __name__ == "__main__":
-    check_raw()
+    save_and_load_cellpy_file()
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/core.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/base.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,883 +1,730 @@
-""" This module contains several of the most important classes used in cellpy.
+"""
+When you make a new loader you have to subclass the Loader class.
+Remember also to register it in cellpy.cellreader.
 
-It also contains functions that are used by readers and utils. And it has the file-
-version definitions.
+(for future development, not used very efficiently yet).
 """
-import datetime
-import importlib
+
+import abc
 import logging
-import os
 import pathlib
-import pickle
-import sys
-import time
-from typing import Any, Tuple, Dict
+import shutil
+import tempfile
+from abc import ABC
+from typing import List, Union
 
-import numpy as np
 import pandas as pd
-from scipy import interpolate
 
-from cellpy.exceptions import NullData
-from cellpy.parameters import prms
-from cellpy.parameters.internal_settings import (
-    ATTRS_CELLPYFILE,
-    get_headers_normal,
-    get_headers_step_table,
-    get_headers_summary,
-    get_default_raw_units,
-    get_default_raw_limits,
+import cellpy.internals.core
+import cellpy.readers.core as core
+from cellpy.parameters.internal_settings import headers_normal
+from cellpy.readers.instruments.configurations import (
+    ModelParameters,
+    register_configuration_from_module,
+)
+from cellpy.readers.instruments.processors import post_processors, pre_processors
+from cellpy.readers.instruments.processors.post_processors import (
+    ORDERED_POST_PROCESSING_STEPS,
 )
 
-CELLPY_FILE_VERSION = 6
-MINIMUM_CELLPY_FILE_VERSION = 4
-STEP_TABLE_VERSION = 5
-RAW_TABLE_VERSION = 5
-SUMMARY_TABLE_VERSION = 5
-PICKLE_PROTOCOL = 4
-
-HEADERS_NORMAL = get_headers_normal()
-HEADERS_SUMMARY = get_headers_summary()
-HEADERS_STEP_TABLE = get_headers_step_table()
-
-
-# https://stackoverflow.com/questions/60067953/
-# 'is-it-possible-to-specify-the-pickle-protocol-when-writing-pandas-to-hdf5
-class PickleProtocol:
-    def __init__(self, level):
-        self.previous = pickle.HIGHEST_PROTOCOL
-        self.level = level
-
-    def __enter__(self):
-        importlib.reload(pickle)
-        pickle.HIGHEST_PROTOCOL = self.level
-
-    def __exit__(self, *exc):
-        importlib.reload(pickle)
-        pickle.HIGHEST_PROTOCOL = self.previous
-
-
-def pickle_protocol(level):
-    return PickleProtocol(level)
-
-
-class FileID:
-    """class for storing information about the raw-data files.
-
-    This class is used for storing and handling raw-data file information.
-    It is important to keep track of when the data was extracted from the
-    raw-data files so that it is easy to know if the hdf5-files used for
-    @storing "treated" data is up-to-date.
-
-    Attributes:
-        name (str): Filename of the raw-data file.
-        full_name (str): Filename including path of the raw-data file.
-        size (float): Size of the raw-data file.
-        last_modified (datetime): Last time of modification of the raw-data
-            file.
-        last_accessed (datetime): last time of access of the raw-data file.
-        last_info_changed (datetime): st_ctime of the raw-data file.
-        location (str): Location of the raw-data file.
+MINIMUM_SELECTION = [
+    "Data_Point",
+    "Test_Time",
+    "Step_Time",
+    "DateTime",
+    "Step_Index",
+    "Cycle_Index",
+    "Current",
+    "Voltage",
+    "Charge_Capacity",
+    "Discharge_Capacity",
+    "Internal_Resistance",
+]
+
+
+# TODO: move this to another module (e.g. inside processors):
+def find_delimiter_and_start(
+    file_name,
+    separators=None,
+    checking_length_header=30,
+    checking_length_whole=200,
+):
+    """function to automatically detect the delimiter and what line the first data appears on.
 
-    """
+    Remark! This function is rather simple, it splits the data into to parts
+        (possible header part (checking_length_header) and the rest of the data). Then it counts the appearances of
+        the different possible delimiters in the rest of the data part, and then selects a delimiter if it has unique
+        counts for all the lines.
 
-    def __init__(self, filename=None):
-        make_defaults = True
-        if filename:
-            if os.path.isfile(filename):
-                fid_st = os.stat(filename)
-                self.name = os.path.abspath(filename)
-                self.full_name = filename
-                self.size = fid_st.st_size
-                self.last_modified = fid_st.st_mtime
-                self.last_accessed = fid_st.st_atime
-                self.last_info_changed = fid_st.st_ctime
-                self.location = os.path.dirname(filename)
-                self.last_data_point = 0  # used later when updating is implemented
-                make_defaults = False
-
-        if make_defaults:
-            self.name = None
-            self.full_name = None
-            self.size = 0
-            self.last_modified = None
-            self.last_accessed = None
-            self.last_info_changed = None
-            self.location = None
-            self._last_data_point = 0  # to be used later when updating is implemented
-
-    def __str__(self):
-        txt = "\n<fileID>\n"
-        txt += f"full name: {self.full_name}\n"
-        txt += f"name: {self.name}\n"
-        txt += f"location: {self.location}\n"
-        if self.last_modified is not None:
-            txt += f"modified: {self.last_modified}\n"
-        else:
-            txt += "modified: NAN\n"
-        if self.size is not None:
-            txt += f"size: {self.size}\n"
-        else:
-            txt += "size: NAN\n"
+        The first line is defined as where the delimiter is used same number of times (probably a header line).
+    """
 
-        txt += f"last data point: {self.last_data_point}\n"
-        return txt
+    if separators is None:
+        separators = [";", "\t", "|", ","]
+    logging.debug(f"checking internals of the file {file_name}")
+
+    empty_lines = 0
+    with open(file_name, "r") as fin:
+        lines = []
+        for j in range(checking_length_whole):
+            line = fin.readline()
+            if not line:
+                break
+            if len(line.strip()):
+                lines.append(line)
+            else:
+                empty_lines += 1
 
-    @property
-    def last_data_point(self):
-        # TODO: consider including a method here to find the last data point (raw data)
-        # ideally, this value should be set when loading the raw data before
-        # merging files (if it consists of several files)
-        return self._last_data_point
-
-    @last_data_point.setter
-    def last_data_point(self, value):
-        self._last_data_point = value
+    checking_length_whole -= empty_lines
+    if checking_length_header - empty_lines < 1:
+        checking_length_header = checking_length_whole // 2
+    separator, number_of_hits = _find_separator(
+        checking_length_whole - checking_length_header, lines, separators
+    )
 
-    def populate(self, filename):
-        """Finds the file-stats and populates the class with stat values.
+    if separator is None:
+        raise IOError(f"could not decide delimiter in {file_name}")
 
-        Args:
-            filename (str): name of the file.
-        """
+    if separator == "\t":
+        logging.debug("seperator = TAB")
+    elif separator == " ":
+        logging.debug("seperator = SPACE")
+    else:
+        logging.debug(f"seperator = {separator}")
 
-        if os.path.isfile(filename):
-            fid_st = os.stat(filename)
-            self.name = os.path.abspath(filename)
-            self.full_name = filename
-            self.size = fid_st.st_size
-            self.last_modified = fid_st.st_mtime
-            self.last_accessed = fid_st.st_atime
-            self.last_info_changed = fid_st.st_ctime
-            self.location = os.path.dirname(filename)
+    first_index = _find_first_line_whit_delimiter(
+        checking_length_header, lines, number_of_hits, separator
+    )
+    logging.debug(f"First line with delimiter: {first_index}")
+    return separator, first_index
 
-    def get_raw(self):
-        """Get a list with information about the file.
 
-        The returned list contains name, size, last_modified and location.
-        """
-        return [self.name, self.size, self.last_modified, self.location]
+def _find_first_line_whit_delimiter(
+    checking_length_header, lines, number_of_hits, separator
+):
+    first_part = lines[:checking_length_header]
+    if number_of_hits is None:
+        # remark! if number of hits (i.e. how many separators pr line) is not given, we set it to the amount of
+        # separators we find in the third last line.
+        number_of_hits = lines[-3].count(separator)
+    return [
+        line_number
+        for line_number, line in enumerate(first_part)
+        if line.count(separator) == number_of_hits
+    ][0]
+
+
+def _find_separator(checking_length, lines, separators):
+    logging.debug("searching for separators")
+    separator = None
+    number_of_hits = None
+    last_part = lines[
+        checking_length:-1
+    ]  # don't include last line since it might be corrupted
+    check_sep = dict()
+
+    for i, v in enumerate(separators):
+        check_sep[i] = [line.count(v) for line in last_part]
+
+    unique_sep_counts = {i: set(v) for i, v in check_sep.items()}
+
+    for index, value in unique_sep_counts.items():
+        value_as_list = list(value)
+        number_of_hits = value_as_list[0]
+        if len(value_as_list) == 1 and number_of_hits > 0:
+            separator = separators[index]
+            break
 
-    def get_name(self):
-        """Get the filename."""
-        return self.name
-
-    def get_size(self):
-        """Get the size of the file."""
-        return self.size
-
-    def get_last(self):
-        """Get last modification time of the file."""
-        return self.last_modified
-
-
-class Cell:
-    """Object to store data for a test.
-
-    This class is used for storing all the relevant data for a 'run', i.e. all
-    the data collected by the tester as stored in the raw-files.
-
-    Attributes:
-        test_no (int): test number.
-        mass (float): mass of electrode [mg].
-        dfdata (pandas.DataFrame): contains the experimental data points.
-        dfsummary (pandas.DataFrame): contains summary of the data pr. cycle.
-        step_table (pandas.DataFrame): information for each step, used for
-            defining type of step (charge, discharge, etc.)
+    return separator, number_of_hits
 
-    """
 
-    def _repr_html_(self):
-        obj = f"<b>Cell-object</b> id={hex(id(self))}"
-        txt = "<p>"
-        for p in dir(self):
-            if not p.startswith("_"):
-                if p not in ["raw", "summary", "steps", "logger"]:
-                    value = self.__getattribute__(p)
-                    txt += f"<b>{p}</b>: {value}<br>"
-        txt += "</p>"
-        try:
-            raw_txt = f"<p><b>raw data-frame (summary)</b><br>{self.raw.describe()._repr_html_()}</p>"
-            raw_txt += f"<p><b>raw data-frame (head)</b><br>{self.raw.head()._repr_html_()}</p>"
-        except AttributeError:
-            raw_txt = "<p><b>raw data-frame </b><br> not found!</p>"
-        except ValueError:
-            raw_txt = "<p><b>raw data-frame </b><br> does not contain any columns!</p>"
-
-        try:
-            summary_txt = f"<p><b>summary data-frame (summary)</b><br>{self.summary.describe()._repr_html_()}</p>"
-            summary_txt += f"<p><b>summary data-frame (head)</b><br>{self.summary.head()._repr_html_()}</p>"
-        except AttributeError:
-            summary_txt = "<p><b>summary data-frame </b><br> not found!</p>"
-        except ValueError:
-            summary_txt = (
-                "<p><b>summary data-frame </b><br> does not contain any columns!</p>"
-            )
+def query_csv(
+    self,
+    name,
+    sep=None,
+    skiprows=None,
+    header=None,
+    encoding=None,
+    decimal=None,
+    thousands=None,
+):
+    logging.debug(f"parsing with pandas.read_csv: {name}")
+    sep = sep or self.sep
+    skiprows = skiprows or self.skiprows
+    header = header or self.header
+    encoding = encoding or self.encoding
+    decimal = decimal or self.decimal
+    thousands = thousands or self.thousands
+    logging.critical(f"{sep=}, {skiprows=}, {header=}, {encoding=}, {decimal=}")
+    data_df = pd.read_csv(
+        name,
+        sep=sep,
+        skiprows=skiprows,
+        header=header,
+        encoding=encoding,
+        decimal=decimal,
+        thousands=thousands,
+    )
+    return data_df
 
-        try:
-            steps_txt = f"<p><b>steps data-frame (summary)</b><br>{self.steps.describe()._repr_html_()}</p>"
-            steps_txt += f"<p><b>steps data-frame (head)</b><br>{self.steps.head()._repr_html_()}</p>"
-        except AttributeError:
-            steps_txt = "<p><b>steps data-frame </b><br> not found!</p>"
-        except ValueError:
-            steps_txt = (
-                "<p><b>steps data-frame </b><br> does not contain any columns!</p>"
-            )
 
-        return obj + txt + summary_txt + steps_txt + raw_txt
+class AtomicLoad:
+    """Atomic loading class"""
 
-    def __init__(self, **kwargs):
-        self.logger = logging.getLogger(__name__)
-        self.logger.debug("created DataSet instance")
-
-        # meta-data
-        self.cell_no = None
-        self.mass = prms.Materials.default_mass  # active material (in mg)
-        self.tot_mass = prms.Materials.default_mass  # total material (in mg)
-        self.no_cycles = 0.0
-        self.charge_steps = None
-        self.discharge_steps = None
-        self.ir_steps = None
-        self.ocv_steps = None
-        self.nom_cap = prms.DataSet.nom_cap  # mAh/g (for finding c-rates)
-        self.mass_given = False
-        self.material = prms.Materials.default_material
-        self.merged = False
-        self.file_errors = None  # not in use at the moment
-        self.loaded_from = None  # loaded from (can be list if merged)
-        self.channel_index = None
-        self.channel_number = None
-        self.creator = None
-        self.item_ID = None
-        self.schedule_file_name = None
-        self.start_datetime = None
-        self.test_ID = None
-        self.name = None
-
-        # new meta data
-        self.voltage_lim_low = None
-        self.voltage_lim_high = None
-        self.cycle_mode = prms.Reader.cycle_mode
-        self.active_electrode_area = None  # [cm2]
-        self.active_electrode_thickness = None  # [micron]
-        self.electrolyte_type = None  #
-        self.electrolyte_volume = None  # [micro-liter]
-        self.active_electrode_type = None
-        self.counter_electrode_type = None
-        self.reference_electrode_type = None
-        self.experiment_type = None
-        self.cell_type = None
-        self.separator_type = None
-        self.active_electrode_current_collector = None
-        self.reference_electrode_current_collector = None
-        self.comment = None
-
-        # custom meta-data
-        for k in kwargs:
-            if hasattr(self, k):
-                setattr(self, k, kwargs[k])
-
-        # methods in CellpyData to update if adding new attributes:
-        # ATTRS_CELLPYFILE
-
-        # place to put "checks" etc:
-        # _extract_meta_from_cellpy_file
-        # _create_infotable()
-
-        self.raw_data_files = []
-        self.raw_data_files_length = []
-        self.raw_units = get_default_raw_units()
-        self.raw_limits = get_default_raw_limits()
-
-        self.raw = pd.DataFrame()
-        self.summary = pd.DataFrame()
-        self.steps = pd.DataFrame()  # is this used? - check!
-        self.summary_table_version = SUMMARY_TABLE_VERSION
-        self.step_table_version = STEP_TABLE_VERSION
-        self.cellpy_file_version = CELLPY_FILE_VERSION
-        self.raw_table_version = RAW_TABLE_VERSION
+    instrument_name = "atomic_loader"
 
-    @staticmethod
-    def _header_str(hdr):
-        txt = "\n"
-        txt += 80 * "-" + "\n"
-        txt += f" {hdr} ".center(80) + "\n"
-        txt += 80 * "-" + "\n"
-        return txt
-
-    def __str__(self):
-        txt = "<DataSet>\n"
-        txt += "loaded from file\n"
-        if isinstance(self.loaded_from, (list, tuple)):
-            for f in self.loaded_from:
-                txt += str(f)
-                txt += "\n"
+    _name = None
+    _temp_file_path = None
+    _fid = None
+    _is_db: bool = False
+    _copy_also_local: bool = True
 
-        else:
-            txt += str(self.loaded_from)
-            txt += "\n"
-        txt += "\n* GLOBAL\n"
-        txt += f"material:            {self.material}\n"
-        txt += f"mass (active):       {self.mass}\n"
-        txt += f"test ID:             {self.test_ID}\n"
-        txt += f"mass (total):        {self.tot_mass}\n"
-        txt += f"nominal capacity:    {self.nom_cap}\n"
-        txt += f"channel index:       {self.channel_index}\n"
-        txt += f"DataSet name:        {self.name}\n"
-        txt += f"creator:             {self.creator}\n"
-        txt += f"schedule file name:  {self.schedule_file_name}\n"
-
-        try:
-            if self.start_datetime:
-                start_datetime_str = xldate_as_datetime(self.start_datetime)
-            else:
-                start_datetime_str = "Not given"
-        except AttributeError:
-            start_datetime_str = "NOT READABLE YET"
-
-        txt += f"start-date:         {start_datetime_str}\n"
-
-        txt += self._header_str("DATA")
-        try:
-            txt += str(self.raw.describe())
-        except (AttributeError, ValueError):
-            txt += "EMPTY (Not processed yet)\n"
-
-        txt += self._header_str("SUMMARY")
-        try:
-            txt += str(self.summary.describe())
-        except (AttributeError, ValueError):
-            txt += "EMPTY (Not processed yet)\n"
-
-        txt += self._header_str("STEP TABLE")
-        try:
-            txt += str(self.steps.describe())
-            txt += str(self.steps.head())
-        except (AttributeError, ValueError):
-            txt += "EMPTY (Not processed yet)\n"
-
-        txt += self._header_str("RAW UNITS")
-        try:
-            txt += str(self.raw.describe())
-            txt += str(self.raw.head())
-        except (AttributeError, ValueError):
-            txt += "EMPTY (Not processed yet)\n"
-        return txt
+    @property
+    def is_db(self):
+        """Is the file stored in the database"""
+        return self._is_db
+
+    @is_db.setter
+    def is_db(self, value: bool):
+        """Is the file stored in the database"""
+        self._is_db = value
 
     @property
-    def has_summary(self):
-        """check if the summary table exists"""
-        try:
-            empty = self.summary.empty
-        except AttributeError:
-            empty = True
-        return not empty
+    def name(self):
+        """The name of the file to be loaded"""
+        return self._name
+
+    @name.setter
+    def name(self, value):
+        """The name of the file to be loaded"""
+        if not self.is_db and not isinstance(value, cellpy.internals.core.OtherPath):
+            logging.debug("converting to OtherPath")
+            value = cellpy.internals.core.OtherPath(value)
+        self._name = value
 
     @property
-    def has_steps(self):
-        """check if the step table exists"""
-        try:
-            empty = self.steps.empty
-        except AttributeError:
-            empty = True
-        return not empty
+    def temp_file_path(self):
+        """The name of the file to be loaded if copied to a temporary file"""
+        return self._temp_file_path
+
+    @temp_file_path.setter
+    def temp_file_path(self, value):
+        """The name of the file to be loaded if copied to a temporary file"""
+        self._temp_file_path = value
 
     @property
-    def has_data(self):
-        try:
-            empty = self.raw.empty
-        except AttributeError:
-            empty = True
-        return not empty
+    def fid(self):
+        """The unique file id"""
+        if self._fid is None:
+            self.generate_fid()
+        return self._fid
+
+    def generate_fid(self, value=None):
+        """Generate a unique file id"""
+        if self.is_db:
+            self._fid = core.FileID(self.name, is_db=True)
+        elif self._temp_file_path is not None:
+            self._fid = core.FileID(self.name)
+        elif self._name is not None:
+            self._fid = core.FileID(self.name)
+        elif value is not None:
+            self._fid = core.FileID(value)
+        else:
+            raise ValueError("could not generate fid")
 
+    def copy_to_temporary(self):
+        """Copy file to a temporary file"""
 
-class InstrumentFactory:
-    def __init__(self):
-        self._builders = {}
-        self._kwargs = {}
+        logging.debug(f"external file received? {self.name.is_external=}")
+        if self.name is None:
+            raise ValueError("no file name given to loader class (self.name is None)")
+
+        if not self._copy_also_local and not self.name.is_external:
+            self._temp_file_path = self.name
+            return
+
+        self._temp_file_path = self.name.copy()
+
+    def loader_executor(self, *args, **kwargs):
+        """Load the file"""
+        name = args[0]
+        self.name = name
+        if not self.is_db:
+            self.copy_to_temporary()
+        cellpy_data = self.loader(*args, **kwargs)
+        return cellpy_data
 
-    def register_builder(self, key: str, builder: Tuple[str, Any], **kwargs) -> None:
-        """register an instrument loader module.
+    def loader(self, *args, **kwargs):
+        """The method that does the actual loading.
 
-        Args:
-            key: instrument id
-            builder: (module_name, module_path)
-            **kwargs: stored in the factory (will be used in the future for allowing to set
-               defaults to the builders to allow for using .query).
+        This method should be overwritten by the specific loader class.
         """
+        ...
 
-        logging.debug(f"Registering instrument {key}")
-        self._builders[key] = builder
-        self._kwargs[key] = kwargs
 
-    def create(self, key: str, **kwargs):
-        """Create the instrument loader module and initialize the loader class.
+class BaseLoader(AtomicLoad, metaclass=abc.ABCMeta):
+    """Main loading class"""
 
-        Args:
-            key: instrument id
-            **kwargs: sent to the initializer of the loader class.
+    instrument_name = "base_loader"
+
+    # TODO: should also include the functions for getting cellpy headers etc
+    #  here
+
+    @staticmethod
+    @abc.abstractmethod
+    def get_raw_units() -> dict:
+        """Include the settings for the units used by the instrument.
+
+        This is needed for example when converting the capacity to a specific capacity.
+        So far, it has been difficult to get any kind of consensus on what the most optimal
+        units are for storing cycling data. Therefore, cellpy implements three levels of units:
+        1) the raw units that the data is loaded in already has and 2) the cellpy units used by cellpy
+        when generating summaries and related information, and 3) output units that can be set to get the data
+        in a specif unit when exporting or creating specific outputs such as ICA.
+
+        Comment 2022.09.11::
+
+            still not sure if we should use raw units or cellpy units in the cellpy-files (.h5/ .cellpy).
+            Currently, the summary is in cellpy units and the raw and step data is in raw units. If
+            you have any input on this topic, let us know.
+
+        The units are defined w.r.t. the SI units ('unit-fractions'; currently only units that are multiples of
+        Si units can be used). For example, for current defined in mA, the value for the
+        current unit-fraction will be 0.001.
+
+        The internal cellpy units are given in the ``cellpy_units`` attribute.
 
         Returns:
-            instance of loader class.
-        """
+            dictionary of units (str)
 
-        module_name, module_path = self._builders.get(key, (None, None))
+        Example:
+            A minimum viable implementation::
 
-        # constant:
-        instrument_class = "DataLoader"
+                @staticmethod
+                def get_raw_units():
+                    raw_units = dict()
+                    raw_units["current"] = "A"
+                    raw_units["charge"] = "Ah"
+                    raw_units["mass"] = "g"
+                    raw_units["voltage"] = "V"
+                    return raw_units
 
-        if not module_name:
-            raise ValueError(key)
+        """
+        raise NotImplementedError
 
-        spec = importlib.util.spec_from_file_location(module_name, module_path)
-        loader_module = importlib.util.module_from_spec(spec)
-        sys.modules[module_name] = loader_module
-        spec.loader.exec_module(loader_module)
-        cls = getattr(loader_module, instrument_class)
+    @abc.abstractmethod
+    def get_raw_limits(self) -> dict:
+        """Include the settings for how to decide what kind of step you are examining here.
+
+        The raw limits are 'epsilons' used to check if the current and/or voltage is stable (for example
+        for galvanostatic steps, one would expect that the current is stable (constant) and non-zero).
+        If the (accumulated) change is less than 'epsilon', then cellpy interpret it to be stable.
+        It is expected that different instruments (with different resolution etc.) have different
+        resolutions and noice levels, thus different 'epsilons'.
 
-        # TODO: get stored kwargs from self.__kwargs and merge them with the supplied kwargs
-        #  (supplied should have preference)
+        Returns: the raw limits (dict)
 
-        return cls(**kwargs)
+        """
+        raise NotImplementedError
 
-    def query(self, key: str, variable: str) -> Any:
-        """performs a get_params lookup for the instrument loader.
+    @classmethod
+    def get_params(cls, parameter: Union[str, None]) -> dict:
+        """Retrieves parameters needed for facilitating working with the
+        instrument without registering it.
 
-        Args:
-            key: instrument id.
-            variable: the variable you want to lookup.
+        Typically, it should include the name and raw_ext.
 
-        Returns:
-            The value of the variable if the loaders get_params method supports it.
+        Return: parameters or a selected parameter
         """
-        loader = self.create(key)
-        try:
-            value = loader.get_params(variable)
-            logging.debug(f"GOT {variable}={value} for {key}")
-            return value
-
-        except (AttributeError, NotImplementedError, KeyError):
-            logging.debug(f"COULD NOT RETRIEVE {variable} for {key}")
-        return
-
-
-def generate_default_factory():
-    instrument_factory = InstrumentFactory()
-    instruments = find_all_instruments()
-    for instrument_id, instrument in instruments.items():
-        instrument_factory.register_builder(instrument_id, instrument)
-    return instrument_factory
-
-
-def find_all_instruments() -> Dict[str, Tuple[str, str]]:
-    """finds all the supported instruments"""
-
-    import cellpy.readers.instruments as hard_coded_instruments_site
-
-    instruments_found = {}
-    logging.debug("Searching for modules in base instrument folder:")
-
-    hard_coded_instruments_site = pathlib.Path(
-        hard_coded_instruments_site.__file__
-    ).parent
-    modules_in_hard_coded_instruments_site = [
-        s
-        for s in hard_coded_instruments_site.glob("*.py")
-        if not (
-            str(s.name).startswith("_")
-            or str(s.name).startswith("dev_")
-            or str(s.name).startswith("base")
-            or str(s.name).startswith("backup")
-            or str(s.name).startswith("registered_loaders")
-        )
-    ]
 
-    for module_path in modules_in_hard_coded_instruments_site:
-        module_name = module_path.name.rstrip(".py")
-        logging.debug(module_name)
-        instruments_found[module_name] = (
-            module_name,
-            module_path,
-        )
-        logging.debug(" -> added")
+        return getattr(cls, parameter)
 
-    logging.debug("Searching for module configurations in user instrument folder:")
-    # These are only yaml-files and should ideally import the appropriate
-    #    custom loader class
-    # Might not be needed.
-    logging.debug("- Not implemented yet")
+    @abc.abstractmethod
+    def loader(self, *args, **kwargs) -> list:
+        """Loads data into a Data object and returns it"""
+        # This method is used by cellreader through the AtomicLoad.loader_executor method.
+        # It should be overwritten by the specific loader class.
+        #
+        # Notice that it is highly recommended that you don't try to implement .loader_executor yourself
+        # in your subclass!
+        pass
 
-    logging.debug("Searching for modules through plug-ins:")
-    # Not sure how to do this yet. Probably also some importlib trick.
-    logging.debug("- Not implemented yet")
-    return instruments_found
+    @staticmethod
+    def identify_last_data_point(data: core.Data) -> core.Data:
+        """This method is used to find the last record in the data."""
+        return core.identify_last_data_point(data)
 
 
-def identify_last_data_point(data):
-    """Find the last data point and store it in the fid instance"""
+class AutoLoader(BaseLoader):
+    """Main autoload class.
 
-    logging.debug("searching for last data point")
-    hdr_data_point = HEADERS_NORMAL.data_point_txt
-    try:
-        if hdr_data_point in data.raw.columns:
+    This class can be sub-classed if you want to make a data-reader for different type of "easily parsed" files
+    (for example csv-files). The subclass needs to have at least one
+    associated CONFIGURATION_MODULE defined and must have the following attributes as minimum::
 
-            last_data_point = data.raw[hdr_data_point].max()
-        else:
-            last_data_point = data.raw.index.max()
-    except AttributeError:
-        logging.debug("AttributeError - setting last data point to 0")
-        last_data_point = 0
-    if not last_data_point > 0:
-        last_data_point = 0
-    data.raw_data_files[0].last_data_point = last_data_point
-    logging.debug(f"last data point: {last_data_point}")
-    return data
-
-
-def check64bit(current_system="python"):
-    """checks if you are on a 64 bit platform"""
-    if current_system == "python":
-        return sys.maxsize > 2147483647
-    elif current_system == "os":
-        import platform
-
-        pm = platform.machine()
-        if pm != ".." and pm.endswith("64"):  # recent Python (not Iron)
-            return True
-        else:
-            if "PROCESSOR_ARCHITEW6432" in os.environ:
-                return True  # 32 bit program running on 64 bit Windows
-            try:
-                # 64 bit Windows 64 bit program
-                return os.environ["PROCESSOR_ARCHITECTURE"].endswith("64")
-            except IndexError:
-                pass  # not Windows
-            try:
-                # this often works in Linux
-                return "64" in platform.architecture()[0]
-            except Exception:
-                # is an older version of Python, assume also an older os@
-                # (best we can guess)
-                return False
-
-
-def humanize_bytes(b, precision=1):
-    """Return a humanized string representation of a number of b."""
-
-    abbrevs = (
-        (1 << 50, "PB"),
-        (1 << 40, "TB"),
-        (1 << 30, "GB"),
-        (1 << 20, "MB"),
-        (1 << 10, "kB"),
-        (1, "b"),
-    )
-    if b == 1:
-        return "1 byte"
-    for factor, suffix in abbrevs:
-        if b >= factor:
-            break
-    # return '%.*f %s' % (precision, old_div(b, factor), suffix)
-    return "%.*f %s" % (precision, b // factor, suffix)
+        default_model: str = NICK_NAME_OF_DEFAULT_CONFIGURATION_MODULE
+        supported_models: dict = SUPPORTED_MODELS
+
+    where SUPPORTED_MODELS is a dictionary with {NICK_NAME : CONFIGURATION_MODULE_NAME}  key-value pairs.
+    Remark! the NICK_NAME must be in upper-case!
 
+    It is also possible to set these in a custom pre_init method::
 
-def xldate_as_datetime(xldate, datemode=0, option="to_datetime"):
-    """Converts a xls date stamp to a more sensible format.
+        @classmethod
+        def pre_init(cls):
+            cls.default_model: str = NICK_NAME_OF_DEFAULT_CONFIGURATION_MODULE
+            cls.supported_models: dict = SUPPORTED_MODELS
 
-    Args:
-        xldate (str): date stamp in Excel format.
-        datemode (int): 0 for 1900-based, 1 for 1904-based.
-        option (str): option in ("to_datetime", "to_float", "to_string"),
-            return value
+    or turn off automatic registering of configuration::
 
-    Returns:
-        datetime (datetime object, float, or string).
+        @classmethod
+        def pre_init(cls):
+            cls.auto_register_config = False  # defaults to True
+
+    During initialisation of the class, if ``auto_register_config == True``,  it will dynamically load the definitions
+    provided in the CONFIGURATION_MODULE.py located in the ``cellpy.readers.instruments.configurations``
+    folder/package.
+
+    Attributes can be set during initialisation of the class as **kwargs that are then handled by the
+    ``parse_formatter_parameters`` method.
+
+    Remark that some also can be provided as arguments to the ``loader`` method and will then automatically
+    be "transparent" to the ``cellpy.get`` function. So if you would like to give the user access to modify
+    these arguments, you should implement them in the ``parse_loader_parameters`` method.
 
     """
 
-    # This does not work for numpy-arrays
+    instrument_name = "auto_loader"
 
-    if option == "to_float":
-        d = (xldate - 25589) * 86400.0
-    else:
-        try:
-            d = datetime.datetime(1899, 12, 30) + datetime.timedelta(
-                days=xldate + 1462 * datemode
+    def __init__(self, *args, **kwargs):
+        self.auto_register_config = True
+        self.pre_init()
+
+        if not hasattr(self, "supported_models"):
+            raise AttributeError(
+                f"missing attribute in sub-class of TxtLoader: supported_models"
+            )
+        if not hasattr(self, "default_model"):
+            raise AttributeError(
+                f"missing attribute in sub-class of TxtLoader: default_model"
             )
-            # date_format = "%Y-%m-%d %H:%M:%S:%f" # with microseconds,
-            # excel cannot cope with this!
-            if option == "to_string":
-                date_format = "%Y-%m-%d %H:%M:%S"  # without microseconds
-                d = d.strftime(date_format)
-        except TypeError:
-            logging.info(f"The date is not of correct type [{xldate}]")
-            d = xldate
-    return d
-
-
-def convert_to_mAhg(c, mass=1.0):
-    """Converts capacity in Ah to capacity in mAh/g.
-
-    Args:
-        c (float or numpy array): capacity in mA.
-        mass (float): mass in mg.
 
-    Returns:
-        float: 1000000 * c / mass
-    """
-    return 1_000_000 * c / mass
+        # in case model is given as argument
+        self.model = kwargs.pop("model", self.default_model)
+        if self.auto_register_config:
+            self.config_params = self.register_configuration()
+
+        self.parse_formatter_parameters(**kwargs)
+
+        self.pre_processors = self.config_params.pre_processors
+        additional_pre_processor_args = kwargs.pop(
+            "pre_processors", None
+        )  # could replace None with an empty dict to get rid of the if-clause:
+        if additional_pre_processor_args:
+            for key in additional_pre_processor_args:
+                self.pre_processors[key] = additional_pre_processor_args[key]
+
+        self.post_processors = self.config_params.post_processors
+        additional_post_processor_args = kwargs.pop(
+            "post_processors", None
+        )  # could replace None with an empty dict to get rid of the if-clause:
+        if additional_post_processor_args:
+            for key in additional_post_processor_args:
+                self.post_processors[key] = additional_post_processor_args[key]
+
+        self.include_aux = kwargs.pop("include_aux", False)
+        self.keep_all_columns = kwargs.pop("keep_all_columns", False)
+        self.cellpy_headers_normal = (
+            headers_normal  # the column headers defined by cellpy
+        )
 
+    @abc.abstractmethod
+    def parse_formatter_parameters(self, **kwargs) -> None:
+        ...
+
+    @abc.abstractmethod
+    def parse_loader_parameters(self, **kwargs):
+        ...
+
+    @abc.abstractmethod
+    def query_file(self, file_path: Union[str, pathlib.Path]) -> pd.DataFrame:
+        ...
+
+    def pre_init(self) -> None:
+        ...
+
+    def register_configuration(self) -> ModelParameters:
+        """Register and load model configuration"""
+        if (
+            self.model is None
+        ):  # in case None was given as argument (model=None in initialisation)
+            self.model = self.default_model
+        model_module_name = self.supported_models.get(self.model.upper(), None)
+        if model_module_name is None:
+            raise Exception(
+                f"The model {self.model} does not have any defined configuration."
+                f"\nCurrent supported models are {[*self.supported_models.keys()]}"
+            )
+        return register_configuration_from_module(self.model, model_module_name)
 
-def collect_ocv_curves():
-    raise NotImplementedError
+    def get_raw_units(self):
+        """Include the settings for the units used by the instrument.
 
+        The units are defined w.r.t. the SI units ('unit-fractions'; currently only units that are multiples of
+        Si units can be used). For example, for current defined in mA, the value for the
+        current unit-fraction will be 0.001.
 
-def collect_capacity_curves(
-    data,
-    direction="charge",
-    trim_taper_steps=None,
-    steps_to_skip=None,
-    steptable=None,
-    max_cycle_number=None,
-    **kwargs,
-):
-    """Create a list of pandas.DataFrames, one for each charge step.
+        Returns:
+            dictionary containing the unit-fractions for current, charge, and mass
 
-    The DataFrames are named by its cycle number.
+        """
+        return self.config_params.raw_units
 
-    Input: CellpyData
-    Returns: list of pandas.DataFrames,
-        list of cycle numbers,
-        minimum voltage value,
-        maximum voltage value"""
+    def get_raw_limits(self):
+        """Include the settings for how to decide what kind of step you are examining here.
 
-    # TODO: should allow for giving cycle numbers as input (e.g. cycle=[1, 2, 10]
-    #  or cycle=2), not only max_cycle_number
+        The raw limits are 'epsilons' used to check if the current and/or voltage is stable (for example
+        for galvanostatic steps, one would expect that the current is stable (constant) and non-zero).
+        It is expected that different instruments (with different resolution etc.) have different
+        'epsilons'.
 
-    minimum_v_value = np.Inf
-    maximum_v_value = -np.Inf
-    charge_list = []
-    cycles = kwargs.pop("cycle", None)
+        Returns:
+            the raw limits (dict)
 
-    if cycles is None:
-        cycles = data.get_cycle_numbers()
+        """
+        return self.config_params.raw_limits
 
-    if max_cycle_number is None:
-        max_cycle_number = max(cycles)
+    @staticmethod
+    def get_headers_aux(raw: pd.DataFrame) -> dict:
+        raise NotImplementedError(
+            f"missing method in sub-class of TxtLoader: get_headers_aux"
+        )
 
-    for cycle in cycles:
-        if cycle > max_cycle_number:
-            break
-        try:
-            if direction == "charge":
-                q, v = data.get_ccap(
-                    cycle,
-                    trim_taper_steps=trim_taper_steps,
-                    steps_to_skip=steps_to_skip,
-                    steptable=steptable,
-                )
-            else:
-                q, v = data.get_dcap(
-                    cycle,
-                    trim_taper_steps=trim_taper_steps,
-                    steps_to_skip=steps_to_skip,
-                    steptable=steptable,
-                )
+    def _pre_process(self):
+        for processor_name in self.pre_processors:
+            if self.pre_processors[processor_name]:
+                if hasattr(pre_processors, processor_name):
+                    logging.critical(f"running pre-processor: {processor_name}")
+                    processor = getattr(pre_processors, processor_name)
+                    self.temp_file_path = processor(self.temp_file_path)
+                else:
+                    raise NotImplementedError(
+                        f"{processor_name} is not currently supported - aborting!"
+                    )
 
-        except NullData as e:
-            logging.warning(e)
-            d = pd.DataFrame()
-            d.name = cycle
-            charge_list.append(d)
-        else:
-            d = pd.DataFrame({"q": q, "v": v})
-            # d.name = f"{cycle}"
-            d.name = cycle
-            charge_list.append(d)
-            v_min = v.min()
-            v_max = v.max()
-            if v_min < minimum_v_value:
-                minimum_v_value = v_min
-            if v_max > maximum_v_value:
-                maximum_v_value = v_max
-    return charge_list, cycles, minimum_v_value, maximum_v_value
-
-
-def interpolate_y_on_x(
-    df,
-    x=None,
-    y=None,
-    new_x=None,
-    dx=10.0,
-    number_of_points=None,
-    direction=1,
-    **kwargs,
-):
-    """Interpolate a column based on another column.
+    def loader(self, name: Union[str, pathlib.Path], **kwargs: str) -> core.Data:
+        """returns a Data object with loaded data.
 
-    Args:
-        df: DataFrame with the (cycle) data.
-        x: Column name for the x-value (defaults to the step-time column).
-        y: Column name for the y-value (defaults to the voltage column).
-        new_x (numpy array or None): Interpolate using these new x-values
-            instead of generating x-values based on dx or number_of_points.
-        dx: step-value (defaults to 10.0)
-        number_of_points: number of points for interpolated values (use
-            instead of dx and overrides dx if given).
-        direction (-1,1): if direction is negative, then invert the
-            x-values before interpolating.
-        **kwargs: arguments passed to scipy.interpolate.interp1d
+        Loads data from a txt file (csv-ish).
 
-    Returns: DataFrame with interpolated y-values based on given or
-        generated x-values.
+        Args:
+            name (str, pathlib.Path): name of the file.
+            kwargs (dict): key-word arguments from raw_loader.
 
-    """
+        Returns:
+            new_tests (list of data objects)
 
-    # TODO: allow for giving a fixed interpolation range (x-values).
-    #  Remember to treat extrapolation properly (e.g. replace with NaN?).
+        """
+        pre_processor_hook = kwargs.pop("pre_processor_hook", None)
 
-    if x is None:
-        x = df.columns[0]
-    if y is None:
-        y = df.columns[1]
-
-    xs = df[x].values
-    ys = df[y].values
-
-    if direction > 0:
-        x_min = xs.min()
-        x_max = xs.max()
-    else:
-        x_max = xs.min()
-        x_min = xs.max()
-        dx = -dx
-
-    bounds_error = kwargs.pop("bounds_error", False)
-    f = interpolate.interp1d(xs, ys, bounds_error=bounds_error, **kwargs)
-    if new_x is None:
-        if number_of_points:
-            new_x = np.linspace(x_min, x_max, number_of_points)
-        else:
-            new_x = np.arange(x_min, x_max, dx)
+        if self.pre_processors:
+            self._pre_process()
 
-    new_y = f(new_x)
+        self.parse_loader_parameters(**kwargs)
 
-    new_df = pd.DataFrame({x: new_x, y: new_y})
+        data_df = self.query_file(self.temp_file_path)
 
-    return new_df
+        if pre_processor_hook is not None:
+            logging.debug("running pre-processing-hook")
+            data_df = pre_processor_hook(data_df)
+
+        data = core.Data()
+
+        # metadata
+        meta = self.parse_meta()
+        data.loaded_from = name
+        data.channel_index = meta.get("channel_index", None)
+        data.test_ID = meta.get("test_ID", None)
+        data.test_name = meta.get("test_name", None)
+        data.creator = meta.get("creator", None)
+        data.schedule_file_name = meta.get("schedule_file_name", None)
+        data.start_datetime = meta.get("start_datetime", None)
+
+        # Generating a FileID project:
+        self.generate_fid()
+        data.raw_data_files.append(self.fid)
+
+        data.raw = data_df
+        data.raw_data_files_length.append(len(data_df))
+        data.summary = (
+            pd.DataFrame()
+        )  # creating an empty frame - loading summary is not implemented
+        data = self._post_process(data)
+        data = self.identify_last_data_point(data)
+        if data.start_datetime is None:
+            data.start_datetime = data.raw[headers_normal.datetime_txt].iat[0]
+
+        data = self.validate(data)
+        return data
+
+    def validate(self, data: core.Data) -> core.Data:
+        """validation of the loaded data, should raise an appropriate exception if it fails."""
 
+        logging.debug(f"no validation of defined in this sub-class of TxtLoader")
+        return data
 
-def group_by_interpolate(
-    df,
-    x=None,
-    y=None,
-    group_by=None,
-    number_of_points=100,
-    tidy=False,
-    individual_x_cols=False,
-    header_name="Unit",
-    dx=10.0,
-    generate_new_x=True,
-):
-    """Do a pandas.DataFrame.group_by and perform interpolation for all groups.
+    def parse_meta(self) -> dict:
+        """method that parses the data for meta-data (e.g. start-time, channel number, ...)"""
+
+        logging.debug(
+            f"no parsing method for meta-data defined in this sub-class of TxtLoader"
+        )
+        return dict()
+
+    def _post_rename_headers(self, data):
+        if self.include_aux:
+            new_aux_headers = self.get_headers_aux(data.raw)
+            data.raw.rename(index=str, columns=new_aux_headers, inplace=True)
+        return data
+
+    def _post_process(self, data):
+        # ordered post-processing steps:
+        for processor_name in ORDERED_POST_PROCESSING_STEPS:
+            if processor_name in self.post_processors:
+                data = self._perform_post_process_step(data, processor_name)
+
+        # non-ordered post-processing steps
+        for processor_name in self.post_processors:
+            if processor_name not in ORDERED_POST_PROCESSING_STEPS:
+                data = self._perform_post_process_step(data, processor_name)
+        return data
+
+    def _perform_post_process_step(self, data, processor_name):
+        if self.post_processors[processor_name]:
+            if hasattr(post_processors, processor_name):
+                logging.critical(f"running post-processor: {processor_name}")
+                processor = getattr(post_processors, processor_name)
+                data = processor(data, self.config_params)
+                if hasattr(self, f"_post_{processor_name}"):  # internal addon-function
+                    _processor = getattr(self, f"_post_{processor_name}")
+                    data = _processor(data)
+            else:
+                raise NotImplementedError(
+                    f"{processor_name} is not currently supported - aborting!"
+                )
+        return data
 
-    This function is a wrapper around an internal interpolation function in
-    cellpy (that uses scipy.interpolate.interp1d) that combines doing a group-by
-    operation and interpolation.
-
-    Args:
-        df (pandas.DataFrame): the dataframe to morph.
-        x (str): the header for the x-value
-            (defaults to normal header step_time_txt) (remark that the default
-            group_by column is the cycle column, and each cycle normally
-            consist of several steps (so you risk interpolating / merging
-            several curves on top of each other (not good)).
-        y (str): the header for the y-value
-            (defaults to normal header voltage_txt).
-        group_by (str): the header to group by
-            (defaults to normal header cycle_index_txt)
-        number_of_points (int): if generating new x-column, how many values it
-            should contain.
-        tidy (bool): return the result in tidy (i.e. long) format.
-        individual_x_cols (bool): return as xy xy xy ... data.
-        header_name (str): name for the second level of the columns (only
-            applies for xy xy xy ... data) (defaults to "Unit").
-        dx (float): if generating new x-column and number_of_points is None or
-            zero, distance between the generated values.
-        generate_new_x (bool): create a new x-column by
-            using the x-min and x-max values from the original dataframe where
-            the method is set by the number_of_points key-word:
-
-            1)  if number_of_points is not None (default is 100):
-
-                ```
-                new_x = np.linspace(x_max, x_min, number_of_points)
-                ```
-            2)  else:
-                ```
-                new_x = np.arange(x_max, x_min, dx)
-                ```
 
+class TxtLoader(AutoLoader, ABC):
+    """Main txt loading class (for sub-classing).
 
-    Returns: pandas.DataFrame with interpolated x- and y-values. The returned
-        dataframe is in tidy (long) format for tidy=True.
+    The subclass of a ``TxtLoader`` gets its information by loading model specifications from its respective module
+    (``cellpy.readers.instruments.configurations.<module>``) or configuration file (yaml).
+
+    Remark that if you implement automatic loading of the formatter, the module / yaml-file must include all
+    the required formatter parameters (sep, skiprows, header, encoding, decimal, thousands).
+
+    If you need more flexibility, try using the ``CustomTxtLoader`` or subclass directly
+    from ``AutoLoader`` or ``Loader``.
+
+    Constructor:
+        model (str): short name of the (already implemented) sub-model.
+        sep (str): delimiter.
+        skiprows (int): number of lines to skip.
+        header (int): number of the header lines.
+        encoding (str): encoding.
+        decimal (str): character used for decimal in the raw data, defaults to '.'.
+        processors (dict): pre-processing steps to take (before loading with pandas).
+        post_processors (dict): post-processing steps to make after loading the data, but before
+        returning them to the caller.
+        include_aux (bool): also parse so-called auxiliary columns / data. Defaults to False.
+        keep_all_columns (bool): load all columns, also columns that are not 100% necessary for ``cellpy`` to work.
+        Remark that the configuration settings for the sub-model must include a list of column header names
+        that should be kept if keep_all_columns is False (default).
+
+    Module:
+        sep (str): the delimiter (also works as a switch to turn on/off automatic detection of delimiter and
+        start of data (skiprows)).
 
     """
-    # TODO: @jepe - create more tests
-    time_00 = time.time()
-    if x is None:
-        x = HEADERS_NORMAL.step_time_txt
-    if y is None:
-        y = HEADERS_NORMAL.voltage_txt
-    if group_by is None:
-        group_by = [HEADERS_NORMAL.cycle_index_txt]
-
-    if not isinstance(group_by, (list, tuple)):
-        group_by = [group_by]
-
-    if not generate_new_x:
-        # check if it makes sence
-        if (not tidy) and (not individual_x_cols):
-            logging.warning("Unlogical condition")
-            generate_new_x = True
-
-    new_x = None
-
-    if generate_new_x:
-        x_max = df[x].max()
-        x_min = df[x].min()
-        if number_of_points:
-            new_x = np.linspace(x_max, x_min, number_of_points)
-        else:
-            new_x = np.arange(x_max, x_min, dx)
 
-    new_dfs = []
-    keys = []
+    instrument_name = "txt_loader"
+    raw_ext = "*"
 
-    for name, group in df.groupby(group_by):
-        keys.append(name)
-        if not isinstance(name, (list, tuple)):
-            name = [name]
+    # override this if needed
+    def parse_loader_parameters(self, **kwargs):
+        sep = kwargs.get("sep", None)
+        if sep is not None:
+            self.sep = sep
+        if self.sep is None:
+            self._auto_formatter()
+
+    # override this if needed
+    def parse_formatter_parameters(self, **kwargs):
+        logging.debug(f"model: {self.model}")
+        if not self.config_params.formatters:
+            # Setting defaults if formatter is not loaded
+            logging.debug("No formatter given - using default values.")
+            self.sep = kwargs.pop("sep", None)
+            self.skiprows = kwargs.pop("skiprows", 0)
+            self.header = kwargs.pop("header", 0)
+            self.encoding = kwargs.pop("encoding", "utf-8")
+            self.decimal = kwargs.pop("decimal", ".")
+            self.thousands = kwargs.pop("thousands", None)
 
-        new_group = interpolate_y_on_x(
-            group, x=x, y=y, new_x=new_x, number_of_points=number_of_points, dx=dx
+        else:
+            # Remark! This will break if one of these parameters are missing
+            # (not a keyword argument and not within the configuration):
+            self.sep = kwargs.pop("sep", self.config_params.formatters["sep"])
+            self.skiprows = kwargs.pop(
+                "skiprows", self.config_params.formatters["skiprows"]
+            )
+            self.header = kwargs.pop("header", self.config_params.formatters["header"])
+            self.encoding = kwargs.pop(
+                "encoding", self.config_params.formatters["encoding"]
+            )
+            self.decimal = kwargs.pop(
+                "decimal", self.config_params.formatters["decimal"]
+            )
+            self.thousands = kwargs.pop(
+                "thousands", self.config_params.formatters["thousands"]
+            )
+        logging.debug(
+            f"Formatters: self.sep={self.sep} self.skiprows={self.skiprows} self.header={self.header} self.encoding={self.encoding}"
+        )
+        logging.debug(
+            f"Formatters (cont.): self.decimal={self.decimal} self.thousands={self.thousands}"
         )
 
-        if tidy or (not tidy and not individual_x_cols):
-            for i, j in zip(group_by, name):
-                new_group[i] = j
-        new_dfs.append(new_group)
+    def _auto_formatter(self):
+        separator, first_index = find_delimiter_and_start(
+            self.name,
+            separators=None,
+            checking_length_header=100,
+            checking_length_whole=200,
+        )
+        self.encoding = "UTF-8"  # consider adding a find_encoding function
+        self.sep = separator
+        self.skiprows = first_index - 1
+        self.header = 0
 
-    if tidy:
-        new_df = pd.concat(new_dfs)
-    else:
-        if individual_x_cols:
-            new_df = pd.concat(new_dfs, axis=1, keys=keys)
-            group_by.append(header_name)
-            new_df.columns.names = group_by
-        else:
-            new_df = pd.concat(new_dfs)
-            new_df = new_df.pivot(index=x, columns=group_by[0], values=y)
+        logging.critical(
+            f"auto-formatting:\n  {self.sep=}\n  {self.skiprows=}\n  {self.header=}\n  {self.encoding=}\n"
+        )
 
-    time_01 = time.time() - time_00
-    logging.debug(f"duration: {time_01} seconds")
-    return new_df
+    # override this if using other query functions
+    def query_file(self, name):
+        logging.debug(f"parsing with pandas.read_csv: {name}")
+        logging.critical(
+            f"{self.sep=}, {self.skiprows=}, {self.header=}, {self.encoding=}, {self.decimal=}"
+        )
+        data_df = pd.read_csv(
+            name,
+            sep=self.sep,
+            skiprows=self.skiprows,
+            header=self.header,
+            encoding=self.encoding,
+            decimal=self.decimal,
+            thousands=self.thousands,
+        )
+        return data_df
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/dbreader.py` & `cellpy-1.0.0a0/cellpy/readers/dbreader.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,57 +1,77 @@
+import abc
 import logging
 import os
 import pathlib
 import re
 import tempfile
 import time
 import warnings
 from dataclasses import asdict
 from datetime import datetime
 
 import numpy as np
 import pandas as pd
+from typing import List
+from typing import Optional
 
 from cellpy.parameters import prms
+from cellpy.readers.core import BaseDbReader
 
 # logger = logging.getLogger(__name__)
 
 
 class DbSheetCols:
-    def __init__(self, level=0):
+    # Note to developers: this should only be used for this Excell reader
+    # (it works, and that is its only reason to still exist)
+    def __init__(self):
         db_cols_from_prms = asdict(prms.DbCols)
-        for table_key, values in db_cols_from_prms.items():
-            setattr(self, table_key, values[level])
+        self.keys = []
+        self.headers = []
+        for table_key, value in db_cols_from_prms.items():
+            if isinstance(value, (list, tuple)):
+                value = value[0]
+            setattr(self, table_key, value)
+            self.keys.append(table_key)
+            self.headers.append(value)
 
     def __repr__(self):
         return f"<DbCols: {self.__dict__}>"
 
 
-class Reader:
+class Reader(BaseDbReader):
     def __init__(
-        self, db_file=None, db_datadir=None, db_datadir_processed=None, db_frame=None
+        self,
+        db_file=None,
+        db_datadir=None,
+        db_datadir_processed=None,
+        db_frame=None,
+        batch=None,
+        batch_col_name=None,
     ):
         """Simple excel reader.
 
         Args:
             db_file (str, pathlib.Path): xlsx-file to read.
             db_datadir(str, pathlib.Path): path where raw date is located.
             db_datadir_processed (str, pathlib.Path): path where cellpy files are located.
             db_frame (pandas.DataFrame): use this instead of reading from xlsx-file.
+            batch (str): batch name to use.
+            batch_col_name (str): name of the column in the db-file that contains the batch name.
         """
 
         self.db_sheet_table = prms.Db.db_table_name
         self.db_header_row = prms.Db.db_header_row
         self.db_unit_row = prms.Db.db_unit_row
         self.db_data_start_row = prms.Db.db_data_start_row
         self.db_search_start_row = prms.Db.db_search_start_row
         self.db_search_end_row = prms.Db.db_search_end_row
 
         self.db_sheet_cols = DbSheetCols()
-        self.db_sheet_cols_units = DbSheetCols(1)
+        self.selected_batch = None
 
         if not db_datadir:
             self.db_datadir = prms.Paths.rawdatadir
         else:
             self.db_datadir = db_datadir
 
         if not db_datadir_processed:
@@ -64,39 +84,94 @@
             self.db_filename = prms.Paths.db_filename
             self.db_file = os.path.join(self.db_path, self.db_filename)
         else:
             self.db_path = os.path.dirname(db_file)
             self.db_filename = os.path.basename(db_file)
             self.db_file = db_file
 
-        self.dtypes_dict = self._create_dtypes_dict()
-        self.headers = self.dtypes_dict.keys()
+        self.headers = self.db_sheet_cols.headers
 
         if db_frame is not None:
             print("Using frame instead of file")
             self.table = db_frame.copy()
         else:
-
             self.skiprows, self.nrows = self._find_out_what_rows_to_skip()
             logging.debug("opening sheet")
 
             self.table = self._open_sheet()
 
+        if batch:
+            self.selected_batch = self.select_batch(
+                batch, batch_col_name=batch_col_name
+            )
         logging.debug("got table")
         logging.debug(self.table)
 
     def __str__(self):
         newline = "\n  - "
         txt = f"<Reader:: \n  - {newline.join(self.__dict__)} \n>\n"
         txt += "Reader.table.head():\n"
         txt += str(self.table.head())
         return txt
 
+    def select_batch(
+        self, batch, batch_col_name=None, case_sensitive=True, drop=True
+    ) -> List[int]:
+        """Selects the rows in column batch_col_number.
+
+        Args:
+            batch: batch to select
+            batch_col_name: column name to use for batch selection (default: DbSheetCols.batch).
+            case_sensitive: if True, the batch name must match exactly (default: True).
+            drop: if True, all un-selected rows are dropped from the table (default: True).
+
+        Returns:
+            List of row indices
+        """
+
+        if self.selected_batch is None:
+            return self._select_batch(
+                batch,
+                batch_col_name=batch_col_name,
+                case_sensitive=case_sensitive,
+                drop=drop,
+            )
+        else:
+            return self.selected_batch
+
+    def _select_batch(self, batch, batch_col_name=None, case_sensitive=True, drop=True):
+        if not batch_col_name:
+            batch_col_name = self.db_sheet_cols.batch
+        logging.debug("selecting batch - %s" % batch)
+        sheet = self.table
+        identity = self.db_sheet_cols.id
+        exists_col_number = self.db_sheet_cols.exists
+
+        if case_sensitive:
+            criterion = sheet.loc[:, batch_col_name] == batch
+        else:
+            criterion = (sheet.loc[:, batch_col_name]).upper() == batch.upper()
+
+        exists = sheet.loc[:, exists_col_number] > 0
+        # This will crash if the col is not of dtype number
+        sheet = sheet[criterion & exists]
+        if drop:
+            self.table = sheet
+        return sheet.loc[:, identity].values.astype(int)
+
+    def from_batch(
+        self,
+        batch_name: str,
+        include_key: bool = False,
+        include_individual_arguments: bool = False,
+    ) -> dict:
+        raise NotImplementedError("This method is not implemented for this reader")
+
     @staticmethod
-    def _parse_argument_str(argument_str: str) -> dict:
+    def _parse_argument_str(argument_str: str) -> Optional[dict]:
         # the argument str must be on the form:
         # "keyword-1=value-1;keyword-2=value2"
         if argument_str is None:
             return
         sep = ";"
         parts = [part.strip() for part in argument_str.split(sep=sep)]
         sep = "="
@@ -149,15 +224,14 @@
         elif splitter:
             datestr = cell_name.split(splitter)[position]
 
         else:
             datestr = cell_name[start:end]
 
         try:
-
             date = datetime.strptime(datestr, strf)
         except ValueError as e:
             logging.debug(e)
             return None
 
         return date
 
@@ -200,61 +274,48 @@
             "float": np.float64,
             "str": str,
             "bol": bool,
             "cat": str,
         }
         return units.get(label.lower(), object)
 
-    def _create_dtypes_dict(self):
-        dtypes_dict = dict()
-        for attr in self.db_sheet_cols.__dict__:
-            header = self.db_sheet_cols.__dict__[attr]
-            unit = self._lookup_unit(self.db_sheet_cols_units.__dict__[attr])
-            dtypes_dict[header] = unit
-        return dtypes_dict
-
     def pick_table(self):
         """Pick the table and return a pandas.DataFrame."""
         return self.table
 
     @staticmethod
     def _select_col(df, no):
         """select specific column"""
         return df.loc[:, no]
 
-    def _open_sheet(self, dtypes_dict=None):
+    def _open_sheet(self):
         """Opens sheets and returns it"""
 
         # Note 14.12.2020: xlrd has explicitly removed support for anything other than xls files
         # Solution: install openpyxl
         # df1=pd.read_excel(
         #      os.path.join(APP_PATH, "Data", "aug_latest.xlsm"),
         #      sheet_name=None,
         #      engine='openpyxl',
         # )
 
         table_name = self.db_sheet_table
         header_row = self.db_header_row
         nrows = self.nrows
-        if dtypes_dict is None:
-            dtypes_dict = self.dtypes_dict
-
         rows_to_skip = self.skiprows
 
         logging.debug(f"Trying to open the file {self.db_file}")
         logging.debug(f"Number of rows (no means all): {nrows}")
         logging.debug(f"Skipping the following rows: {rows_to_skip}")
-        logging.debug(f"Declaring the following dtyps: {dtypes_dict}")
         work_book = pd.ExcelFile(self.db_file, engine="openpyxl")
         try:
             sheet = work_book.parse(
                 table_name,
                 header=header_row,
                 skiprows=rows_to_skip,
-                dtype=dtypes_dict,
                 nrows=nrows,
             )
         except ValueError as e:
             logging.debug(
                 "Could not parse all the columns (ValueError) "
                 "using given dtypes. Trying without dtypes."
             )
@@ -373,14 +434,19 @@
         return insp
 
     def get_label(self, serial_number):
         column_name = self.db_sheet_cols.label
         insp = self._pick_info(serial_number, column_name)
         return insp
 
+    def get_area(self, serial_number):
+        column_name = self.db_sheet_cols.area
+        insp = self._pick_info(serial_number, column_name)
+        return insp
+
     def get_cell_name(self, serial_number):
         column_name = self.db_sheet_cols.cell_name
         insp = self._pick_info(serial_number, column_name)
         return insp
 
     def get_comment(self, serial_number):
         column_name = self.db_sheet_cols.comment_general
@@ -426,15 +492,15 @@
             logging.warning(f"{argument_str}")
             logging.warning(f"Error message: {e}")
             return {}
 
         return argument
 
     def get_mass(self, serial_number):
-        column_name_mass = self.db_sheet_cols.active_material
+        column_name_mass = self.db_sheet_cols.mass_active
         mass = self._pick_info(serial_number, column_name_mass)
         return mass
 
     def get_nom_cap(self, serial_number):
         column_name = self.db_sheet_cols.nom_cap
         return self._pick_info(serial_number, column_name)
 
@@ -443,22 +509,22 @@
         return self._pick_info(serial_number, column_name)
 
     def get_instrument(self, serial_number):
         column_name = self.db_sheet_cols.instrument
         return self._pick_info(serial_number, column_name)
 
     def get_total_mass(self, serial_number):
-        column_name_mass = self.db_sheet_cols.total_material
+        column_name_mass = self.db_sheet_cols.mass_total
         total_mass = self._pick_info(serial_number, column_name_mass)
         return total_mass
 
     def get_all(self):
         return self.filter_by_col([self.db_sheet_cols.id, self.db_sheet_cols.exists])
 
-    def get_fileid(self, serial_number, full_path=True):
+    def get_fileid(self, serial_number, full_path=True):  # NOT USED
         column_name = self.db_sheet_cols.file_name_indicator
         if not full_path:
             filename = self._pick_info(serial_number, column_name)
         else:
             filename = os.path.join(
                 self.db_datadir_processed, self._pick_info(serial_number, column_name)
             )
@@ -589,79 +655,24 @@
         sheet = self.table
         identity = self.db_sheet_cols.id
         exists_col_number = self.db_sheet_cols.exists
 
         exists = sheet.loc[:, exists_col_number] > 0
 
         if min_val is not None and max_val is not None:
-
             criterion1 = sheet.loc[:, column_name] >= min_val
             criterion2 = sheet.loc[:, column_name] <= max_val
             sheet = sheet[criterion1 & criterion2 & exists]
 
         elif min_val is not None or max_val is not None:
-
             if min_val is not None:
                 criterion = sheet.loc[:, column_name] >= min_val
 
             if max_val is not None:
                 criterion = sheet.loc[:, column_name] <= max_val
 
             # noinspection PyUnboundLocalVariable
             sheet = sheet[criterion & exists]
         else:
             sheet = sheet[exists]
 
         return sheet.loc[:, identity].values.astype(int)
-
-    def select_batch(self, batch, batch_col_name=None, case_sensitive=True):
-        """selects the rows  in column batch_col_number
-        (default: DbSheetCols.batch)"""
-
-        if not batch_col_name:
-            batch_col_name = self.db_sheet_cols.batch
-        logging.debug("selecting batch - %s" % batch)
-        sheet = self.table
-        identity = self.db_sheet_cols.id
-        exists_col_number = self.db_sheet_cols.exists
-
-        if case_sensitive:
-            criterion = sheet.loc[:, batch_col_name] == batch
-        else:
-            criterion = (sheet.loc[:, batch_col_name]).upper() == batch.upper()
-
-        exists = sheet.loc[:, exists_col_number] > 0
-        # This will crash if the col is not of dtype number
-        sheet = sheet[criterion & exists]
-        return sheet.loc[:, identity].values.astype(int)
-
-
-if __name__ == "__main__":
-    from cellpy import log, prms
-
-    # check if Paths work:
-    filelogdir = "/Users/jepe/cellpy_data/logs"
-    db_path = "/Users/jepe/cellpy_data/db"
-    filelogdir = pathlib.Path(filelogdir)
-    db_path = pathlib.Path(db_path)
-    # seems to work OK
-
-    prms.Paths.filelogdir = filelogdir
-    prms.Paths.db_path = db_path
-    prms.Paths.db_filename = "cellpy_db2.xlsx"
-    log.setup_logging(default_level="DEBUG")
-
-    logging.info("-logging works-")
-    r = Reader()
-    ok = r._validate()
-    logging.info(f"db-file is OK: {ok}")
-    print(r)
-    print()
-
-    print("--------------------------------------------------------")
-    print(r.table.id)
-    print(r.table.describe())
-    print(r.table.dtypes)
-    print(r.select_serial_number_row(615))
-    r.print_serial_number_info(615)
-    tm = r.get_total_mass(615)
-    print(tm)
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/arbin_res.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/arbin_res.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,25 +1,26 @@
 """arbin res-type data files"""
 import logging
 import os
+import pathlib
 import platform
 import shutil
 import sys
 import tempfile
 import time
 import warnings
 
 import numpy as np
 import pandas as pd
 import sqlalchemy as sa
 
 from cellpy import prms
 from cellpy.parameters.internal_settings import HeaderDict, get_headers_normal
 from cellpy.readers.core import (
-    Cell,
+    Data,
     FileID,
     check64bit,
     humanize_bytes,
     xldate_as_datetime,
 )
 from cellpy.readers.instruments.base import MINIMUM_SELECTION, BaseLoader
 
@@ -61,15 +62,15 @@
             "python 64bit and office 32bit -> " "setting use_subprocess to True"
         )
         use_subprocess = True
 
 if use_subprocess and not is_posix:
     # The windows users most likely have a strange custom path to mdbtools etc.
     logging.debug(
-        "using subprocess (most lilkely mdbtools) " "on non-posix (most likely windows)"
+        "using subprocess (most likely mdbtools) on non-posix (most likely windows)"
     )
     if not prms.Instruments.Arbin.sub_process_path:
         sub_process_path = str(prms._sub_process_path)
     else:
         sub_process_path = str(prms.Instruments.Arbin.sub_process_path)
 
 if is_posix:
@@ -169,15 +170,15 @@
         detect_subprocess_need
         sub_process_path
         office_version
         SQL_server
 
     """
 
-    name = "arbin_res"
+    instrument_name = "arbin_res"
     raw_ext = "res"
 
     def __init__(self, *args, **kwargs):
         """initiates the ArbinLoader class"""
         # could use __init__(self, cellpydata_object) and
         # set self.logger = cellpydata_object.logger etc.
         # then remember to include that as prm in "out of class" functions
@@ -198,18 +199,18 @@
         self.arbin_headers_aux_global = self.get_headers_aux_global()
         self.arbin_headers_aux = self.get_headers_aux()
         self.current_chunk = 0  # use this to set chunks to load
 
     @staticmethod
     def get_raw_units():
         raw_units = dict()
-        raw_units["current"] = 1.0  # A
-        raw_units["charge"] = 1.0  # Ah
-        raw_units["mass"] = 1.0  # g
-        raw_units["voltage"] = 1.0  # V
+        raw_units["current"] = "A"
+        raw_units["charge"] = "Ah"
+        raw_units["mass"] = "g"
+        raw_units["voltage"] = "V"
         return raw_units
 
     @staticmethod
     def get_headers_normal():
         """Defines the so-called normal column headings for Arbin .res-files"""
         headers = HeaderDict()
         # - normal (raw-data) column headings (specific for Arbin)
@@ -318,15 +319,14 @@
         raw_limits["stable_voltage_soft"] = 4.0
         raw_limits["stable_charge_hard"] = 0.001
         raw_limits["stable_charge_soft"] = 5.0
         raw_limits["ir_change"] = 0.00001
         return raw_limits
 
     def _get_res_connector(self, temp_filename):
-
         if use_ado:  # deprecated
             is64bit_python = check64bit(current_system="python")
             if is64bit_python:
                 constr = (
                     f"Provider=Microsoft.ACE.OLEDB.12.0; DataSource={temp_filename}"
                 )
             else:
@@ -395,25 +395,21 @@
         return constr
 
     def _get_connection_or_engine(self, temp_filename):
         # updated to use sqlalchemy - needs sqlalchemy-access
         constr = self._get_res_connector(temp_filename)
         self.logger.debug(f"constr str: {constr}")
         if use_ado:
-            conn = dbloader.connect(constr)
+            raise DeprecationWarning("use_ado not supported anymore")
         else:
-            if USE_SQLALCHEMY_ACCESS_ENGINE:
-                connection_url = sa.engine.URL.create(
-                    "access+pyodbc", query={"odbc_connect": constr}
-                )
-                engine = sa.create_engine(connection_url)
-                conn = engine
-            else:
-                conn = dbloader.connect(constr, autocommit=True)
-        return conn
+            connection_url = sa.engine.URL.create(
+                "access+pyodbc", query={"odbc_connect": constr}
+            )
+            engine = sa.create_engine(connection_url)
+        return engine
 
     def _clean_up_loadres(self, cur, conn, filename):
         if cur is not None:
             cur.close()  # adodbapi
         if conn is not None:
             conn.close()  # adodbapi
         if os.path.isfile(filename):
@@ -454,15 +450,14 @@
             except Exception as e:
                 txt = (
                     f"Exception raised ({e})\n"
                     f"key: {key} old_header: {old_header}"
                     f"cellpy headers normal type {type(self.cellpy_headers_normal)}"
                 )
                 raise Exception(txt)
-                logging.debug(f"Could not rename summary df ::\n{e}")
 
         if fix_datetime:
             h_datetime = self.cellpy_headers_normal.datetime_txt
             logging.debug("converting to datetime format")
             # print(data.raw.columns)
             data.raw[h_datetime] = data.raw[h_datetime].apply(
                 xldate_as_datetime, option="to_datetime"
@@ -484,23 +479,20 @@
     def _inspect(self, run_data):
         """Inspect the file -> reports to log (debug)"""
 
         if not any([DEBUG_MODE]):
             return run_data
 
         if DEBUG_MODE:
-            checked_rundata = []
-            for data in run_data:
-                new_cols = data.raw.columns
-                for col in self.arbin_headers_normal:
-                    if col not in new_cols:
-                        logging.debug(f"Missing col: {col}")
-                        # data.raw[col] = np.nan
-                checked_rundata.append(data)
-            return checked_rundata
+            new_cols = run_data.raw.columns
+            for col in self.arbin_headers_normal:
+                if col not in new_cols:
+                    logging.debug(f"Missing col: {col}")
+                    # data.raw[col] = np.nan
+            return run_data
 
     def _iterdump(self, file_name, headers=None):  # Deprecated - use on own risk
         """
         Function for dumping values from a file.
 
         Should only be used by developers.
 
@@ -530,30 +522,29 @@
         self.logger.info(txt)
 
         table_name_global = TABLE_NAMES["global"]
         table_name_stats = TABLE_NAMES["statistic"]
         table_name_normal = TABLE_NAMES["normal"]
 
         # creating temporary file and connection
+        self.copy_to_temporary()
 
-        temp_dir = tempfile.gettempdir()
-        temp_filename = os.path.join(temp_dir, os.path.basename(file_name))
-        shutil.copy2(file_name, temp_dir)
-        constr = self._get_res_connector(temp_filename)
+        constr = self._get_res_connector(self._temp_file_path)
         if use_ado:
             conn = dbloader.connect(constr)
         else:
             conn = dbloader.connect(constr, autocommit=True)
 
-        self.logger.debug("tmp file: %s" % temp_filename)
+        self.logger.debug("tmp file: %s" % self._temp_file_path)
         self.logger.debug("constr str: %s" % constr)
 
         # --------- read global-data ------------------------------------
         self.logger.debug("reading global data table")
         sql = "select * from %s" % table_name_global
+
         global_data_df = pd.read_sql_query(sql=sa.text(sql), con=conn.connect())
         # col_names = list(global_data_df.columns.values)
         self.logger.debug("sql statement: %s" % sql)
 
         tests = global_data_df[self.arbin_headers_normal.test_id_txt]
         number_of_sets = len(tests)
         self.logger.debug("number of datasets: %i" % number_of_sets)
@@ -561,29 +552,32 @@
         test_no = 0
         self.logger.debug("setting data for test number %i" % test_no)
         loaded_from = file_name
         # fid = FileID(file_name)
         start_datetime = global_data_df[
             self.arbin_headers_global["start_datetime_txt"]
         ][test_no]
-        test_ID = int(
+        _internal_test_number = int(
             global_data_df[self.arbin_headers_normal.test_id_txt][test_no]
         )  # OBS
         test_name = global_data_df[self.arbin_headers_global["test_name_txt"]][test_no]
 
         # --------- read raw-data (normal-data) -------------------------
         self.logger.debug("reading raw-data")
 
         columns = ["Data_Point", "Step_Index", "Cycle_Index"]
         columns.extend(headers)
         columns_txt = ", ".join(["%s"] * len(columns)) % tuple(columns)
 
         sql_1 = "select %s " % columns_txt
         sql_2 = "from %s " % table_name_normal
-        sql_3 = "where %s=%s " % (self.arbin_headers_normal.test_id_txt, test_ID)
+        sql_3 = "where %s=%s " % (
+            self.arbin_headers_normal.test_id_txt,
+            _internal_test_number,
+        )
         sql_5 = "order by %s" % self.arbin_headers_normal.data_point_txt
         import time
 
         info_list = []
         info_header = ["cycle", "row_count", "start_point", "end_point"]
         info_header.extend(headers)
         self.logger.info(" ".join(info_header))
@@ -607,160 +601,25 @@
             end_point = normal_df[point_txt].max()
             last = normal_df.iloc[-1, :]
 
             step_list = [cycle_number, row_count, start_point, end_point]
             step_list.extend([last[x] for x in headers])
             info_list.append(step_list)
 
-        self._clean_up_loadres(None, conn, temp_filename)
-        info_dict = pd.DataFrame(info_list, columns=info_header)
-        return info_dict
-
-    def investigate(self, file_name):  # Deprecated - use on own risk
-        """Investigate a .res file.
-
-        Args:
-            file_name: name of the file
-
-        Returns: dictionary with div. stats and info.
-
-        """
-        step_txt = self.arbin_headers_normal.step_index_txt
-        point_txt = self.arbin_headers_normal.data_point_txt
-        cycle_txt = self.arbin_headers_normal.cycle_index_txt
-
-        self.logger.debug("investigating file: %s" % file_name)
-        if not os.path.isfile(file_name):
-            print("Missing file_\n   %s" % file_name)
-
-        filesize = os.path.getsize(file_name)
-        hfilesize = humanize_bytes(filesize)
-        txt = "Filesize: %i (%s)" % (filesize, hfilesize)
-        self.logger.info(txt)
-
-        table_name_global = TABLE_NAMES["global"]
-        table_name_stats = TABLE_NAMES["statistic"]
-        table_name_normal = TABLE_NAMES["normal"]
-
-        # creating temporary file and connection
-
-        temp_dir = tempfile.gettempdir()
-        temp_filename = os.path.join(temp_dir, os.path.basename(file_name))
-        shutil.copy2(file_name, temp_dir)
-        constr = self._get_res_connector(temp_filename)
-
-        if use_ado:
-            conn = dbloader.connect(constr)
-        else:
-            conn = dbloader.connect(constr, autocommit=True)
-
-        self.logger.debug("tmp file: %s" % temp_filename)
-        self.logger.debug("constr str: %s" % constr)
-
-        # --------- read global-data ------------------------------------
-        self.logger.debug("reading global data table")
-        sql = "select * from %s" % table_name_global
-        global_data_df = pd.read_sql_query(sql=sa.text(sql), con=conn.connect())
-        # col_names = list(global_data_df.columns.values)
-        self.logger.debug("sql statement: %s" % sql)
-
-        tests = global_data_df[self.arbin_headers_normal.test_id_txt]
-        number_of_sets = len(tests)
-        self.logger.debug("number of datasets: %i" % number_of_sets)
-        self.logger.debug("only selecting first test")
-        test_no = 0
-        self.logger.debug("setting data for test number %i" % test_no)
-        loaded_from = file_name
-        # fid = FileID(file_name)
-        start_datetime = global_data_df[
-            self.arbin_headers_global["start_datetime_txt"]
-        ][test_no]
-        test_ID = int(
-            global_data_df[self.arbin_headers_normal.test_id_txt][test_no]
-        )  # OBS
-        test_name = global_data_df[self.arbin_headers_global["test_name_txt"]][test_no]
-
-        # --------- read raw-data (normal-data) -------------------------
-        self.logger.debug("reading raw-data")
-
-        columns = ["Data_Point", "Step_Index", "Cycle_Index"]
-        columns_txt = ", ".join(["%s"] * len(columns)) % tuple(columns)
-
-        sql_1 = "select %s " % columns_txt
-        sql_2 = "from %s " % table_name_normal
-        sql_3 = "where %s=%s " % (self.arbin_headers_normal.test_id_txt, test_ID)
-        sql_5 = "order by %s" % self.arbin_headers_normal.data_point_txt
-        import time
-
-        info_list = []
-        info_header = ["cycle", "step", "row_count", "start_point", "end_point"]
-        self.logger.info(" ".join(info_header))
-        self.logger.info("-------------------------------------------------")
-        for cycle_number in range(1, 2000):
-            t1 = time.time()
-            self.logger.debug("picking cycle %i" % cycle_number)
-            sql_4 = "AND %s=%i " % (cycle_txt, cycle_number)
-            sql = sql_1 + sql_2 + sql_3 + sql_4 + sql_5
-            self.logger.debug("sql statement: %s" % sql)
-            normal_df = pd.read_sql_query(sql=sa.text(sql), con=conn.connect())
-            t2 = time.time()
-            dt = t2 - t1
-            self.logger.debug("time: %f" % dt)
-            if normal_df.empty:
-                self.logger.debug("reached the end")
-                break
-            row_count, _ = normal_df.shape
-            steps = normal_df[self.arbin_headers_normal.step_index_txt].unique()
-            txt = "cycle %i: %i [" % (cycle_number, row_count)
-            for step in steps:
-                self.logger.debug(" step: %i" % step)
-                step_df = normal_df.loc[normal_df[step_txt] == step]
-                step_row_count, _ = step_df.shape
-                start_point = step_df[point_txt].min()
-                end_point = step_df[point_txt].max()
-                txt += " %i-(%i)" % (step, step_row_count)
-                step_list = [cycle_number, step, step_row_count, start_point, end_point]
-                info_list.append(step_list)
-
-            txt += "]"
-            self.logger.info(txt)
-
-        self._clean_up_loadres(None, conn, temp_filename)
+        self._clean_up_loadres(None, conn, self._temp_file_path)
         info_dict = pd.DataFrame(info_list, columns=info_header)
         return info_dict
 
     def repair(self, file_name):
         """try to repair a broken/corrupted file"""
         raise NotImplemented
 
-    def dump(self, file_name, path):
-        """Dumps the raw file to an intermediate hdf5 file.
-
-        This method can be used if the raw file is too difficult to load and it
-        is likely that it is more efficient to convert it to an hdf5 format
-        and then load it using the `from_intermediate_file` function.
-
-        Args:
-            file_name: name of the raw file
-            path: path to where to store the intermediate hdf5 file (optional)
-
-        Returns:
-            full path to stored intermediate hdf5 file
-            information about the raw file (needed by the
-            `from_intermediate_file` function)
-
-        """
-
-        # information = None # contains information needed by the from_
-        # intermediate_file reader
-        # full_path = None
-        # return full_path, information
-        raise NotImplemented
-
     def _query_table(self, table_name, conn, sql=None):
+        from sqlalchemy import create_engine, text
+
         self.logger.debug(f"reading {table_name}")
         if sql is None:
             sql = f"select * from {table_name}"
         self.logger.debug(f"sql statement: {sql}")
         df = pd.read_sql_query(sql=sa.text(sql), con=conn.connect())
         return df
 
@@ -786,98 +645,85 @@
         temp_filename,
         *args,
         bad_steps=None,
         dataset_number=None,
         data_points=None,
         **kwargs,
     ):
-
-        new_tests = []
         conn = None
 
         table_name_global = TABLE_NAMES["global"]
         table_name_aux_global = TABLE_NAMES["aux_global"]
         table_name_aux = TABLE_NAMES["aux"]
         table_name_stats = TABLE_NAMES["statistic"]
         table_name_normal = TABLE_NAMES["normal"]
 
         if DEBUG_MODE:
             time_0 = time.time()
 
         conn = self._get_connection_or_engine(temp_filename)
 
-        # if use_ado:
-        #     conn = dbloader.connect(constr)
-        # else:
-        #     conn = dbloader.connect(constr, autocommit=True)
-
         self.logger.debug("reading global data table")
 
         global_data_df = self._query_table(table_name=table_name_global, conn=conn)
         tests = global_data_df[self.arbin_headers_normal.test_id_txt]
         number_of_sets = len(tests)
         self.logger.debug(f"number of datasets: {number_of_sets}")
 
         if dataset_number is not None:
             self.logger.info(f"Dataset number given: {dataset_number}")
             self.logger.info(f"Available dataset numbers: {tests}")
-            test_nos = [dataset_number]
+
         else:
-            test_nos = range(number_of_sets)
+            dataset_number = 0
 
-        for counter, test_no in enumerate(test_nos):
-            if counter > 0:
-                self.logger.warning("** WARNING ** MULTI-TEST-FILE (not recommended)")
-                if not ALLOW_MULTI_TEST_FILE:
-                    break
-            data = self._init_data(file_name, global_data_df, test_no)
-            test_id = data.test_ID
-            self.logger.debug("reading raw-data")
-
-            # --------- read raw-data (normal-data) ------------------------
-            length_of_test, normal_df = self._load_res_normal_table(
-                conn, test_id, bad_steps, data_points
-            )
-            # --------- read auxiliary data (aux-data) ---------------------
-            normal_df = self._load_win_res_auxiliary_table(
-                conn, normal_df, table_name_aux, table_name_aux_global, test_id
-            )
+        data = self._init_data(file_name, global_data_df, dataset_number)
+        test_id = data._internal_test_number
+        self.logger.debug("reading raw-data")
 
-            # --------- read stats-data (summary-data) ---------------------
-            sql = "select * from %s where %s=%s order by %s" % (
-                table_name_stats,
-                self.arbin_headers_normal.test_id_txt,
-                data.test_ID,
-                self.arbin_headers_normal.data_point_txt,
-            )
-            summary_df = pd.read_sql_query(sql=sa.text(sql), con=conn.connect())
+        # --------- read raw-data (normal-data) ------------------------
+        length_of_test, normal_df = self._load_res_normal_table(
+            conn, test_id, bad_steps, data_points
+        )
+        # --------- read auxiliary data (aux-data) ---------------------
+        normal_df = self._load_win_res_auxiliary_table(
+            conn, normal_df, table_name_aux, table_name_aux_global, test_id
+        )
 
-            if summary_df.empty and prms.Reader.use_cellpy_stat_file:
-                txt = "\nCould not find any summary (stats-file)!"
-                txt += "\n -> issue make_summary(use_cellpy_stat_file=False)"
-                logging.debug(txt)
-                # TODO: Enforce creating a summary df or modify renaming summary df (post process part)
-            # normal_df = normal_df.set_index("Data_Point")
-
-            data.summary = summary_df
-            if DEBUG_MODE:
-                mem_usage = normal_df.memory_usage()
-                logging.debug(
-                    f"memory usage for "
-                    f"loaded data: \n{mem_usage}"
-                    f"\ntotal: {humanize_bytes(mem_usage.sum())}"
-                )
-                logging.debug(f"time used: {(time.time() - time_0):2.4f} s")
+        # --------- read stats-data (summary-data) ---------------------
+        sql = "select * from %s where %s=%s order by %s" % (
+            table_name_stats,
+            self.arbin_headers_normal.test_id_txt,
+            data._internal_test_number,
+            self.arbin_headers_normal.data_point_txt,
+        )
+        summary_df = pd.read_sql_query(sql=sa.text(sql), con=conn.connect())
 
-            data.raw = normal_df
-            data.raw_data_files_length.append(length_of_test)
-            data = self._post_process(data)
-            data = self.identify_last_data_point(data)
-            new_tests.append(data)
-        return new_tests
+        if summary_df.empty and prms.Reader.use_cellpy_stat_file:
+            txt = "\nCould not find any summary (stats-file)!"
+            txt += "\n -> issue make_summary(use_cellpy_stat_file=False)"
+            logging.debug(txt)
+            # TODO: Enforce creating a summary df or modify renaming summary df (post process part)
+        # normal_df = normal_df.set_index("Data_Point")
+
+        data.summary = summary_df
+        if DEBUG_MODE:
+            mem_usage = normal_df.memory_usage()
+            logging.debug(
+                f"memory usage for "
+                f"loaded data: \n{mem_usage}"
+                f"\ntotal: {humanize_bytes(mem_usage.sum())}"
+            )
+            logging.debug(f"time used: {(time.time() - time_0):2.4f} s")
+
+        data.raw = normal_df
+        data.raw_data_files_length.append(length_of_test)
+        data = self._post_process(data)
+        data = self.identify_last_data_point(data)
+        return data
 
     def _load_win_res_auxiliary_table(
         self, conn, normal_df, table_name_aux, table_name_aux_global, test_id
     ):
         aux_global_data_df = self._query_table(table_name_aux_global, conn)
         if not aux_global_data_df.empty:
             aux_df = self._get_aux_df(conn, test_id, table_name_aux)
@@ -963,16 +809,14 @@
 
         table_name_global = TABLE_NAMES["global"]
         table_name_stats = TABLE_NAMES["statistic"]
         table_name_normal = TABLE_NAMES["normal"]
         table_name_aux_global = TABLE_NAMES["aux_global"]
         table_name_aux = TABLE_NAMES["aux"]
 
-        new_tests = []
-
         if is_posix:
             if is_macos:
                 self.logger.debug("\nMAC OSX USING MDBTOOLS")
             else:
                 self.logger.debug("\nPOSIX USING MDBTOOLS")
         else:
             self.logger.debug("\nWINDOWS USING MDBTOOLS-WIN")
@@ -1001,161 +845,142 @@
         tests = global_data_df[self.arbin_headers_normal.test_id_txt]
         number_of_sets = len(tests)
         self.logger.debug("number of datasets: %i" % number_of_sets)
 
         if dataset_number is not None:
             self.logger.info(f"Dataset number given: {dataset_number}")
             self.logger.info(f"Available dataset numbers: {tests}")
-            test_nos = [dataset_number]
         else:
-            test_nos = range(number_of_sets)
+            dataset_number = 0
 
-        for counter, test_no in enumerate(test_nos):
-            if counter > 0:
-                self.logger.warning("** WARNING ** MULTI-TEST-FILE (not recommended)")
-                if not ALLOW_MULTI_TEST_FILE:
-                    break
-            data = self._init_data(file_name, global_data_df, test_no)
-
-            self.logger.debug("reading raw-data")
-
-            (
-                length_of_test,
-                normal_df,
-                summary_df,
-                aux_global_data_df,
-                aux_df,
-            ) = self._load_from_tmp_files(
-                data,
-                tmp_name_global,
-                tmp_name_raw,
-                tmp_name_stats,
-                tmp_name_aux_global,
-                tmp_name_aux,
-                temp_filename,
-                bad_steps,
-                data_points,
-            )
+        data = self._init_data(file_name, global_data_df, dataset_number)
 
-            # --------- read auxiliary data (aux-data) ---------------------
-            normal_df = self._load_posix_res_auxiliary_table(
-                aux_global_data_df, aux_df, normal_df
-            )
+        self.logger.debug("reading raw-data")
 
-            if summary_df.empty and prms.Reader.use_cellpy_stat_file:
-                txt = "\nCould not find any summary (stats-file)!"
-                txt += "\n -> issue make_summary(use_cellpy_stat_file=False)"
-                logging.debug(txt)
-            # normal_df = normal_df.set_index("Data_Point")
-
-            data.summary = summary_df
-            if DEBUG_MODE:
-                mem_usage = normal_df.memory_usage()
-                logging.debug(
-                    f"memory usage for "
-                    f"loaded data: \n{mem_usage}"
-                    f"\ntotal: {humanize_bytes(mem_usage.sum())}"
-                )
-                logging.debug(f"time used: {(time.time() - time_0):2.4f} s")
+        (
+            length_of_test,
+            normal_df,
+            summary_df,
+            aux_global_data_df,
+            aux_df,
+        ) = self._load_from_tmp_files(
+            data,
+            tmp_name_global,
+            tmp_name_raw,
+            tmp_name_stats,
+            tmp_name_aux_global,
+            tmp_name_aux,
+            temp_filename,
+            bad_steps,
+            data_points,
+        )
+
+        # --------- read auxiliary data (aux-data) ---------------------
+        normal_df = self._load_posix_res_auxiliary_table(
+            aux_global_data_df, aux_df, normal_df
+        )
 
-            data.raw = normal_df
-            data.raw_data_files_length.append(length_of_test)
-            data = self._post_process(data)
-            data = self.identify_last_data_point(data)
-            new_tests.append(data)
-        return new_tests
+        if summary_df.empty and prms.Reader.use_cellpy_stat_file:
+            txt = "\nCould not find any summary (stats-file)!"
+            txt += "\n -> issue make_summary(use_cellpy_stat_file=False)"
+            logging.debug(txt)
+        # normal_df = normal_df.set_index("Data_Point")
+
+        data.summary = summary_df
+        if DEBUG_MODE:
+            mem_usage = normal_df.memory_usage()
+            logging.debug(
+                f"memory usage for "
+                f"loaded data: \n{mem_usage}"
+                f"\ntotal: {humanize_bytes(mem_usage.sum())}"
+            )
+            logging.debug(f"time used: {(time.time() - time_0):2.4f} s")
+
+        data.raw = normal_df
+        data.raw_data_files_length.append(length_of_test)
+        data = self._post_process(data)
+        data = self.identify_last_data_point(data)
+        return data
 
     def loader(
         self,
-        file_name,
+        name,
         *args,
         bad_steps=None,
         dataset_number=None,
         data_points=None,
         **kwargs,
     ):
         """Loads data from arbin .res files.
 
         Args:
-            file_name (str): path to .res file.
+            name (str): path to .res file.
             bad_steps (list of tuples): (c, s) tuples of steps s (in cycle c)
                 to skip loading.
             dataset_number (int): the data set number to select if you are dealing
                 with arbin files with more than one data-set.
             data_points (tuple of ints): load only data from data_point[0] to
                     data_point[1] (use None for infinite).
 
         Returns:
-            new_tests (list of data objects)
+            new data (Data)
         """
         # TODO: @jepe - insert kwargs - current chunk, only normal data, etc
 
-        if not os.path.isfile(file_name):
-            self.logger.info("Missing file_\n   %s" % file_name)
-            return None
-
-        self.logger.debug("in loader")
-        self.logger.debug("filename: %s" % file_name)
+        self.logger.debug(f"tmp file: {self.temp_file_path}")
+        self.logger.debug(f"tmp dir: {self.temp_file_path.parent}")
 
-        filesize = os.path.getsize(file_name)
-        hfilesize = humanize_bytes(filesize)
-        txt = "Filesize: %i (%s)" % (filesize, hfilesize)
+        file_size = os.path.getsize(self.temp_file_path)
+        hfilesize = humanize_bytes(file_size)
+        txt = f"File size: {file_size} ({hfilesize})"
         self.logger.debug(txt)
-        if (
-            filesize > prms.Instruments.Arbin.max_res_filesize
-            and not prms.Reader.load_only_summary
-        ):
+        if file_size > prms.Instruments.Arbin.max_res_filesize:
             error_message = "\nERROR (loader):\n"
-            error_message += "%s > %s - File is too big!\n" % (
-                hfilesize,
-                humanize_bytes(prms.Instruments.Arbin.max_res_filesize),
+            error_message += (
+                f"{hfilesize} > {humanize_bytes(prms.Instruments.Arbin.max_res_filesize)} "
+                f"- File is too big!\n"
             )
             error_message += "(edit prms.Instruments.Arbin ['max_res_filesize'])\n"
             print(error_message)
             return None
 
-        temp_dir = tempfile.gettempdir()
-        temp_filename = os.path.join(temp_dir, os.path.basename(file_name))
-        shutil.copy2(file_name, temp_dir)
-        self.logger.debug("tmp file: %s" % temp_filename)
-
         use_mdbtools = False
         if use_subprocess:
             use_mdbtools = True
         if is_posix:
             use_mdbtools = True
 
         if use_mdbtools:
-            new_tests = self._loader_posix(
-                file_name,
-                temp_filename,
-                temp_dir,
+            new_data = self._loader_posix(
+                self.name,
+                self.temp_file_path,
+                self.temp_file_path.parent,
                 *args,
                 bad_steps=bad_steps,
                 dataset_number=dataset_number,
                 data_points=data_points,
                 **kwargs,
             )
         else:
-            new_tests = self._loader_win(
-                file_name,
-                temp_filename,
+            new_data = self._loader_win(
+                self.name,
+                self.temp_file_path,
                 *args,
                 bad_steps=bad_steps,
                 dataset_number=dataset_number,
                 data_points=data_points,
                 **kwargs,
             )
 
-        new_tests = self._inspect(new_tests)
+        new_data = self._inspect(new_data)
 
-        return new_tests
+        return new_data
 
+    @staticmethod
     def _create_tmp_files(
-        self,
         table_name_global,
         table_name_normal,
         table_name_stats,
         table_name_aux_global,
         table_name_aux,
         temp_dir,
         temp_filename,
@@ -1175,16 +1000,25 @@
             (table_name_stats, temp_csv_filename_stats),
             (table_name_aux_global, temp_csv_filename_aux_global),
             (table_name_aux, temp_csv_filename_aux),
         ]
         # executing cmds
         for table_name, tmp_file in mdb_prms:
             with open(tmp_file, "w") as f:
-                subprocess.call([sub_process_path, temp_filename, table_name], stdout=f)
-                self.logger.debug(f"ran mdb-export {str(f)} {table_name}")
+                try:
+                    subprocess.call(
+                        [sub_process_path, temp_filename, table_name], stdout=f
+                    )
+                    logging.debug(f"ran mdb-export {str(f)} {table_name}")
+                except FileNotFoundError as e:
+                    logging.critical(
+                        f"Could not run {sub_process_path} on {temp_filename}"
+                    )
+                    logging.critical(f"Possible work-around: install mdbtools")
+                    raise e
         return (
             temp_csv_filename_global,
             temp_csv_filename_normal,
             temp_csv_filename_stats,
             temp_csv_filename_aux_global,
             temp_csv_filename_aux,
         )
@@ -1197,37 +1031,35 @@
         temp_csv_filename_stats,
         temp_csv_filename_aux_global,
         temp_csv_filename_aux,
         temp_filename,
         bad_steps,
         data_points,
     ):
-
         """
         if bad_steps is not None:
             if not isinstance(bad_steps, (list, tuple)):
                 bad_steps = [bad_steps]
             for bad_cycle, bad_step in bad_steps:
                 self.logger.debug(f"bad_step def: [c={bad_cycle}, s={bad_step}]")
                 sql_4 += "AND NOT (%s=%i " % (
                     self.headers_normal.cycle_index_txt,
                     bad_cycle,
                 )
                 sql_4 += "AND %s=%i) " % (self.headers_normal.step_index_txt, bad_step)
 
-
-
         """
         # should include a more efficient to load the csv (maybe a loop where
         #   we load only chuncks and only keep the parts that fullfill the
         #   filters (e.g. bad_steps, data_points,...)
         normal_df = pd.read_csv(temp_csv_filename_normal)
         # filter on test ID
         normal_df = normal_df[
-            normal_df[self.arbin_headers_normal.test_id_txt] == data.test_ID
+            normal_df[self.arbin_headers_normal.test_id_txt]
+            == data._internal_test_number
         ]
         # sort on data point
         if prms._sort_if_subprocess:
             normal_df = normal_df.sort_values(self.arbin_headers_normal.data_point_txt)
 
         if bad_steps is not None:
             logging.debug("removing bad steps")
@@ -1285,69 +1117,70 @@
             temp_csv_filename_aux_global,
             temp_csv_filename_aux,
         ]:
             if os.path.isfile(f):
                 try:
                     os.remove(f)
                 except WindowsError as e:
-                    self.logger.warning(f"could not remove tmp-file\n{f} {e}")
+                    logging.warning(f"could not remove tmp-file\n{f} {e}")
         return length_of_test, normal_df, summary_df, aux_global_df, aux_df
 
     def _init_data(self, file_name, global_data_df, test_no):
-        data = Cell()
-        data.cell_no = test_no
+        data = Data()
         data.loaded_from = file_name
-        fid = FileID(file_name)
+        self.generate_fid()
         # name of the .res file it is loaded from:
         # data.parent_filename = os.path.basename(file_name)
         data.channel_index = int(
             global_data_df[self.arbin_headers_global["channel_index_txt"]][test_no]
         )
-        data.channel_number = int(
-            global_data_df[self.arbin_headers_global["channel_number_txt"]][test_no]
-        )
+
         data.creator = global_data_df[self.arbin_headers_global["creator_txt"]][test_no]
-        data.item_ID = global_data_df[self.arbin_headers_global["item_id_txt"]][test_no]
+        data.test_ID = global_data_df[self.arbin_headers_global["item_id_txt"]][test_no]
         data.schedule_file_name = global_data_df[
             self.arbin_headers_global["schedule_file_name_txt"]
         ][test_no]
         data.start_datetime = global_data_df[
             self.arbin_headers_global["start_datetime_txt"]
         ][test_no]
-        data.test_ID = int(
+        data._internal_test_number = int(
             global_data_df[self.arbin_headers_normal.test_id_txt][test_no]
         )
         data.test_name = global_data_df[self.arbin_headers_global["test_name_txt"]][
             test_no
         ]
-        data.raw_data_files.append(fid)
+        data.raw_data_files.append(self.fid)
         return data
 
     def _normal_table_generator(self, **kwargs):
         pass
 
-    def _load_res_normal_table(self, conn, test_ID, bad_steps, data_points):
+    def _load_res_normal_table(
+        self, conn, _internal_test_number, bad_steps, data_points
+    ):
         self.logger.debug("starting loading raw-data")
-        self.logger.debug(f"connection: {conn} test-ID: {test_ID}")
+        self.logger.debug(
+            f"connection: {conn} internal test-ID: {_internal_test_number}"
+        )
         self.logger.debug(f"bad steps:  {bad_steps}")
 
         table_name_normal = TABLE_NAMES["normal"]
 
-        if prms.Reader.load_only_summary:  # SETTING
-            warnings.warn("not implemented")
-
         if prms.Reader.select_minimal:  # SETTING
             columns = MINIMUM_SELECTION
             columns_txt = ", ".join(["%s"] * len(columns)) % tuple(columns)
         else:
             columns_txt = "*"
 
         sql_1 = "select %s " % columns_txt
         sql_2 = "from %s " % table_name_normal
-        sql_3 = "where %s=%s " % (self.arbin_headers_normal.test_id_txt, test_ID)
+        sql_3 = "where %s=%s " % (
+            self.arbin_headers_normal.test_id_txt,
+            _internal_test_number,
+        )
         sql_4 = ""
 
         if bad_steps is not None:
             if not isinstance(bad_steps, (list, tuple)):
                 bad_steps = [bad_steps]
             if not isinstance(bad_steps[0], (list, tuple)):
                 bad_steps = [bad_steps]
@@ -1399,15 +1232,19 @@
             # memory here
             normal_df = pd.read_sql_query(sql=sa.text(sql), con=conn.connect())
             # memory here
             length_of_test = normal_df.shape[0]
         else:
             self.logger.debug(f"chunk-size: {prms.Instruments.Arbin.chunk_size}")
             self.logger.debug("creating a pd.read_sql_query generator")
-            normal_df_reader = pd.read_sql_query(sql=sa.text(sql), con=conn.connect(), chunksize=prms.Instruments.Arbin.chunk_size
+
+            normal_df_reader = pd.read_sql_query(
+                sql=sa.text(sql),
+                con=conn.connect(),
+                chunksize=prms.Instruments.Arbin.chunk_size,
             )
             normal_df = None
             chunk_number = 0
             self.logger.debug("created pandas sql reader")
             self.logger.debug("iterating chunk-wise")
             for i, chunk in enumerate(normal_df_reader):
                 self.logger.debug(f"iteration number {i}")
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/arbin_sql.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/arbin_sql_7.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,27 +1,30 @@
-"""arbin MS SQL Server data"""
+"""arbin MS SQL Server data for MITS 7.0"""
+
 import datetime
 import logging
 import os
 import platform
 import shutil
 import sys
 import tempfile
 import time
 import warnings
+import sqlalchemy
+import urllib
 
 import numpy as np
 import pandas as pd
 import pyodbc
 from dateutil.parser import parse
 
 from cellpy import prms
 from cellpy.parameters.internal_settings import HeaderDict, get_headers_normal
 from cellpy.readers.core import (
-    Cell,
+    Data,
     FileID,
     check64bit,
     humanize_bytes,
     xldate_as_datetime,
 )
 from cellpy.readers.instruments.base import BaseLoader
 
@@ -34,15 +37,14 @@
 ODBC = prms._odbc
 SEARCH_FOR_ODBC_DRIVERS = prms._search_for_odbc_driver  # not used
 SQL_SERVER = prms.Instruments.Arbin["SQL_server"]
 SQL_UID = prms.Instruments.Arbin["SQL_UID"]
 SQL_PWD = prms.Instruments.Arbin["SQL_PWD"]
 SQL_DRIVER = prms.Instruments.Arbin["SQL_Driver"]
 
-
 # Names of the tables in the SQL Server db that is used by cellpy
 
 # Not used anymore - maybe use a similar dict for the SQL table names (they are hard-coded at the moment)
 TABLE_NAMES = {
     "normal": "Channel_Normal_Table",
     "global": "Global_Table",
     "statistic": "Channel_Statistic_Table",
@@ -123,15 +125,16 @@
     time_in_str = datetime_object.strftime("%y-%m-%d %H:%M:%S:%f")
     return time_in_str
 
 
 class DataLoader(BaseLoader):
     """Class for loading arbin-data from MS SQL server."""
 
-    name = "arbin_sql"
+    name = "arbin_sql_7"
+    _is_db = True
 
     def __init__(self, *args, **kwargs):
         """initiates the ArbinSQLLoader class"""
         self.arbin_headers_normal = (
             get_headers_normal()
         )  # the column headers defined by Arbin
         self.cellpy_headers_normal = (
@@ -209,18 +212,18 @@
         headers["test_id_txt"] = "Test_ID"  # KEEP FOR CELLPY FILE FORMAT
         headers["test_name_txt"] = "Test_Name"  # KEEP FOR CELLPY FILE FORMAT
         return headers
 
     @staticmethod
     def get_raw_units():
         raw_units = dict()
-        raw_units["current"] = 1.0  # A
-        raw_units["charge"] = 1.0  # Ah
-        raw_units["mass"] = 1.0  # g
-        raw_units["voltage"] = 1.0  # V
+        raw_units["current"] = "A"
+        raw_units["charge"] = "Ah"
+        raw_units["mass"] = "g"
+        raw_units["voltage"] = "V"
         return raw_units
 
     @staticmethod
     def get_raw_limits():
         """returns a dictionary with resolution limits"""
         raw_limits = dict()
         raw_limits["current_hard"] = 0.000_000_000_000_1
@@ -230,83 +233,90 @@
         raw_limits["stable_voltage_hard"] = 2.0
         raw_limits["stable_voltage_soft"] = 4.0
         raw_limits["stable_charge_hard"] = 0.001
         raw_limits["stable_charge_soft"] = 5.0
         raw_limits["ir_change"] = 0.00001
         return raw_limits
 
-    # TODO: rename this (for all instruments) to e.g. load
-    # TODO: implement more options (bad_cycles, ...)
     def loader(self, name, **kwargs):
-        """returns a Cell object with loaded data.
+        """returns a Data object with loaded data.
 
         Loads data from arbin SQL server db.
 
         Args:
             name (str): name of the test
 
         Returns:
             new_tests (list of data objects)
         """
-        new_tests = []
 
-        data_df, stat_df = self._query_sql(name)
+        warnings.warn(
+            "This loader is under development and might be missing some features."
+        )
+
+        # new_tests = []
+        # chonmj: seems to be broken at the moment. cellreader assumes a
+        #         datatype "loader", not a list. removing the list for now.
+        # jepegit: Correct, this is no longer supported.
+
+        data_df, meta_data = self._query_sql()
+
         aux_data_df = None  # Needs to be implemented
-        meta_data = None  # Should be implemented
 
         # init data
+        # selecting only one value (might implement id selection later)
+        test_id = meta_data["Test_ID"].iloc[0]
+        id_name = f"{SQL_SERVER}:{self.name}:{test_id}"
 
-        # selecting only one value (might implement multi-channel/id use later)
-        test_id = data_df["Test_ID"].iloc[0]
-        id_name = f"{SQL_SERVER}:{name}:{test_id}"
-
-        channel_id = data_df["Channel_ID"].iloc[0]
+        channel_id = meta_data["IV_Ch_ID"][0]
 
-        data = Cell()
+        data = Data()
         data.loaded_from = id_name
         data.channel_index = channel_id
         data.test_ID = test_id
-        data.test_name = name
+        data.test_name = self.name
 
-        # The following meta data is not implemented yet for SQL loader:
+        # The following metadata is not implemented yet for SQL loader:
         data.channel_number = None
-        data.creator = None
         data.item_ID = None
-        data.schedule_file_name = None
-        data.start_datetime = None
-
-        # Generating a FileID project - needs to be updated to allow for db queries:
-        fid = FileID(id_name)
-        data.raw_data_files.append(fid)
+        # Implemented metadata:
+        data.schedule_file_name = meta_data["Schedule_File_Name"][0]
+        data.start_datetime = meta_data["First_Start_DateTime"][0]
+        data.creator = meta_data["Creator"][0]
+
+        # Generating a FileID project:
+        self.generate_fid()
+        data.raw_data_files.append(self.fid)
 
         data.raw = data_df
         data.raw_data_files_length.append(len(data_df))
-        data.summary = stat_df
         data = self._post_process(data)
-        data = self.identify_last_data_point(data)
-        new_tests.append(data)
 
-        return new_tests
+        # TODO: implement this:
+        # data = self.identify_last_data_point(data)
+
+        return data
 
     def _post_process(self, data, **kwargs):
         # TODO: move this to parent
-
+        logging.debug(f"{kwargs=}")
         fix_datetime = kwargs.pop("fix_datetime", True)
         set_index = kwargs.pop("set_index", True)
         rename_headers = kwargs.pop("rename_headers", True)
         extract_start_datetime = kwargs.pop("extract_start_datetime", True)
 
         # TODO:  insert post-processing and div tests here
         #    - check dtypes
 
         # Remark that we also set index during saving the file to hdf5 if
         #   it is not set.
         from pprint import pprint
 
         if rename_headers:
+            logging.debug("rename headers: True")
             columns = {}
             for key in self.arbin_headers_normal:
                 old_header = normal_headers_renaming_dict.get(key, None)
                 new_header = self.cellpy_headers_normal[key]
                 if old_header:
                     columns[old_header] = new_header
                 logging.debug(
@@ -323,104 +333,138 @@
                     except KeyError:
                         columns[old_header] = old_header.lower()
                 data.summary.rename(index=str, columns=columns, inplace=True)
             except Exception as e:
                 logging.debug(f"Could not rename summary df ::\n{e}")
 
         if fix_datetime:
-
+            logging.debug("fix date_time: true")
             h_datetime = self.cellpy_headers_normal.datetime_txt
             logging.debug("converting to datetime format")
 
             data.raw[h_datetime] = data.raw[h_datetime].apply(from_arbin_to_datetime)
 
             h_datetime = h_datetime
             if h_datetime in data.summary:
                 data.summary[h_datetime] = data.summary[h_datetime].apply(
                     from_arbin_to_datetime
                 )
 
-        if set_index:
-            hdr_data_point = self.cellpy_headers_normal.data_point_txt
-            if data.raw.index.name != hdr_data_point:
-                data.raw = data.raw.set_index(hdr_data_point, drop=False)
+        # if set_index:
+        #     hdr_data_point = self.cellpy_headers_normal.data_point_txt
+        #     if data.raw.index.name != hdr_data_point:
+        #         data.raw = data.raw.set_index(hdr_data_point, drop=False)
 
         if extract_start_datetime:
             hdr_date_time = self.arbin_headers_normal.datetime_txt
             data.start_datetime = parse("20" + data.raw[hdr_date_time].iat[0][:-7])
 
         return data
 
-    def _query_sql(self, name):
+    def _query_sql(self):
         # TODO: refactor and include optional SQL arguments
+        name = self.name
         name_str = f"('{name}', '')"
-        con_str = (
-            f"Driver={{{SQL_DRIVER}}};" + f"Server={SQL_SERVER};Trusted_Connection=yes;"
+
+        # prepare engine
+        params = urllib.parse.quote_plus(
+            f"DRIVER={SQL_DRIVER};"
+            f"SERVER={SQL_SERVER};"
+            f"DATABASE=ArbinMasterData;"
+            f"UID={SQL_UID};"
+            f"PWD={SQL_PWD}"
         )
 
-        # TODO: use variable for the name of the main db (ArbinPro8....)
-        # TODO: consider making a function that searches for correct ArbinPro version
+        # Create engine to SQL server using SQLAlchemy (mssql+pyodbc)
+        con_url = "mssql+pyodbc:///?odbc_connect={}".format(params)
+        engine = sqlalchemy.create_engine(con_url)
+
+        # Initial query to obtain metadata info on cell with 'name'
+
         master_q = (
-            "SELECT Database_Name, Test_Name FROM "
-            "ArbinPro8MasterInfo.dbo.TestList_Table WHERE "
-            f"ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name IN {name_str}"
+            "SELECT ArbinMasterData.dbo.TestIVChList_Table.*, "
+            "ArbinMasterData.dbo.TestList_Table.* FROM "
+            "ArbinMasterData.dbo.TestIVChList_Table "
+            "JOIN ArbinMasterData.dbo.TestList_Table "
+            "ON ArbinMasterData.dbo.TestIVChList_Table.Test_ID = "
+            "ArbinMasterData.dbo.TestList_Table.Test_ID "
+            f"WHERE ArbinMasterData.dbo.TestList_Table.Test_Name IN {name_str}"
         )
+        with engine.connect() as connection:
+            meta_data = pd.read_sql(master_q, connection)
 
-        conn = pyodbc.connect(con_str)
-        sql_query = pd.read_sql_query(master_q, conn)
+        # drop duplicate columns
+        meta_data = meta_data.loc[:, ~meta_data.columns.duplicated()].copy()
 
+        # query data
         datas_df = []
-        stats_df = []
 
-        for index, row in sql_query.iterrows():
+        for index, row in meta_data.iterrows():
             # TODO: use variables - see above
             # TODO: consider to use f-strings
+
+            # MITS 7 organizes raw channel data by Channel_ID and Date_Time, so the
+            # data query requires that we filter by these tables from the events table.
+            # Also, Date_Time is 7 orders higher than standard datetime, hence the additional
+            # zeroes in the query.
             data_query = (
-                "SELECT "
+                "SELECT " + str(row["Database_Name"]) + f".dbo.Channel_RawData_Table.* "
+                "FROM " + str(row["Database_Name"]) + ".dbo.Channel_RawData_Table "
+                " WHERE "
+                + str(row["Database_Name"])
+                + ".dbo.Channel_RawData_Table.Channel_ID = "
+                + str(row["IV_Ch_ID"])
+                + " AND "
                 + str(row["Database_Name"])
-                + ".dbo.IV_Basic_Table.*, ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name "
-                "FROM " + str(row["Database_Name"]) + ".dbo.IV_Basic_Table "
-                "JOIN ArbinPro8MasterInfo.dbo.TestList_Table "
-                "ON "
+                + ".dbo.Channel_RawData_Table.Date_Time >= "
+                + str(row["First_Start_DateTime"])
+                + str("0000000")
+                + " AND "
                 + str(row["Database_Name"])
-                + ".dbo.IV_Basic_Table.Test_ID = ArbinPro8MasterInfo.dbo.TestList_Table.Test_ID "
-                "WHERE ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name IN "
-                + str(name_str)
+                + ".dbo.Channel_RawData_Table.Date_Time <= "
+                + str(row["Last_End_DateTime"])
+                + str("0000000")
             )
 
-            stat_query = (
-                "SELECT "
-                + str(row["Database_Name"])
-                + ".dbo.StatisticData_Table.*, ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name "
-                "FROM " + str(row["Database_Name"]) + ".dbo.StatisticData_Table "
-                "JOIN ArbinPro8MasterInfo.dbo.TestList_Table "
-                "ON "
-                + str(row["Database_Name"])
-                + ".dbo.StatisticData_Table.Test_ID = ArbinPro8MasterInfo.dbo.TestList_Table.Test_ID "
-                "WHERE ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name IN "
-                + str(name_str)
+            with engine.connect() as connection:
+                raw_df = pd.read_sql(data_query, connection)
+
+            # sort dataframe via pivot table:
+            datas_df.append(
+                raw_df.pivot(
+                    index="Date_Time", columns="Data_Type", values="Data_Value"
+                ).reset_index()
             )
 
-            datas_df.append(pd.read_sql_query(data_query, conn))
-            stats_df.append(pd.read_sql_query(stat_query, conn))
+            # convert column headers to strings
+            datas_df[index].columns = datas_df[index].columns.astype(str)
+            # TODO: rename columns
+            #   21: PV_Voltage
+            #   22: PV_Current
+            #   23: PV_Charge_Capacity
+            #   24: PV_Discharge_Capacity
+            #   25: PV_Charge_Energy
+            #   26: PV_Discharge_Energy
+            #   27: PV_dVdt
+            #   30: PV_InternalResistance
+            # Full column key found in 'SQL Table IDs.txt' file.
 
         data_df = pd.concat(datas_df, axis=0)
-        stat_df = pd.concat(stats_df, axis=0)
 
-        return data_df, stat_df
+        return data_df, meta_data
 
 
 def check_sql_loader(server: str = None, tests: list = None):
     test_name = tuple(tests) + ("",)  # neat trick :-)
     print(f"** test str: {test_name}")
     con_str = "Driver={SQL Server};Server=" + server + ";Trusted_Connection=yes;"
     master_q = (
         "SELECT Database_Name, Test_Name FROM "
-        "ArbinPro8MasterInfo.dbo.TestList_Table WHERE "
-        f"ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name IN {test_name}"
+        "ArbinMasterData.dbo.TestList_Table WHERE "
+        f"ArbinMasterData.dbo.TestList_Table.Test_Name IN {test_name}"
     )
 
     conn = pyodbc.connect(con_str)
     print("** connected to server")
     sql_query = pd.read_sql_query(master_q, conn)
     print("** SQL query:")
     print(sql_query)
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/arbin_sql_csv.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/arbin_sql_csv.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """arbin MS SQL Server csv data"""
 
 import pandas as pd
 from dateutil.parser import parse
 
 from cellpy import prms
 from cellpy.parameters.internal_settings import HeaderDict, get_headers_normal
-from cellpy.readers.core import Cell, FileID
+from cellpy.readers.core import Data, FileID
 from cellpy.readers.instruments.base import BaseLoader
 
 DEBUG_MODE = prms.Reader.diagnostics  # not used
 ALLOW_MULTI_TEST_FILE = prms._allow_multi_test_file  # not used
 
 # Not used yet - only supporting loading raw data (normal)
 FILE_NAME_POST_LABEL = {
@@ -79,15 +79,15 @@
     f"dV/dQ({incremental_unit_labels['dv_dq']})": "dv_dq",
 }
 
 
 class DataLoader(BaseLoader):
     """Class for loading arbin-data from MS SQL server."""
 
-    name = "arbin_sql_csv"
+    instrument_name = "arbin_sql_csv"
     raw_ext = "csv"
 
     def __init__(self, *args, **kwargs):
         """initiates the ArbinSQLLoader class"""
         self.arbin_headers_normal = (
             self.get_headers_normal()
         )  # the column headers defined by Arbin
@@ -114,18 +114,18 @@
                 headers[col] = ncol.lower()
 
         return headers
 
     @staticmethod
     def get_raw_units():
         raw_units = dict()
-        raw_units["current"] = 1.0  # A
-        raw_units["charge"] = 1.0  # Ah
-        raw_units["mass"] = 1.0  # g
-        raw_units["voltage"] = 1.0  # V
+        raw_units["current"] = "A"
+        raw_units["charge"] = "Ah"
+        raw_units["mass"] = "g"
+        raw_units["voltage"] = "V"
         return raw_units
 
     @staticmethod
     def get_raw_limits():
         """returns a dictionary with resolution limits"""
         raw_limits = dict()
         raw_limits["current_hard"] = 0.000_000_000_000_1
@@ -138,55 +138,52 @@
         raw_limits["stable_charge_soft"] = 5.0
         raw_limits["ir_change"] = 0.00001
         return raw_limits
 
     # TODO: rename this (for all instruments) to e.g. load
     # TODO: implement more options (bad_cycles, ...)
     def loader(self, name, **kwargs):
-        """returns a Cell object with loaded data.
+        """returns a Data object with loaded data.
 
         Loads data from arbin SQL server db.
 
         Args:
             name (str): name of the file
 
         Returns:
             new_tests (list of data objects)
         """
-        new_tests = []
+        # self.name = name
+        # self.copy_to_temporary()
+        data_df = self._query_csv(self.temp_file_path)
 
-        data_df = self._query_csv(name)
-
-        data = Cell()
+        data = Data()
 
         # metadata is unfortunately not available for csv dumps
-        data.loaded_from = name
+        data.loaded_from = self.name
         data.channel_index = None
         data.test_ID = None
-        data.test_name = name  # should fix this
-        data.channel_number = None
+        data.test_name = self.name.name
         data.creator = None
-        data.item_ID = None
         data.schedule_file_name = None
         data.start_datetime = None
 
         # Generating a FileID project:
-        fid = FileID(name)
-        data.raw_data_files.append(fid)
+        self.generate_fid()
+        data.raw_data_files.append(self.fid)
 
         data.raw = data_df
         data.raw_data_files_length.append(len(data_df))
         data.summary = (
             pd.DataFrame()
         )  # creating an empty frame - loading summary is not implemented yet
         data = self._post_process(data)
         data = self.identify_last_data_point(data)
-        new_tests.append(data)
 
-        return new_tests
+        return data
 
     def _post_process(self, data):
         set_index = True
         rename_headers = True
         if rename_headers:
             columns = {}
             for key in self.arbin_headers_normal:
@@ -239,26 +236,26 @@
 
     from cellpy import cellreader
 
     datadir = pathlib.Path(
         r"C:\scripts\cellpy\dev_data\arbin_new\2021_02_02_standardageing_1C_25dC_1_2021_02_02_130709"
     )
     name = datadir / "2021_02_02_standardageing_1C_25dC_1_Channel_1_Wb_1.CSV"
-    c = cellreader.CellpyData()
+    c = cellreader.CellpyCell()
     c.set_instrument("arbin_sql_csv")
 
     c.from_raw(name)
     c.set_mass(1000)
 
     c.make_step_table()
     c.make_summary()
 
-    # raw = c.cell.raw
-    # steps = c.cell.steps
-    # summary = c.cell.summary
+    # raw = c.data.raw
+    # steps = c.data.steps
+    # summary = c.data.summary
     # raw.to_csv(r"C:\scripts\notebooks\Div\trash\raw.csv", sep=";")
     # steps.to_csv(r"C:\scripts\notebooks\Div\trash\steps.csv", sep=";")
     # summary.to_csv(r"C:\scripts\notebooks\Div\trash\summary.csv", sep=";")
     #
     # n = c.get_number_of_cycles()
     # print(f"number of cycles: {n}")
     #
@@ -282,15 +279,15 @@
     # plt.plot(t, steps)
     # plt.show()
 
     outfile = datadir / "test_out"
     c.save(outfile)
 
 
-def test_seamless_files():
+def check_seamless_files():
     import pathlib
 
     import matplotlib.pyplot as plt
 
     from cellpy import cellreader, prms
 
     datadir = pathlib.Path(
@@ -301,26 +298,26 @@
         / r"20210430_seam10_01_01_cc_01_2021_04_30_172207\20210430_seam10_01_01_cc_01_Channel_48_Wb_1.csv"
     )
     name2 = (
         datadir
         / r"20210430_seam10_01_01_cc_01_2021_04_30_172207\20210430_seam10_01_01_cc_01_Channel_48_Wb_1.csv"
     )
 
-    c = cellreader.CellpyData()
+    c = cellreader.CellpyCell()
     c.set_instrument("arbin_sql_csv")
 
     prms.Reader.sep = ";"
 
     c.from_raw(name1)
     c.set_mass(0.016569)
 
     c.make_step_table()
     c.make_summary()
 
     names = [name1, name2]
-    cell_data = cellreader.CellpyData()
+    cell_data = cellreader.CellpyCell()
     cell_data.set_instrument("arbin_sql_csv")
-    cell_data.loadcell(names, mass=0.016569)
+    cell_data.from_raw(names, mass=0.016569)
 
 
 if __name__ == "__main__":
-    test_seamless_files()
+    check_seamless_files()
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/arbin_sql_xlsx.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/arbin_sql_xlsx.py`

 * *Files 9% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 
 import pandas as pd
 from dateutil.parser import parse
 
 from cellpy import prms
 from cellpy.exceptions import WrongFileVersion
 from cellpy.parameters.internal_settings import HeaderDict, get_headers_normal
-from cellpy.readers.core import Cell, FileID
+from cellpy.readers.core import Data, FileID
 from cellpy.readers.instruments.base import BaseLoader
 
 DEBUG_MODE = prms.Reader.diagnostics  # not used
 ALLOW_MULTI_TEST_FILE = prms._allow_multi_test_file  # not used
 
 SHEET_NAME_KEYWORD = "Channel"
 
@@ -86,15 +86,15 @@
     f"dV/dQ({incremental_unit_labels['dv_dq']})": "dv_dq",
 }
 
 
 class DataLoader(BaseLoader):
     """Class for loading arbin-data from MS SQL server."""
 
-    name = "arbin_sql_xlsx"
+    instrument_name = "arbin_sql_xlsx"
     raw_ext = "xlsx"
 
     def __init__(self, *args, **kwargs):
         """initiates the ArbinSQLLoader class"""
         self.arbin_headers_normal = (
             self.get_headers_normal()
         )  # the column headers defined by Arbin
@@ -119,19 +119,18 @@
                 headers[col] = ncol.lower()
 
         return headers
 
     @staticmethod
     def get_raw_units():
         raw_units = dict()
-        raw_units["current"] = 1.0  # A
-        raw_units["charge"] = 1.0  # Ah
-        raw_units["mass"] = 1.0  # g
-        raw_units["voltage"] = 1.0  # V
-
+        raw_units["current"] = "A"
+        raw_units["charge"] = "Ah"
+        raw_units["mass"] = "g"
+        raw_units["voltage"] = "V"
         return raw_units
 
     @staticmethod
     def get_raw_limits():
         """returns a dictionary with resolution limits"""
         raw_limits = dict()
         raw_limits["current_hard"] = 0.000_000_000_000_1
@@ -144,52 +143,50 @@
         raw_limits["stable_charge_soft"] = 5.0
         raw_limits["ir_change"] = 0.00001
         return raw_limits
 
     # TODO: rename this (for all instruments) to e.g. load
     # TODO: implement more options (bad_cycles, ...)
     def loader(self, name, **kwargs):
-        """returns a Cell object with loaded data.
+        """returns a Data object with loaded data.
 
         Loads data from arbin SQL server db.
 
         Args:
             name (str): name of the file
 
         Returns:
-            new_tests (list of data objects)
+            data object
         """
-        new_tests = []
-        data_df = self._parse_xlsx_data(name)
-        data = Cell()
+        # self.name = name
+        # self.copy_to_temporary()
+        data_df = self._parse_xlsx_data()
+        data = Data()
 
         # metadata is unfortunately not available for csv dumps
-        data.loaded_from = name
+        data.loaded_from = self.name
         data.channel_index = None
         data.test_ID = None
-        data.test_name = name  # should fix this
-        data.channel_number = None
+        data.test_name = self.name.name  # should fix this
         data.creator = None
-        data.item_ID = None
         data.schedule_file_name = None
         data.start_datetime = None
 
         # Generating a FileID project:
-        fid = FileID(name)
-        data.raw_data_files.append(fid)
+        self.generate_fid()
+        data.raw_data_files.append(self.fid)
 
         data.raw = data_df
         data.raw_data_files_length.append(len(data_df))
         data.summary = (
             pd.DataFrame()
         )  # creating an empty frame - loading summary is not implemented yet
         data = self._post_process(data)
         data = self.identify_last_data_point(data)
-        new_tests.append(data)
-        return new_tests
+        return data
 
     def _post_process(self, data):
         set_index = True
         rename_headers = True
         forward_fill_ir = True
         backward_fill_ir = True
 
@@ -229,15 +226,16 @@
 
         hdr_date_time = self.arbin_headers_normal.datetime_txt
         start = data.raw[hdr_date_time].iat[0]
         data.start_datetime = start
 
         return data
 
-    def _parse_xlsx_data(self, file_name):
+    def _parse_xlsx_data(self):
+        file_name = self.temp_file_path
         date_time_col = normal_headers_renaming_dict["datetime_txt"]
         file_name = pathlib.Path(file_name)
         xlsx_file = pd.ExcelFile(file_name)
         sheet_names = [
             sheet
             for sheet in xlsx_file.sheet_names
             if SHEET_NAME_KEYWORD.upper() in sheet.upper()
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/backup_arbin.py` & `cellpy-1.0.0a0/cellpy/cli.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,1189 +1,1688 @@
-"""arbin res-type data files"""
+import base64
+import getpass
 import logging
 import os
-import platform
-import shutil
+import pathlib
+from pprint import pprint
+import re
+import subprocess
 import sys
-import tempfile
-import time
-import warnings
-
-import numpy as np
-import pandas as pd
-
-from cellpy import prms
-from cellpy.parameters.internal_settings import get_headers_normal
-from cellpy.readers.core import (
-    Cell,
-    FileID,
-    check64bit,
-    humanize_bytes,
-    xldate_as_datetime,
-)
-from cellpy.readers.instruments.base import BaseLoader
-
-DEBUG_MODE = prms.Reader.diagnostics
-ALLOW_MULTI_TEST_FILE = False
-
-# Select odbc module
-ODBC = prms._odbc
-SEARCH_FOR_ODBC_DRIVERS = prms._search_for_odbc_driver
-
-use_subprocess = prms.Instruments.Arbin.use_subprocess
-detect_subprocess_need = prms.Instruments.Arbin.detect_subprocess_need
-
-# Finding out some stuff about the platform
-is_posix = False
-is_macos = False
-if os.name == "posix":
-    is_posix = True
-current_platform = platform.system()
-if current_platform == "Darwin":
-    is_macos = True
-
-if DEBUG_MODE:
-    logging.debug("DEBUG_MODE")
-    logging.debug(f"ODBC: {ODBC}")
-    logging.debug(f"SEARCH_FOR_ODBC_DRIVERS: {SEARCH_FOR_ODBC_DRIVERS}")
-    logging.debug(f"use_subprocess: {use_subprocess}")
-    logging.debug(f"detect_subprocess_need: {detect_subprocess_need}")
-    logging.debug(f"current_platform: {current_platform}")
+from typing import Union
+import urllib
+from pathlib import Path
 
-if detect_subprocess_need:
-    logging.debug("detect_subprocess_need is True: checking versions")
-    python_version, os_version = platform.architecture()
-    if python_version == "64bit" and prms.Instruments.Arbin.office_version == "32bit":
-        logging.debug(
-            "python 64bit and office 32bit -> " "setting use_subprocess to True"
+import click
+import pkg_resources
+from github import Github
+
+import cellpy._version
+from cellpy.exceptions import ConfigFileNotWritten
+from cellpy.parameters import prmreader
+from cellpy.parameters.internal_settings import OTHERPATHS
+from cellpy.internals.core import OtherPath
+
+VERSION = cellpy._version.__version__
+REPO = "jepegit/cellpy"
+USER = "jepegit"
+GITHUB_PWD_VAR_NAME = "GD_PWD"
+
+
+def save_prm_file(prm_filename):
+    """saves (writes) the prms to file"""
+    prmreader._write_prm_file(prm_filename)
+
+
+def get_package_prm_dir():
+    """gets the folder where the cellpy package lives"""
+    prm_dir = pkg_resources.resource_filename("cellpy", "parameters")
+    return pathlib.Path(prm_dir)
+
+
+def get_default_config_file_path(init_filename=None):
+    """gets the path to the default config-file"""
+    prm_dir = get_package_prm_dir()
+    if not init_filename:
+        init_filename = prmreader.DEFAULT_FILENAME
+    src = prm_dir / init_filename
+    return src
+
+
+def get_dst_file(user_dir, init_filename):
+    user_dir = pathlib.Path(user_dir)
+    dst_file = user_dir / init_filename
+    return dst_file
+
+
+def check_if_needed_modules_exists():
+    pass
+
+
+def modify_config_file():
+    pass
+
+
+def create_cellpy_folders():
+    pass
+
+
+@click.group("cellpy")
+def cli():
+    pass
+
+
+# ----------------------- setup --------------------------------------
+@click.command()
+@click.option(
+    "--interactive",
+    "-i",
+    is_flag=True,
+    default=False,
+    help="Allows you to specify div. folders and setting.",
+)
+@click.option(
+    "--not-relative",
+    "-nr",
+    is_flag=True,
+    default=False,
+    help="If root-dir is given, put it directly in the root (/) folder"
+    " i.e. don't put it in your home directory. Defaults to False. Remark"
+    " that if you specifically write a path name instead of selecting the"
+    " suggested default, the path you write will be used as is.",
+)
+@click.option(
+    "--dry-run",
+    "-dr",
+    is_flag=True,
+    default=False,
+    help="Run setup in dry mode (only print - do not execute). This is"
+    " typically used when developing and testing cellpy. Defaults to"
+    " False.",
+)
+@click.option(
+    "--reset",
+    "-r",
+    is_flag=True,
+    default=False,
+    help="Do not suggest path defaults based on your current configuration-file",
+)
+@click.option(
+    "--root-dir",
+    "-d",
+    default=None,
+    type=click.Path(),
+    help="Use custom root dir. If not given, your home directory"
+    " will be used as the top level where cellpy-folders"
+    " will be put. The folder path must follow"
+    " directly after this option (if used). Example:\n"
+    " $ cellpy setup -d 'MyDir'",
+)
+@click.option(
+    "--folder-name",
+    "-n",
+    default=None,
+    type=click.Path(),
+    help="",
+)
+@click.option(
+    "--test_user", "-t", default=None, help="Fake name for fake user (for testing)"
+)
+def setup(interactive, not_relative, dry_run, reset, root_dir, folder_name, test_user):
+    """This will help you to set up cellpy."""
+
+    click.echo("[cellpy] (setup)")
+    click.echo(f"[cellpy] root-dir: {root_dir}")
+
+    # generate variables
+    init_filename = prmreader.create_custom_init_filename()
+    user_dir, dst_file = prmreader.get_user_dir_and_dst(init_filename)
+
+    if dry_run:
+        click.echo("Create custom init filename and get user_dir and destination")
+        click.echo(f"Got the following parameters:")
+        click.echo(f" - init_filename: {init_filename}")
+        click.echo(f" - user_dir: {user_dir}")
+        click.echo(f" - dst_file: {dst_file}")
+        click.echo(f" - not_relative: {not_relative}")
+
+    if root_dir and not interactive:
+        click.echo("[cellpy] custom root-dir can only be used in interactive mode")
+        click.echo("[cellpy] -> setting interactive mode")
+        interactive = True
+
+    if not root_dir:
+        root_dir = user_dir
+        # root_dir = pathlib.Path(os.getcwd())
+    root_dir = pathlib.Path(root_dir)
+
+    if dry_run:
+        click.echo(f" - root_dir: {root_dir}")
+
+    if test_user:
+        click.echo(f"[cellpy] (setup) DEV-MODE test_user: {test_user}")
+        init_filename = prmreader.create_custom_init_filename(test_user)
+        user_dir = root_dir
+        dst_file = get_dst_file(user_dir, init_filename)
+        click.echo(f"[cellpy] (setup) DEV-MODE user_dir: {user_dir}")
+        click.echo(f"[cellpy] (setup) DEV-MODE dst_file: {dst_file}")
+
+    if not pathlib.Path(dst_file).is_file():
+        click.echo(f"[cellpy] {dst_file} not found -> I will make one for you!")
+        reset = True
+
+    if interactive:
+        click.echo(" interactive mode ".center(80, "-"))
+        _update_paths(
+            custom_dir=root_dir,
+            relative_home=not not_relative,
+            default_dir=folder_name,
+            dry_run=dry_run,
+            reset=reset,
         )
-        use_subprocess = True
+        _write_config_file(user_dir, dst_file, init_filename, dry_run)
+        _check(dry_run=dry_run)
 
-if use_subprocess and not is_posix:
-    # The windows users most likely have a strange custom path to mdbtools etc.
-    logging.debug(
-        "using subprocess (most lilkely mdbtools) " "on non-posix (most likely windows)"
-    )
-    if not prms.Instruments.Arbin.sub_process_path:
-        sub_process_path = str(prms._sub_process_path)
     else:
-        sub_process_path = str(prms.Instruments.Arbin.sub_process_path)
+        if reset:
+            _update_paths(
+                user_dir,
+                False,
+                default_dir=folder_name,
+                dry_run=dry_run,
+                reset=True,
+                silent=True,
+            )
+        _write_config_file(user_dir, dst_file, init_filename, dry_run)
+        _check(dry_run=dry_run)
 
-if is_posix:
-    sub_process_path = "mdb-export"
 
-try:
-    driver_dll = prms.Instruments.Arbin.odbc_driver
-except AttributeError:
-    driver_dll = None
+def _update_paths(
+    custom_dir=None,
+    relative_home=True,
+    reset=False,
+    dry_run=False,
+    default_dir=None,
+    silent=False,
+):
+    # please, refactor me :-(
+
+    h = prmreader.get_user_dir()
+
+    if default_dir is None:
+        default_dir = "cellpy_data"
+
+    if dry_run:
+        click.echo(f" - default_dir: {default_dir}")
+        click.echo(f" - custom_dir: {custom_dir}")
+        click.echo(f" - retalive_home: {relative_home}")
+
+    if custom_dir:
+        reset = True
+        if relative_home:
+            h = h / custom_dir
+        if not custom_dir.parts[-1] == default_dir:
+            h = h / default_dir
+
+    if not reset:
+        outdatadir = pathlib.Path(prmreader.prms.Paths.outdatadir)
+        rawdatadir = OtherPath(prmreader.prms.Paths.rawdatadir)
+        cellpydatadir = OtherPath(prmreader.prms.Paths.cellpydatadir)
+        filelogdir = pathlib.Path(prmreader.prms.Paths.filelogdir)
+        examplesdir = pathlib.Path(prmreader.prms.Paths.examplesdir)
+        db_path = pathlib.Path(prmreader.prms.Paths.db_path)
+        db_filename = prmreader.prms.Paths.db_filename
+        notebookdir = pathlib.Path(prmreader.prms.Paths.notebookdir)
+        batchfiledir = pathlib.Path(prmreader.prms.Paths.batchfiledir)
+        templatedir = pathlib.Path(prmreader.prms.Paths.templatedir)
+        instrumentdir = pathlib.Path(prmreader.prms.Paths.instrumentsdir)
+    else:
+        outdatadir = "out"
+        rawdatadir = "raw"
+        cellpydatadir = "cellpyfiles"
+        filelogdir = "logs"
+        examplesdir = "examples"
+        db_path = "db"
+        db_filename = "cellpy_db.xlsx"
+        notebookdir = "notebooks"
+        batchfiledir = "batchfiles"
+        templatedir = "templates"
+        instrumentdir = "instruments"
+
+    outdatadir = h / outdatadir
+    rawdatadir = h / rawdatadir
+    cellpydatadir = h / cellpydatadir
+    filelogdir = h / filelogdir
+    examplesdir = h / examplesdir
+    db_path = h / db_path
+    notebookdir = h / notebookdir
+    batchfiledir = h / batchfiledir
+    templatedir = h / templatedir
+    instrumentdir = h / instrumentdir
+
+    if dry_run:
+        click.echo(f" - base (h): {h}")
+
+    if not silent:
+        outdatadir = _ask_about_path(
+            "where to output processed data and results", outdatadir
+        )
+        rawdatadir = _ask_about_otherpath("where your raw data are located", rawdatadir)
+        cellpydatadir = _ask_about_otherpath("where to put cellpy-files", cellpydatadir)
+        filelogdir = _ask_about_path("where to dump the log-files", filelogdir)
+        examplesdir = _ask_about_path(
+            "where to download cellpy examples and tests", examplesdir
+        )
+        db_path = _ask_about_path("what folder your db file lives in", db_path)
+        db_filename = _ask_about_name("the name of your db-file", db_filename)
+        notebookdir = _ask_about_path(
+            "where to put your jupyter notebooks", notebookdir
+        )
+        batchfiledir = _ask_about_path("where to put your batch files", batchfiledir)
+        templatedir = _ask_about_path("where to put your batch files", templatedir)
+        instrumentdir = _ask_about_path("where to put your batch files", instrumentdir)
+
+    # update folders based on suggestions
+    for d in [
+        outdatadir,
+        rawdatadir,
+        cellpydatadir,
+        filelogdir,
+        examplesdir,
+        notebookdir,
+        db_path,
+        batchfiledir,
+        templatedir,
+        instrumentdir,
+    ]:
+        if not dry_run:
+            _create_dir(d)
+        else:
+            click.echo(f"dry run (so I did not create {d})")
+
+    # update config-file based on suggestions
+    prmreader.prms.Paths.outdatadir = str(outdatadir)
+    prmreader.prms.Paths.rawdatadir = str(rawdatadir)
+    prmreader.prms.Paths.cellpydatadir = str(cellpydatadir)
+    prmreader.prms.Paths.filelogdir = str(filelogdir)
+    prmreader.prms.Paths.examplesdir = str(examplesdir)
+    prmreader.prms.Paths.db_path = str(db_path)
+    prmreader.prms.Paths.db_filename = str(db_filename)
+    prmreader.prms.Paths.notebookdir = str(notebookdir)
+    prmreader.prms.Paths.batchfiledir = str(batchfiledir)
+    prmreader.prms.Paths.templatedir = str(templatedir)
+    prmreader.prms.Paths.instrumentdir = str(instrumentdir)
+
+
+def _ask_about_path(q, p):
+    click.echo(f"\n[cellpy] (setup) input {q}")
+    click.echo(f"[cellpy] (setup) current: {p}")
+    new_path = input("[cellpy] (setup) [KEEP/new value] >>> ").strip()
+    if not new_path:
+        new_path = p
+    return pathlib.Path(new_path)
+
+
+def _ask_about_otherpath(q, p):
+    click.echo(f"\n[cellpy] (setup) input {q}")
+    click.echo(f"[cellpy] (setup) current: {p}")
+    new_path = input("[cellpy] (setup) [KEEP/new value] >>> ").strip()
+    if not new_path:
+        new_path = p
+    return OtherPath(new_path)
+
+
+def _ask_about_name(q, n):
+    click.echo(f"\n[cellpy] (setup) input {q}")
+    click.echo(f"[cellpy] (setup) current: {n}")
+    new_name = input("[cellpy] (setup) [KEEP/new value] >>> ").strip()
+    if not new_name:
+        new_name = n
+    return new_name
+
+
+def _create_dir(path, confirm=True, parents=True, exist_ok=True):
+    if isinstance(path, OtherPath):
+        if path.is_external:
+            return path
+    o = path.resolve()
+    if not o.is_dir():
+        o_parent = o.parent
+        create_dir = True
+        if confirm:
+            if not o_parent.is_dir():
+                create_dir = input(
+                    f"\n[cellpy] (setup) {o_parent} does not exist. Create it [y]/n ?"
+                )
+                if not create_dir:
+                    create_dir = True
+                elif create_dir in ["y", "Y"]:
+                    create_dir = True
+                else:
+                    create_dir = False
 
-use_ado = False
+        if create_dir:
+            try:
+                o.mkdir(parents=parents, exist_ok=exist_ok)
+                click.echo(f"[cellpy] (setup) Created {o}")
+            except FileExistsError:
+                click.echo(f"[cellpy] (setup) {o} already exists.")
+            except FileNotFoundError:
+                click.echo(f"[cellpy] (setup) {o} not available.")
+            except Exception as e:
+                click.echo(f"[cellpy] (setup) WARNING! Could not create {o}.")
+                logging.debug(e)
+                click.echo(f"[cellpy] (setup) ...continuing anyway.")
+        else:
+            click.echo(f"[cellpy] (setup) Could not create {o}")
+    return o
 
-if ODBC == "ado":
-    use_ado = True
-    logging.debug("Trying to use adodbapi as ado loader")
+
+def _check_import_cellpy():
     try:
-        import adodbapi as dbloader  # http://adodbapi.sourceforge.net/
-    except ImportError:
-        use_ado = False
+        import cellpy
+        from cellpy import log
+        from cellpy.readers import cellreader
+
+        return True
+    except:
+        return False
+
+
+def _check_import_pyodbc():
+    import platform
+
+    from cellpy.parameters import prms
+
+    ODBC = prms._odbc
+    SEARCH_FOR_ODBC_DRIVERS = prms._search_for_odbc_driver
+
+    use_subprocess = prms.Instruments.Arbin.use_subprocess
+    detect_subprocess_need = prms.Instruments.Arbin.detect_subprocess_need
+    click.echo(f" reading prms")
+    click.echo(f" - ODBC: {ODBC}")
+    click.echo(f" - SEARCH_FOR_ODBC_DRIVERS: {SEARCH_FOR_ODBC_DRIVERS}")
+    click.echo(f" - use_subprocess: {use_subprocess}")
+    click.echo(f" - detect_subprocess_need: {detect_subprocess_need}")
+    click.echo(f" - stated office version: {prms.Instruments.Arbin.office_version}")
+
+    click.echo(" checking system")
+    is_posix = False
+    is_macos = False
+    if os.name == "posix":
+        is_posix = True
+        click.echo(f" - running on posix")
+    current_platform = platform.system()
+    if current_platform == "Darwin":
+        is_macos = True
+        click.echo(f" - running on a mac")
+
+    python_version, os_version = platform.architecture()
+    click.echo(f" - python version: {python_version}")
+    click.echo(f" - os version: {os_version}")
+
+    if not is_posix:
+        if not prms.Instruments.Arbin.sub_process_path:
+            sub_process_path = str(prms._sub_process_path)
+        else:
+            sub_process_path = str(prms.Instruments.Arbin.sub_process_path)
+        click.echo(f" stated path to sub-process: {sub_process_path}")
+        if not os.path.isfile(sub_process_path):
+            click.echo(f" - OBS! missing")
+
+    if is_posix:
+        click.echo(" checking existence of mdb-export")
+        sub_process_path = "mdb-export"
+        from subprocess import PIPE, run
+
+        command = ["command", "-v", sub_process_path]
 
-if not use_ado:
-    if ODBC == "pyodbc":
         try:
-            import pyodbc as dbloader
-        except ImportError:
-            warnings.warn("COULD NOT LOAD DBLOADER!", ImportWarning)
-            dbloader = None
+            result = run(command, stdout=PIPE, stderr=PIPE, universal_newlines=True)
+            if result.returncode == 0:
+                click.echo(f" - found it: {result.stdout}")
+            else:
+                click.echo(f" - failed finding it")
+
+            if is_macos:
+                driver = "/usr/local/lib/libmdbodbc.dylib"
+                click.echo(f" looks like you are on a mac (driver set to\n {driver})")
+                if not os.path.isfile(driver):
+                    click.echo(" - but cannot find it!")
+                    return False
+            return True
+
+        except AssertionError:
+            click.echo(" - not found")
+            return False
+
+    # not posix - checking for odbc drivers
+    # 1) checking if you have defined one
+    try:
+        driver = prms.Instruments.Arbin.odbc_driver
+        if not driver:
+            raise AttributeError
+        click.echo("You have defined an odbc driver in your conifg file")
+        click.echo(f"driver: {driver}")
+    except AttributeError:
+        click.echo("FYI: you have not defined any odbc_driver(s)")
+        click.echo(
+            "(The name of the driver from the configuration file is "
+            "used as a backup when cellpy cannot locate a driver by itself)"
+        )
+
+    use_ado = False
 
-    elif ODBC == "pypyodbc":
+    if ODBC == "ado":
+        use_ado = True
+        click.echo(" you stated that you prefer the ado loader")
+        click.echo(" checking if adodbapi is installed")
         try:
-            import pypyodbc as dbloader
+            import adodbapi as dbloader
         except ImportError:
-            warnings.warn("COULD NOT LOAD DBLOADER!", ImportWarning)
-            dbloader = None
+            use_ado = False
+            click.echo(" Failed! Try setting pyodbc as your loader or install")
+            click.echo(" adodbapi (http://adodbapi.sourceforge.net/)")
+
+    if not use_ado:
+        if ODBC == "pyodbc":
+            click.echo(" you stated that you prefer the pyodbc loader")
+            try:
+                import pyodbc as dbloader
+            except ImportError:
+                click.echo(" Failed! Could not import it.")
+                click.echo(" Try 'pip install pyodbc'")
+                dbloader = None
 
-if DEBUG_MODE:
-    logging.debug(f"dbloader: {dbloader}")
+        elif ODBC == "pypyodbc":
+            click.echo(" you stated that you prefer the pypyodbc loader")
+            try:
+                import pypyodbc as dbloader
+            except ImportError:
+                click.echo(" Failed! Could not import it.")
+                click.echo(" try 'pip install pypyodbc'")
+                click.echo(" or set pyodbc as your loader in your prm file")
+                click.echo(" (and install it)")
+                dbloader = None
 
-# The columns to choose if minimum selection is selected
-MINIMUM_SELECTION = [
-    "Data_Point",
-    "Test_Time",
-    "Step_Time",
-    "DateTime",
-    "Step_Index",
-    "Cycle_Index",
-    "Current",
-    "Voltage",
-    "Charge_Capacity",
-    "Discharge_Capacity",
-    "Internal_Resistance",
-]
-
-# Names of the tables in the .res db that is used by cellpy
-TABLE_NAMES = {
-    "normal": "Channel_Normal_Table",
-    "global": "Global_Table",
-    "statistic": "Channel_Statistic_Table",
-}
-
-
-normal_headers_renaming_dict = {
-    "aci_phase_angle_txt": "ACI_Phase_Angle",
-    "ref_aci_phase_angle_txt": "Reference_ACI_Phase_Angle",
-    "ac_impedance_txt": "AC_Impedance",
-    "ref_ac_impedance_txt": "Reference_AC_Impedance",
-    "charge_capacity_txt": "Charge_Capacity",
-    "charge_energy_txt": "Charge_Energy",
-    "current_txt": "Current",
-    "cycle_index_txt": "Cycle_Index",
-    "data_point_txt": "Data_Point",
-    "datetime_txt": "DateTime",
-    "discharge_capacity_txt": "Discharge_Capacity",
-    "discharge_energy_txt": "Discharge_Energy",
-    "internal_resistance_txt": "Internal_Resistance",
-    "is_fc_data_txt": "Is_FC_Data",
-    "step_index_txt": "Step_Index",
-    "sub_step_index_txt": "Sub_Step_Index",  # new
-    "step_time_txt": "Step_Time",
-    "sub_step_time_txt": "Sub_Step_Time",  # new
-    "test_id_txt": "Test_ID",
-    "test_time_txt": "Test_Time",
-    "voltage_txt": "Voltage",
-    "ref_voltage_txt": "Reference_Voltage",  # new
-    "dv_dt_txt": "dV/dt",
-    "frequency_txt": "Frequency",  # new
-    "amplitude_txt": "Amplitude",  # new
-}
-
-
-class DataLoader(BaseLoader):
-    """Class for loading arbin-data from res-files."""
-
-    def __init__(self, *args, **kwargs):
-        """initiates the ArbinLoader class"""
-        # could use __init__(self, cellpydata_object) and
-        # set self.logger = cellpydata_object.logger etc.
-        # then remember to include that as prm in "out of class" functions
-        # self.prms = prms
-        self.logger = logging.getLogger(__name__)
-        # use the following prm to limit to loading only
-        # one cycle or from cycle>x to cycle<x+n
-        # prms.Reader.limit_loaded_cycles = [cycle from, cycle to]
-
-        self.headers_normal = get_headers_normal()
-        self.headers_global = self.get_headers_global()
-        self.current_chunk = 0  # use this to set chunks to load
-
-    @staticmethod
-    def get_raw_units():
-        raw_units = dict()
-        raw_units["current"] = 1.0  # A
-        raw_units["charge"] = 1.0  # Ah
-        raw_units["mass"] = 1.0  # g
-        raw_units["voltage"] = 1.0  # V
-        return raw_units
-
-    @staticmethod
-    def get_headers_global():
-        """Defines the so-called global column headings for Arbin .res-files"""
-        headers = dict()
-        # - global column headings (specific for Arbin)
-        headers["applications_path_txt"] = "Applications_Path"
-        headers["channel_index_txt"] = "Channel_Index"
-        headers["channel_number_txt"] = "Channel_Number"
-        headers["channel_type_txt"] = "Channel_Type"
-        headers["comments_txt"] = "Comments"
-        headers["creator_txt"] = "Creator"
-        headers["daq_index_txt"] = "DAQ_Index"
-        headers["item_id_txt"] = "Item_ID"
-        headers["log_aux_data_flag_txt"] = "Log_Aux_Data_Flag"
-        headers["log_chanstat_data_flag_txt"] = "Log_ChanStat_Data_Flag"
-        headers["log_event_data_flag_txt"] = "Log_Event_Data_Flag"
-        headers["log_smart_battery_data_flag_txt"] = "Log_Smart_Battery_Data_Flag"
-        headers["mapped_aux_conc_cnumber_txt"] = "Mapped_Aux_Conc_CNumber"
-        headers["mapped_aux_di_cnumber_txt"] = "Mapped_Aux_DI_CNumber"
-        headers["mapped_aux_do_cnumber_txt"] = "Mapped_Aux_DO_CNumber"
-        headers["mapped_aux_flow_rate_cnumber_txt"] = "Mapped_Aux_Flow_Rate_CNumber"
-        headers["mapped_aux_ph_number_txt"] = "Mapped_Aux_PH_Number"
-        headers["mapped_aux_pressure_number_txt"] = "Mapped_Aux_Pressure_Number"
-        headers["mapped_aux_temperature_number_txt"] = "Mapped_Aux_Temperature_Number"
-        headers["mapped_aux_voltage_number_txt"] = "Mapped_Aux_Voltage_Number"
-        headers[
-            "schedule_file_name_txt"
-        ] = "Schedule_File_Name"  # KEEP FOR CELLPY FILE FORMAT
-        headers["start_datetime_txt"] = "Start_DateTime"
-        headers["test_id_txt"] = "Test_ID"  # KEEP FOR CELLPY FILE FORMAT
-        headers["test_name_txt"] = "Test_Name"  # KEEP FOR CELLPY FILE FORMAT
-        return headers
-
-    @staticmethod
-    def get_raw_limits():
-        raw_limits = dict()
-        raw_limits["current_hard"] = 0.000_000_000_000_1
-        raw_limits["current_soft"] = 0.000_01
-        raw_limits["stable_current_hard"] = 2.0
-        raw_limits["stable_current_soft"] = 4.0
-        raw_limits["stable_voltage_hard"] = 2.0
-        raw_limits["stable_voltage_soft"] = 4.0
-        raw_limits["stable_charge_hard"] = 0.001
-        raw_limits["stable_charge_soft"] = 5.0
-        raw_limits["ir_change"] = 0.00001
-        return raw_limits
-
-    def _get_res_connector(self, temp_filename):
-
-        if use_ado:
-            is64bit_python = check64bit(current_system="python")
-            if is64bit_python:
-                constr = (
-                    "Provider=Microsoft.ACE.OLEDB.12.0; Data Source=%s" % temp_filename
-                )
-            else:
-                constr = (
-                    "Provider=Microsoft.Jet.OLEDB.4.0; Data Source=%s" % temp_filename
-                )
-            return constr
+    click.echo(" searching for odbc drivers")
+    try:
+        drivers = [
+            driver
+            for driver in dbloader.drivers()
+            if "Microsoft Access Driver" in driver
+        ]
+        click.echo(f"Found these: {drivers}")
+        driver = drivers[0]
+        click.echo(f"odbc driver: {driver}")
+        return True
+
+    except IndexError as e:
+        logging.debug("Unfortunately, it seems the list of drivers is emtpy.")
+        click.echo(
+            "\nCould not find any odbc-drivers suitable for .res-type files. "
+            "Check out the homepage of pydobc for info on installing drivers"
+        )
+        click.echo(
+            "One solution that might work is downloading "
+            "the Microsoft Access database engine "
+            "(in correct bytes (32 or 64)) "
+            "from:\n"
+            "https://www.microsoft.com/en-us/download/details.aspx?id=13255"
+        )
+        click.echo("Or install mdbtools and set it up (check the cellpy docs for help)")
+        click.echo("\n")
+        return False
 
-        if SEARCH_FOR_ODBC_DRIVERS:
-            logging.debug("Searching for odbc drivers")
-            try:
-                drivers = [
-                    driver
-                    for driver in dbloader.drivers()
-                    if "Microsoft Access Driver" in driver
-                ]
-                logging.debug(f"Found these: {drivers}")
-                driver = drivers[0]
 
-            except IndexError as e:
+def _check_config_file():
+    prm_file_name = _configloc()
+    prm_dict = prmreader._read_prm_file_without_updating(prm_file_name)
+    try:
+        prm_paths = prm_dict["Paths"]
+        required_dirs = [
+            "cellpydatadir",
+            "examplesdir",
+            "filelogdir",
+            "notebookdir",
+            "outdatadir",
+            "rawdatadir",
+            "batchfiledir",
+            "templatedir",
+            "db_path",
+        ]
+        missing = 0
+        for k in required_dirs:
+            value = prm_paths.get(k, None)
+            click.echo(f"{k}: {value}")
+            # splitting this into two if-statements to make it easier to debug if OtherPath changes
+            if value in OTHERPATHS:
                 logging.debug(
-                    "Unfortunately, it seems the " "list of drivers is emtpy."
+                    "skipping check for external rawdatadir and cellpydatadir (for now)"
                 )
-                logging.debug("Use driver-name from config (if existing).")
-                driver = driver_dll
-                if is_macos:
-                    driver = "/usr/local/lib/libmdbodbc.dylib"
-                else:
-                    if not driver:
-                        print(
-                            "\nCould not find any odbc-drivers suitable "
-                            "for .res-type files. "
-                            "Check out the homepage of pydobc for info on "
-                            "installing drivers"
-                        )
-                        print(
-                            "One solution that might work is downloading "
-                            "the Microsoft Access database engine (in correct"
-                            " bytes (32 or 64)) "
-                            "from:\n"
-                            "https://www.microsoft.com/en-us/download/"
-                            "details.aspx?id=13255"
-                        )
-                        print(
-                            "Or install mdbtools and set it up "
-                            "(check the cellpy docs for help)"
-                        )
-                        print("\n")
-                    else:
-                        logging.debug("Using driver dll from config file")
-                        logging.debug(f"driver dll: {driver}")
+                if not OtherPath(
+                    value
+                ).is_dir():  # Assuming OtherPath returns True if it is external.
+                    missing += 1
+                    click.echo("COULD NOT CONNECT!")
+                    click.echo(f"({value} is not a directory)")
+            elif value and not pathlib.Path(value).is_dir():
+                missing += 1
+                click.echo("COULD NOT CONNECT!")
+                click.echo(f"({value} is not a directory)")
+            if not value:
+                missing += 1
+                click.echo("MISSING")
+
+        value = prm_paths.get("db_filename", None)
+        click.echo(f"db_filename: {value}")
+        if not value:
+            missing += 1
+            click.echo("MISSING")
+
+        if missing:
+            return False
+        else:
+            return True
+
+    except Exception as e:
+        click.echo("Following error occurred:")
+        click.echo(e)
+        return False
+
+
+def _check(dry_run=False):
+    click.echo(" checking ".center(80, "="))
+    if dry_run:
+        click.echo("*** dry-run: skipping the test")
+        return
+    failed_checks = 0
+    number_of_checks = 0
+
+    def sub_check(check_type, check_func):
+        failed = 0
+        click.echo(f"[cellpy] * - Checking {check_type}")
+        if check_func():
+            click.echo(f"[cellpy] -> succeeded!")
+        else:
+            click.echo("f[cellpy] -> failed!!!!")
+            failed = 1
+        click.echo(80 * "-")
+        return failed
+
+    check_types = ["cellpy imports", "importing pyodbc", "configuration (prm) file"]
+    check_funcs = [_check_import_cellpy, _check_import_pyodbc, _check_config_file]
+
+    for ct, cf in zip(check_types, check_funcs):
+        try:
+            failed_checks += sub_check(ct, cf)
+        except Exception as e:
+            click.echo(f"[cellpy] check raised an exception ({e})")
+        number_of_checks += 1
+    succeeded_checks = number_of_checks - failed_checks
+    if failed_checks > 0:
+        click.echo(f"[cellpy] OH NO!!! You (or I) failed!")
+        click.echo(f"[cellpy] Failed {failed_checks} out of {number_of_checks} checks.")
+    else:
+        click.echo(
+            f"[cellpy] Succeeded {succeeded_checks} out of {number_of_checks} checks."
+        )
+    click.echo(80 * "=")
+
 
-            self.logger.debug(f"odbc constr: {driver}")
+def _write_config_file(user_dir, dst_file, init_filename, dry_run):
+    click.echo(" update configuration ".center(80, "-"))
+    click.echo("[cellpy] (setup) Writing configurations to user directory:")
+    click.echo(f"\n         {user_dir}\n")
 
+    if os.path.isfile(dst_file):
+        click.echo("[cellpy] (setup) File already exists!")
+        click.echo("[cellpy] (setup) Keeping most of the old configuration parameters")
+    try:
+        if dry_run:
+            click.echo(
+                f"*** dry-run: skipping actual saving of {dst_file} ***", color="red"
+            )
         else:
-            is64bit_python = check64bit(current_system="python")
-            if is64bit_python:
-                driver = "{Microsoft Access Driver (*.mdb, *.accdb)}"
+            click.echo(f"[cellpy] (setup) Saving file ({dst_file})")
+            save_prm_file(dst_file)
+
+    except ConfigFileNotWritten:
+        click.echo("[cellpy] (setup) Something went wrong! Could not write the file")
+        click.echo(
+            "[cellpy] (setup) Trying to write a file"
+            + f"called {prmreader.DEFAULT_FILENAME} instead"
+        )
+
+        try:
+            user_dir, dst_file = prmreader.get_user_dir_and_dst(init_filename)
+            if dry_run:
+                click.echo(
+                    f"*** dry-run: skipping actual saving of {dst_file} ***",
+                    color="red",
+                )
             else:
-                driver = "Microsoft Access Driver (*.mdb)"
-            self.logger.debug("odbc constr: {}".format(driver))
-        constr = "Driver=%s;Dbq=%s" % (driver, temp_filename)
-
-        logging.debug(f"constr: {constr}")
-
-        return constr
-
-    def _clean_up_loadres(self, cur, conn, filename):
-        if cur is not None:
-            cur.close()  # adodbapi
-        if conn is not None:
-            conn.close()  # adodbapi
-        if os.path.isfile(filename):
+                save_prm_file(dst_file)
+
+        except ConfigFileNotWritten:
+            _txt = "[cellpy] (setup) No, that did not work either.\n"
+            _txt += "[cellpy] (setup) Well, guess you have to talk to the developers."
+            click.echo(_txt)
+    else:
+        click.echo(f"[cellpy] (setup) Configuration file written!")
+        click.echo(
+            f"[cellpy] (setup) OK! Now you can edit it. For example by "
+            f"issuing \n\n         [your-favourite-editor] {init_filename}\n"
+        )
+
+
+# ----------------------- edit ---------------------------------------
+@click.command()
+@click.option(
+    "--default-editor",
+    "-e",
+    default=None,
+    type=str,
+    help="try to use this editor instead (e.g. notepad.exe)",
+)
+def edit(default_editor):
+    """Edit your cellpy config file."""
+
+    config_file = _configloc()
+    if config_file:
+        config_file_str = str(config_file.resolve())
+
+        if default_editor is not None:
+            args = [default_editor, config_file_str]
+            click.echo(f"[cellpy] (edit) Calling '{default_editor}'")
             try:
-                os.remove(filename)
-            except WindowsError as e:
-                self.logger.warning("could not remove tmp-file\n%s %s" % (filename, e))
-
-    def _post_process(self, data):
-        fix_datetime = True
-        set_index = True
-        rename_headers = True  # could safely set this to false for now, but...
-
-        # TODO:  insert post-processing and div tests here
-        #    - check dtypes
-
-        # Remark that we also set index during saving the file to hdf5 if
-        #   it is not set.
-
-        if rename_headers:
-            columns = {}
-            for key in self.headers_normal:
-                old_header = normal_headers_renaming_dict[key]
-                new_header = self.headers_normal[key]
-                columns[old_header] = new_header
-
-            data.raw.rename(index=str, columns=columns)
-
-        if fix_datetime:
-            h_datetime = self.headers_normal.datetime_txt
-            logging.debug("converting to datetime format")
-            data.raw[h_datetime] = data.raw[h_datetime].apply(
-                xldate_as_datetime, option="to_datetime"
-            )
+                subprocess.call(args)
+            except:
+                click.echo(f"[cellpy] (edit) Failed!")
+                click.echo(
+                    "[cellpy] (edit) Try 'cellpy edit -e notepad.exe' if you are on Windows"
+                )
 
-            h_datetime = h_datetime
-            if h_datetime in data.summary:
-                data.summary[h_datetime] = data.summary[h_datetime].apply(
-                    xldate_as_datetime, option="to_datetime"
+        if default_editor is None:
+            try:
+                import editor
+
+                editor.edit(filename=config_file_str)
+            except ImportError:
+                click.echo(f"[cellpy] (edit) Failed!")
+                click.echo(
+                    f"[cellpy] (edit) Searching for editors uses the python-editor package"
+                )
+                click.echo(f"[cellpy] (edit) Possible fixes:")
+                click.echo(
+                    f"[cellpy] (edit) - provide a default editor "
+                    f"using the -e option (e.g. cellpy edit -e notepad.exe)"
+                )
+                click.echo(
+                    f"[cellpy] (edit) - install teh python-editor package "
+                    f"(pip install python-editor)"
                 )
 
-        if set_index:
-            hdr_data_point = self.headers_normal.data_point_txt
-            if data.raw.index.name != hdr_data_point:
-                data.raw = data.raw.set_index(hdr_data_point, drop=False)
-
-        return data
-
-    def _inspect(self, run_data):
-        """Inspect the file -> reports to log (debug)"""
-
-        if not any([DEBUG_MODE]):
-            return run_data
-
-        if DEBUG_MODE:
-            checked_rundata = []
-            for data in run_data:
-                new_cols = data.raw.columns
-                for col in self.headers_normal:
-                    if col not in new_cols:
-                        logging.debug(f"Missing col: {col}")
-                        # data.raw[col] = np.nan
-                checked_rundata.append(data)
-            return checked_rundata
-
-    def _iterdump(self, file_name, headers=None):
-        """
-        Function for dumping values from a file.
-
-        Should only be used by developers.
-
-        Args:
-            file_name: name of the file
-            headers: list of headers to pick
-                default:
-                ["Discharge_Capacity", "Charge_Capacity"]
-
-        Returns: pandas.DataFrame
-
-        """
-        if headers is None:
-            headers = ["Discharge_Capacity", "Charge_Capacity"]
-
-        step_txt = self.headers_normal.step_index_txt
-        point_txt = self.headers_normal.data_point_txt
-        cycle_txt = self.headers_normal.cycle_index_txt
-
-        self.logger.debug("iterating through file: %s" % file_name)
-        if not os.path.isfile(file_name):
-            print("Missing file_\n   %s" % file_name)
-
-        filesize = os.path.getsize(file_name)
-        hfilesize = humanize_bytes(filesize)
-        txt = "Filesize: %i (%s)" % (filesize, hfilesize)
-        self.logger.info(txt)
-
-        table_name_global = TABLE_NAMES["global"]
-        table_name_stats = TABLE_NAMES["statistic"]
-        table_name_normal = TABLE_NAMES["normal"]
-
-        # creating temporary file and connection
-
-        temp_dir = tempfile.gettempdir()
-        temp_filename = os.path.join(temp_dir, os.path.basename(file_name))
-        shutil.copy2(file_name, temp_dir)
-        constr = self._get_res_connector(temp_filename)
 
-        if use_ado:
-            conn = dbloader.connect(constr)
-        else:
-            conn = dbloader.connect(constr, autocommit=True)
+# ----------------------- info ---------------------------------------
+@click.command()
+@click.option("--version", "-v", is_flag=True, help="Print version information.")
+@click.option(
+    "--configloc", "-l", is_flag=True, help="Print full path to the config file."
+)
+@click.option("--params", "-p", is_flag=True, help="Dump all parameters to screen.")
+@click.option(
+    "--check",
+    "-c",
+    is_flag=True,
+    help="Do a sanity check to see if things" " works as they should.",
+)
+def info(version, configloc, params, check):
+    """This will give you some valuable information about your cellpy."""
+    complete_info = True
+
+    if check:
+        complete_info = False
+        _check()
+
+    if version:
+        complete_info = False
+        _version()
+
+    if configloc:
+        complete_info = False
+        _configloc()
+
+    if params:
+        complete_info = False
+        _dump_params()
+
+    if complete_info:
+        _version()
+        _configloc()
+
+
+# ----------------------- run ----------------------------------------
+@click.command()
+@click.option(
+    "--journal",
+    "-j",
+    is_flag=True,
+    help="Run a batch job defined in the given journal-file",
+)
+@click.option("--key", "-k", is_flag=True, help="Run a batch job defined by batch-name")
+@click.option(
+    "--folder",
+    "-f",
+    is_flag=True,
+    help="Run all batch jobs iteratively in a given folder",
+)
+@click.option(
+    "--cellpy-project",
+    "-p",
+    is_flag=True,
+    help="Use PaperMill to run the notebook(s) within the given project folder "
+    "(will only work properly if the notebooks can be sorted in correct run-order by 'sorted'). "
+    "Warning! since we are using `click` - the NAME will be 'converted' when it is loaded "
+    "(same as print(name) does) - "
+    "so you can't use backslash ('\\') as normal in windows (use either '/' or '\\\\' instead).",
+)
+@click.option("--debug", "-d", is_flag=True, help="Run in debug mode.")
+@click.option("--silent", "-s", is_flag=True, help="Run in silent mode.")
+@click.option("--raw", is_flag=True, help="Force loading raw-file(s).")
+@click.option("--cellpyfile", is_flag=True, help="Force cellpy-file(s).")
+@click.option("--minimal", is_flag=True, help="Minimal processing.")
+@click.option(
+    "--nom-cap",
+    default=None,
+    type=float,
+    help="nominal capacity (used in calculating rates etc)",
+)
+@click.option(
+    "--batch_col",
+    default=None,
+    type=str,
+    help="batch column (if selecting running from db)",
+)
+@click.option(
+    "--project",
+    default=None,
+    type=str,
+    help="name of the project (if selecting running from db)",
+)
+@click.option("--list", "-l", "list_", is_flag=True, help="List batch-files.")
+@click.argument("name", default="NONE")
+def run(
+    journal,
+    key,
+    folder,
+    cellpy_project,
+    debug,
+    silent,
+    raw,
+    cellpyfile,
+    minimal,
+    nom_cap,
+    batch_col,
+    project,
+    list_,
+    name,
+):
+    """Run a cellpy process (batch-job, edit db, ...).
+
+    You can use this to launch specific applications.
+
+    Examples:
+
+        edit your cellpy database
+
+           cellpy run db
+
+        run a batch job described in a journal file
+
+           cellpy run -j my_experiment.json
+
+    """
+    if list_:
+        _run_list(name)
+        return
+
+    if name == "NONE":
+        click.echo(
+            "Usage: cellpy run [OPTIONS] NAME\n"
+            "Try 'cellpy run --help' for help.\n\n"
+            "Error: Missing argument 'NAME'."
+        )
+        sys.exit(-1)
 
-        self.logger.debug("tmp file: %s" % temp_filename)
-        self.logger.debug("constr str: %s" % constr)
+    if debug:
+        click.echo("[cellpy] (run) debug mode on")
+
+    if silent:
+        click.echo("[cellpy] (run) silent mode on")
+
+    click.echo("[cellpy]\n")
+
+    if cellpy_project:
+        _run_project(name)
+
+    elif journal:
+        _run_journal(name, debug, silent, raw, cellpyfile, minimal, nom_cap)
+
+    elif folder:
+        _run_journals(name, debug, silent, raw, cellpyfile, minimal)
+
+    elif key:
+        _run_from_db(
+            name,
+            debug,
+            silent,
+            raw,
+            cellpyfile,
+            minimal,
+            nom_cap,
+            batch_col,
+            project,
+        )
+
+    elif name.lower() == "db":
+        _run_db(debug, silent)
+
+    else:
+        _run(name, debug, silent)
+
+
+def _run_from_db(
+    name,
+    debug,
+    silent,
+    raw,
+    cellpyfile,
+    minimal,
+    nom_cap,
+    batch_col,
+    project,
+):
+    click.echo(
+        f"running from db \nkey={name}, batch_col={batch_col}, project={project}"
+    )
+
+    kwargs = dict()
+    kwargs["name"] = name
+
+    if debug:
+        kwargs["default_log_level"] = "DEBUG"
+    if not minimal:
+        kwargs["export_raw"] = False
+        kwargs["export_cycles"] = False
+        kwargs["export_ica"] = False
+
+    if batch_col is not None:
+        kwargs["batch_col"] = batch_col
+    if project is None:
+        kwargs["project"] = "various"
+    else:
+        kwargs["project"] = project
+
+    click.echo("Warming up ...")
+
+    from cellpy.utils import batch
+
+    click.echo("  - starting batch processing")
+    b = batch.process_batch(
+        force_raw_file=raw,
+        force_cellpy=cellpyfile,
+        nom_cap=nom_cap,
+        backend="matplotlib",
+        **kwargs,
+    )
+
+    if b is not None and not silent:
+        print(b)
+    click.echo("---")
+
+
+def _run_journal(file_name, debug, silent, raw, cellpyfile, minimal, nom_cap):
+    click.echo(f"running journal {file_name}")
+    # click.echo(f" --debug [{debug}]")
+    # click.echo(f" --silent [{silent}]")
+    # click.echo(f" --raw [{raw}]")
+    # click.echo(f" --cellpyfile [{cellpyfile}]")
+    # click.echo(f" --minimal [{minimal}]")
+    # click.echo(f" --nom_cap [{nom_cap}] {type(nom_cap)}")
+
+    kwargs = dict()
+    if debug:
+        kwargs["default_log_level"] = "DEBUG"
+    if not minimal:
+        kwargs["export_raw"] = False
+        kwargs["export_cycles"] = False
+        kwargs["export_ica"] = False
+
+    from cellpy import prms
+    from cellpy.utils import batch
+
+    batchfiledir = pathlib.Path(prms.Paths.batchfiledir)
+    file = pathlib.Path(file_name)
+    if not file.is_file():
+        click.echo(f"file_name={file_name} not found - looking into batchfiledir")
+        if not batchfiledir.is_dir():
+            click.echo("batchfiledir not found - aborting")
+            return
+        file = batchfiledir / file.name
+
+    if not file.is_file():
+        click.echo(f"{file} not found - aborting")
+        return
+
+    b = batch.process_batch(
+        file,
+        force_raw_file=raw,
+        force_cellpy=cellpyfile,
+        nom_cap=nom_cap,
+        backend="matplotlib",
+        **kwargs,
+    )
+    if b is not None and not silent:
+        print(b)
+    click.echo("---")
 
-        # --------- read global-data ------------------------------------
-        self.logger.debug("reading global data table")
-        sql = "select * from %s" % table_name_global
-        global_data_df = pd.read_sql_query(sql, conn)
-        # col_names = list(global_data_df.columns.values)
-        self.logger.debug("sql statement: %s" % sql)
-
-        tests = global_data_df[self.headers_normal.test_id_txt]
-        number_of_sets = len(tests)
-        self.logger.debug("number of datasets: %i" % number_of_sets)
-        self.logger.debug("only selecting first test")
-        test_no = 0
-        self.logger.debug("setting data for test number %i" % test_no)
-        loaded_from = file_name
-        # fid = FileID(file_name)
-        start_datetime = global_data_df[self.headers_global["start_datetime_txt"]][
-            test_no
-        ]
-        test_ID = int(global_data_df[self.headers_normal.test_id_txt][test_no])  # OBS
-        test_name = global_data_df[self.headers_global["test_name_txt"]][test_no]
 
-        # --------- read raw-data (normal-data) -------------------------
-        self.logger.debug("reading raw-data")
+def _run_list(batchfiledir):
+    from cellpy import prms
 
-        columns = ["Data_Point", "Step_Index", "Cycle_Index"]
-        columns.extend(headers)
-        columns_txt = ", ".join(["%s"] * len(columns)) % tuple(columns)
-
-        sql_1 = "select %s " % columns_txt
-        sql_2 = "from %s " % table_name_normal
-        sql_3 = "where %s=%s " % (self.headers_normal.test_id_txt, test_ID)
-        sql_5 = "order by %s" % self.headers_normal.data_point_txt
-        import time
-
-        info_list = []
-        info_header = ["cycle", "row_count", "start_point", "end_point"]
-        info_header.extend(headers)
-        self.logger.info(" ".join(info_header))
-        self.logger.info("-------------------------------------------------")
-
-        for cycle_number in range(1, 2000):
-            t1 = time.time()
-            self.logger.debug("picking cycle %i" % cycle_number)
-            sql_4 = "AND %s=%i " % (cycle_txt, cycle_number)
-            sql = sql_1 + sql_2 + sql_3 + sql_4 + sql_5
-            self.logger.debug("sql statement: %s" % sql)
-            normal_df = pd.read_sql_query(sql, conn)
-            t2 = time.time()
-            dt = t2 - t1
-            self.logger.debug("time: %f" % dt)
-            if normal_df.empty:
-                self.logger.debug("reached the end")
-                break
-            row_count, _ = normal_df.shape
-            start_point = normal_df[point_txt].min()
-            end_point = normal_df[point_txt].max()
-            last = normal_df.iloc[-1, :]
-
-            step_list = [cycle_number, row_count, start_point, end_point]
-            step_list.extend([last[x] for x in headers])
-            info_list.append(step_list)
-
-        self._clean_up_loadres(None, conn, temp_filename)
-        info_dict = pd.DataFrame(info_list, columns=info_header)
-        return info_dict
-
-    def investigate(self, file_name):
-        """Investigate a .res file.
-
-        Args:
-            file_name: name of the file
-
-        Returns: dictionary with div. stats and info.
-
-        """
-        step_txt = self.headers_normal.step_index_txt
-        point_txt = self.headers_normal.data_point_txt
-        cycle_txt = self.headers_normal.cycle_index_txt
-
-        self.logger.debug("investigating file: %s" % file_name)
-        if not os.path.isfile(file_name):
-            print("Missing file_\n   %s" % file_name)
-
-        filesize = os.path.getsize(file_name)
-        hfilesize = humanize_bytes(filesize)
-        txt = "Filesize: %i (%s)" % (filesize, hfilesize)
-        self.logger.info(txt)
-
-        table_name_global = TABLE_NAMES["global"]
-        table_name_stats = TABLE_NAMES["statistic"]
-        table_name_normal = TABLE_NAMES["normal"]
-
-        # creating temporary file and connection
-
-        temp_dir = tempfile.gettempdir()
-        temp_filename = os.path.join(temp_dir, os.path.basename(file_name))
-        shutil.copy2(file_name, temp_dir)
-        constr = self._get_res_connector(temp_filename)
+    if batchfiledir == "NONE" or batchfiledir is None:
+        batchfiledir = pathlib.Path(prms.Paths.batchfiledir)
+    else:
+        batchfiledir = pathlib.Path(batchfiledir).resolve()
 
-        if use_ado:
-            conn = dbloader.connect(constr)
+    if batchfiledir.is_dir():
+        click.echo(f"Content of '{batchfiledir}':\n")
+        i = 0
+        for i, f in enumerate(batchfiledir.glob("cellpy*.json")):
+            click.echo(f"{f.name}")
+        if i:
+            print(f"\nnumber of batch-files located: {i}")
         else:
-            conn = dbloader.connect(constr, autocommit=True)
+            print("No batch-files found in this directory.")
+    else:
+        click.echo(f"{batchfiledir} not found.")
 
-        self.logger.debug("tmp file: %s" % temp_filename)
-        self.logger.debug("constr str: %s" % constr)
 
-        # --------- read global-data ------------------------------------
-        self.logger.debug("reading global data table")
-        sql = "select * from %s" % table_name_global
-        global_data_df = pd.read_sql_query(sql, conn)
-        # col_names = list(global_data_df.columns.values)
-        self.logger.debug("sql statement: %s" % sql)
-
-        tests = global_data_df[self.headers_normal.test_id_txt]
-        number_of_sets = len(tests)
-        self.logger.debug("number of datasets: %i" % number_of_sets)
-        self.logger.debug("only selecting first test")
-        test_no = 0
-        self.logger.debug("setting data for test number %i" % test_no)
-        loaded_from = file_name
-        # fid = FileID(file_name)
-        start_datetime = global_data_df[self.headers_global["start_datetime_txt"]][
-            test_no
-        ]
-        test_ID = int(global_data_df[self.headers_normal.test_id_txt][test_no])  # OBS
-        test_name = global_data_df[self.headers_global["test_name_txt"]][test_no]
+def _run_journals(folder_name, debug, silent, raw, cellpyfile, minimal):
+    click.echo(f"running journals in {folder_name}")
+    # click.echo(f" --debug [{debug}]")
+    # click.echo(f" --silent [{silent}]")
+    # click.echo(f" --raw [{raw}]")
+    # click.echo(f" --cellpyfile [{cellpyfile}]")
+    # click.echo(f" --minimal [{minimal}]")
+
+    kwargs = dict()
+    if debug:
+        kwargs["default_log_level"] = "DEBUG"
+    if not minimal:
+        kwargs["export_raw"] = False
+        kwargs["export_cycles"] = False
+        kwargs["export_ica"] = False
+
+    from cellpy.utils import batch
+
+    folder_name = pathlib.Path(folder_name).resolve()
+
+    if not folder_name.is_dir():
+        click.echo(f"{folder_name} not found - aborting")
+        return
+
+    batch.iterate_batches(
+        folder_name, force_raw_file=raw, force_cellpy=cellpyfile, silent=True, **kwargs
+    )
+    click.echo("---")
+
+
+def _run_project(our_new_project, **kwargs):
+    try:
+        import papermill as pm
+    except ImportError:
+        click.echo(
+            "[cellpy]: You need to install papermill for automatically execute the notebooks."
+        )
+        click.echo("[cellpy]: You can install it using pip like this:")
+        click.echo(" >> pip install papermill")
+        return
+    our_new_project = pathlib.Path(our_new_project)
+    click.echo(f"[cellpy]: trying to run notebooks in {our_new_project}")
+    notebooks = sorted(list(our_new_project.glob("*.ipynb")))
+    for notebook in notebooks:
+        click.echo(f"[cellpy - papermill] running {notebook.name}")
+        pm.execute_notebook(notebook, notebook, parameters=kwargs)
+
+
+def _run(name, debug, silent):
+    click.echo(f"running {name}")
+    click.echo(f" --debug [{debug}]")
+    click.echo(f" --silent [{silent}]")
+
+
+def _run_db(debug, silent):
+    import platform
 
-        # --------- read raw-data (normal-data) -------------------------
-        self.logger.debug("reading raw-data")
+    from cellpy import prms
 
-        columns = ["Data_Point", "Step_Index", "Cycle_Index"]
-        columns_txt = ", ".join(["%s"] * len(columns)) % tuple(columns)
+    if not silent:
+        click.echo(f"running database editor")
+    if debug:
+        click.echo("running in debug-mode, but nothing to tell")
 
-        sql_1 = "select %s " % columns_txt
-        sql_2 = "from %s " % table_name_normal
-        sql_3 = "where %s=%s " % (self.headers_normal.test_id_txt, test_ID)
-        sql_5 = "order by %s" % self.headers_normal.data_point_txt
-        import time
-
-        info_list = []
-        info_header = ["cycle", "step", "row_count", "start_point", "end_point"]
-        self.logger.info(" ".join(info_header))
-        self.logger.info("-------------------------------------------------")
-        for cycle_number in range(1, 2000):
-            t1 = time.time()
-            self.logger.debug("picking cycle %i" % cycle_number)
-            sql_4 = "AND %s=%i " % (cycle_txt, cycle_number)
-            sql = sql_1 + sql_2 + sql_3 + sql_4 + sql_5
-            self.logger.debug("sql statement: %s" % sql)
-            normal_df = pd.read_sql_query(sql, conn)
-            t2 = time.time()
-            dt = t2 - t1
-            self.logger.debug("time: %f" % dt)
-            if normal_df.empty:
-                self.logger.debug("reached the end")
-                break
-            row_count, _ = normal_df.shape
-            steps = normal_df[self.headers_normal.step_index_txt].unique()
-            txt = "cycle %i: %i [" % (cycle_number, row_count)
-            for step in steps:
-                self.logger.debug(" step: %i" % step)
-                step_df = normal_df.loc[normal_df[step_txt] == step]
-                step_row_count, _ = step_df.shape
-                start_point = step_df[point_txt].min()
-                end_point = step_df[point_txt].max()
-                txt += " %i-(%i)" % (step, step_row_count)
-                step_list = [cycle_number, step, step_row_count, start_point, end_point]
-                info_list.append(step_list)
-
-            txt += "]"
-            self.logger.info(txt)
-
-        self._clean_up_loadres(None, conn, temp_filename)
-        info_dict = pd.DataFrame(info_list, columns=info_header)
-        return info_dict
-
-    def repair(self, file_name):
-        """try to repair a broken/corrupted file"""
-        raise NotImplemented
-
-    def dump(self, file_name, path):
-        """Dumps the raw file to an intermediate hdf5 file.
-
-        This method can be used if the raw file is too difficult to load and it
-        is likely that it is more efficient to convert it to an hdf5 format
-        and then load it using the `from_intermediate_file` function.
-
-        Args:
-            file_name: name of the raw file
-            path: path to where to store the intermediate hdf5 file (optional)
-
-        Returns:
-            full path to stored intermediate hdf5 file
-            information about the raw file (needed by the
-            `from_intermediate_file` function)
-
-        """
-
-        # information = None # contains information needed by the from_
-        # intermediate_file reader
-        # full_path = None
-        # return full_path, information
-        raise NotImplemented
-
-    # def loader_win(
-    #     self,
-    #     temp_filename,
-    #     table_name_global,
-    #     file_name,
-    #     bad_steps=None,
-    #     dataset_number=None,
-    #     data_points=None,
-    # ):
-    #
-    #     constr = self._get_res_connector(temp_filename)
-    #
-    #     if use_ado:
-    #         conn = dbloader.connect(constr)
-    #     else:
-    #         conn = dbloader.connect(constr, autocommit=True)
-    #     self.logger.debug("constr str: %s" % constr)
-    #
-    #     self.logger.debug("reading global data table")
-    #     sql = "select * from %s" % table_name_global
-    #     self.logger.debug("sql statement: %s" % sql)
-    #     global_data_df = pd.read_sql_query(sql, conn)
-    #     # col_names = list(global_data_df.columns.values)
-    #     tests = global_data_df[self.headers_normal.test_id_txt]
-    #
-    #     number_of_sets = len(tests)
-    #     self.logger.debug("number of datasets: %i" % number_of_sets)
-    #     self.logger.debug(f"datasets: {tests}")
-    #
-    #     if dataset_number is not None:
-    #         self.logger.info(f"Dataset number given: {dataset_number}")
-    #         self.logger.info(f"Available dataset numbers: {tests}")
-    #         test_nos = [dataset_number]
-    #     else:
-    #         test_nos = range(number_of_sets)
-    #
-    #     for counter, test_no in enumerate(test_nos):
-    #         if counter > 0:
-    #             self.logger.warning("** WARNING ** MULTI-TEST-FILE (not recommended)")
-    #             if not ALLOW_MULTI_TEST_FILE:
-    #                 break
-    #         data = self._init_data(file_name, global_data_df, test_no)
-    #
-    #         self.logger.debug("reading raw-data")
-    #         if not use_mdbtools:
-    #             # --------- read raw-data (normal-data) ------------------------
-    #             length_of_test, normal_df = self._load_res_normal_table(
-    #                 conn, data.test_ID, bad_steps
-    #             )
-    #             # --------- read stats-data (summary-data) ---------------------
-    #             sql = "select * from %s where %s=%s order by %s" % (
-    #                 table_name_stats,
-    #                 self.headers_normal.test_id_txt,
-    #                 data.test_ID,
-    #                 self.headers_normal.data_point_txt,
-    #             )
-    #             summary_df = pd.read_sql_query(sql, conn)
-    #
-    #         else:
-    #             length_of_test, normal_df, summary_df = self._load_from_tmp_files(
-    #                 data, tmp_name_global, tmp_name_raw, tmp_name_stats, temp_filename
-    #             )
-    #
-    #         if summary_df.empty and prms.Reader.use_cellpy_stat_file:
-    #             txt = "\nCould not find any summary (stats-file)!"
-    #             txt += "\n -> issue make_summary(use_cellpy_stat_file=False)"
-    #             logging.debug(txt)
-    #         # normal_df = normal_df.set_index("Data_Point")
-    #
-    #         data.summary = summary_df
-    #         if DEBUG_MODE:
-    #             mem_usage = normal_df.memory_usage()
-    #             logging.debug(
-    #                 f"memory usage for "
-    #                 f"loaded data: \n{mem_usage}"
-    #                 f"\ntotal: {humanize_bytes(mem_usage.sum())}"
-    #             )
-    #             logging.debug(f"time used: {(time.time() - time_0):2.4f} s")
-    #
-    #         data.raw = normal_df
-    #         data.raw_data_files_length.append(length_of_test)
-    #
-    #         data = self._post_process(data)
-    #         data = self.identify_last_data_point(data)
-    #
-    #         new_tests.append(data)
-    #
-    #     pass
-
-    # def loader_posix(
-    #     self, file_name, bad_steps=None, dataset_number=None, data_points=None
-    # ):
-    #     if is_posix:
-    #         if is_macos:
-    #             self.logger.debug("\nMAC OSX USING MDBTOOLS")
-    #         else:
-    #             self.logger.debug("\nPOSIX USING MDBTOOLS")
-    #     else:
-    #         self.logger.debug("\nWINDOWS USING MDBTOOLS-WIN")
-    #
-    #     tmp_name_global, tmp_name_raw, tmp_name_stats = self._create_tmp_files(
-    #         table_name_global,
-    #         table_name_normal,
-    #         table_name_stats,
-    #         temp_dir,
-    #         temp_filename,
-    #     )
-    #
-    #     # use pandas to load in the data
-    #     global_data_df = pd.read_csv(tmp_name_global)
-    #     tests = global_data_df[self.headers_normal.test_id_txt]
-    #
-    #     number_of_sets = len(tests)
-    #     self.logger.debug("number of datasets: %i" % number_of_sets)
-    #     self.logger.debug(f"datasets: {tests}")
-    #
-    #     if dataset_number is not None:
-    #         self.logger.info(f"Dataset number given: {dataset_number}")
-    #         self.logger.info(f"Available dataset numbers: {tests}")
-    #         test_nos = [dataset_number]
-    #     else:
-    #         test_nos = range(number_of_sets)
-    #
-    #     for counter, test_no in enumerate(test_nos):
-    #         if counter > 0:
-    #             self.logger.warning("** WARNING ** MULTI-TEST-FILE (not recommended)")
-    #             if not ALLOW_MULTI_TEST_FILE:
-    #                 break
-    #         data = self._init_data(file_name, global_data_df, test_no)
-    #
-    #         self.logger.debug("reading raw-data")
-    #         if not use_mdbtools:
-    #             # --------- read raw-data (normal-data) ------------------------
-    #             length_of_test, normal_df = self._load_res_normal_table(
-    #                 conn, data.test_ID, bad_steps
-    #             )
-    #             # --------- read stats-data (summary-data) ---------------------
-    #             sql = "select * from %s where %s=%s order by %s" % (
-    #                 table_name_stats,
-    #                 self.headers_normal.test_id_txt,
-    #                 data.test_ID,
-    #                 self.headers_normal.data_point_txt,
-    #             )
-    #             summary_df = pd.read_sql_query(sql, conn)
-    #
-    #         else:
-    #             length_of_test, normal_df, summary_df = self._load_from_tmp_files(
-    #                 data, tmp_name_global, tmp_name_raw, tmp_name_stats, temp_filename
-    #             )
-    #
-    #         if summary_df.empty and prms.Reader.use_cellpy_stat_file:
-    #             txt = "\nCould not find any summary (stats-file)!"
-    #             txt += "\n -> issue make_summary(use_cellpy_stat_file=False)"
-    #             logging.debug(txt)
-    #         # normal_df = normal_df.set_index("Data_Point")
-    #
-    #         data.summary = summary_df
-    #         if DEBUG_MODE:
-    #             mem_usage = normal_df.memory_usage()
-    #             logging.debug(
-    #                 f"memory usage for "
-    #                 f"loaded data: \n{mem_usage}"
-    #                 f"\ntotal: {humanize_bytes(mem_usage.sum())}"
-    #             )
-    #             logging.debug(f"time used: {(time.time() - time_0):2.4f} s")
-    #
-    #         data.raw = normal_df
-    #         data.raw_data_files_length.append(length_of_test)
-    #
-    #         data = self._post_process(data)
-    #         data = self.identify_last_data_point(data)
-    #
-    #         new_tests.append(data)
-    #
-    #     pass
-
-    def loader(self, file_name, bad_steps=None, dataset_number=None, data_points=None):
-        """Loads data from arbin .res files.
-
-        Args:
-            file_name (str): path to .res file.
-            bad_steps (list of tuples): (c, s) tuples of steps s (in cycle c)
-                to skip loading.
-            dataset_number (int): the data set number to select if you are dealing
-                with arbin files with more than one data-set.
-            data_points (tuple of ints): load only data from data_point[0] to
-                    data_point[1] (use None for infinite).
-
-        Returns:
-            new_tests (list of data objects)
-        """
-        # TODO: @jepe - insert kwargs - current chunk, only normal data, etc
-
-        if DEBUG_MODE:
-            time_0 = time.time()
-
-        new_tests = []
-        conn = None
-
-        if not os.path.isfile(file_name):
-            self.logger.info("Missing file_\n   %s" % file_name)
-            return None
-
-        self.logger.debug("in loader")
-        self.logger.debug("filename: %s" % file_name)
-
-        filesize = os.path.getsize(file_name)
-        hfilesize = humanize_bytes(filesize)
-        txt = "Filesize: %i (%s)" % (filesize, hfilesize)
-        self.logger.debug(txt)
-        if (
-            filesize > prms.Instruments.Arbin.max_res_filesize
-            and not prms.Reader.load_only_summary
-        ):
-            error_message = "\nERROR (loader):\n"
-            error_message += "%s > %s - File is too big!\n" % (
-                hfilesize,
-                humanize_bytes(prms.Instruments.Arbin.max_res_filesize),
+    db_path = Path(prms.Paths.db_path) / prms.Paths.db_filename
+
+    if platform.system() == "Windows":
+        try:
+            os.system(f'start excel "{str(db_path)}"')
+        except Exception as e:
+            click.echo("Something went wrong trying to open")
+            click.echo(db_path)
+            print()
+            print(e)
+
+    elif platform.system() == "Linux":
+        click.echo("RUNNING LINUX")
+        # not tested
+        subprocess.check_call(["open", "-a", "Microsoft Excel", db_path])
+
+    elif platform.system() == "Darwin":
+        click.echo(f" - running on a mac")
+        subprocess.check_call(["open", "-a", "Microsoft Excel", db_path])
+
+    else:
+        print("RUNNING SOMETHING ELSE")
+        print(platform.system())
+        # not tested
+        subprocess.check_call(["open", "-a", "Microsoft Excel", db_path])
+
+
+# ----------------------- pull ---------------------------------------
+@click.command()
+@click.option("--tests", "-t", is_flag=True, help="Download test-files from repo.")
+@click.option(
+    "--examples", "-e", is_flag=True, help="Download example-files from repo."
+)
+@click.option("--clone", "-c", is_flag=True, help="Clone the full repo.")
+@click.option("--directory", "-d", default=None, help="Save into custom directory DIR")
+@click.option("--password", "-p", default=None, help="Password option for the repo")
+def pull(tests, examples, clone, directory, password):
+    """Download examples or tests from the big internet (needs git)."""
+    if directory is not None:
+        click.echo(f"[cellpy] (pull) custom directory: {directory}")
+    else:
+        directory = pathlib.Path(prmreader.prms.Paths.examplesdir)
+
+    if password is not None:
+        click.echo("DEV MODE: password provided")
+    if clone:
+        _clone_repo(directory, password)
+    else:
+        if tests:
+            _pull_tests(directory, password)
+        if examples:
+            _pull_examples(directory, password)
+        else:
+            click.echo(
+                f"[cellpy] (pull) Nothing selected for pulling. "
+                f"Please select an option (--tests,--examples, -clone, ...) "
             )
-            error_message += "(edit prms.Instruments.Arbin ['max_res_filesize'])\n"
-            print(error_message)
-            return None
-
-        table_name_global = TABLE_NAMES["global"]
-        table_name_stats = TABLE_NAMES["statistic"]
-        table_name_normal = TABLE_NAMES["normal"]
-
-        # creating temporary file and connection
-        tmp_name_global = None
-        tmp_name_raw = None
-        tmp_name_stats = None
-
-        temp_dir = tempfile.gettempdir()
-        temp_filename = os.path.join(temp_dir, os.path.basename(file_name))
-        shutil.copy2(file_name, temp_dir)
-        self.logger.debug("tmp file: %s" % temp_filename)
-
-        use_mdbtools = False
-        if use_subprocess:
-            use_mdbtools = True
-        if is_posix:
-            use_mdbtools = True
-
-        # windows with same python bit as windows bit (the ideal case)
-
-        # SPLIT FROM HERE
-        if not use_mdbtools:
-            constr = self._get_res_connector(temp_filename)
 
-            if use_ado:
-                conn = dbloader.connect(constr)
-            else:
-                conn = dbloader.connect(constr, autocommit=True)
-            self.logger.debug("constr str: %s" % constr)
 
-            self.logger.debug("reading global data table")
-            sql = "select * from %s" % table_name_global
-            self.logger.debug("sql statement: %s" % sql)
-            global_data_df = pd.read_sql_query(sql, conn)
-            # col_names = list(global_data_df.columns.values)
+def _clone_repo(directory, password):
+    directory = pathlib.Path(directory)
+    txt = "[cellpy] The plan is that this "
+    txt += "[cellpy] cmd will pull (clone) the cellpy repo.\n"
+    txt += "[cellpy] For now it only prints the link to the git-hub\n"
+    txt += "[cellpy] repository:\n"
+    txt += "[cellpy]\n"
+    txt += "[cellpy] https://github.com/jepegit/cellpy.git\n"
+    txt += "[cellpy]\n"
+    click.echo(txt)
+
+
+def _pull_tests(directory, pw=None):
+    txt = (
+        "[cellpy] (pull) Pulling tests from",
+        " https://github.com/jepegit/cellpy.git",
+    )
+    click.echo(txt)
+    _pull(gdirpath="tests", rootpath=directory, pw=pw)
+    _pull(gdirpath="testdata", rootpath=directory, pw=pw)
+
+
+def _pull_examples(directory, pw):
+    txt = (
+        "[cellpy] (pull) Pulling examples from",
+        " https://github.com/jepegit/cellpy.git",
+    )
+    click.echo(txt)
+    _pull(gdirpath="examples", rootpath=directory, pw=pw)
 
+
+def _version():
+    txt = "[cellpy] version: " + str(VERSION)
+    click.echo(txt)
+
+
+def _configloc():
+    _, config_file_name = prmreader.get_user_dir_and_dst()
+    click.echo("[cellpy] ->%s" % config_file_name)
+    if not os.path.isfile(config_file_name):
+        click.echo("[cellpy] File does not exist!")
+    else:
+        return config_file_name
+
+
+def _dump_params():
+    click.echo("[cellpy] Dumping parameters to screen:\n")
+    prmreader.info()
+
+
+def _download_g_blob(name, local_path):
+    import urllib.request
+
+    dirs = local_path.parent
+    if not dirs.is_dir():
+        click.echo(f"[cellpy] (pull) creating dir: {dirs}")
+        dirs.mkdir(parents=True)
+
+    filename, headers = urllib.request.urlretrieve(
+        name.download_url, filename=local_path
+    )
+    click.echo(f"[cellpy] (pull) downloaded blob: {filename}")
+
+
+def _parse_g_dir(repo, gdirpath):
+    """parses a repo directory two-levels deep"""
+    for f in repo.get_contents(gdirpath):
+        if f.type == "dir":
+            for sf in repo.get_contents(f.path):
+                yield sf
         else:
-            if is_posix:
-                if is_macos:
-                    self.logger.debug("\nMAC OSX USING MDBTOOLS")
-                else:
-                    self.logger.debug("\nPOSIX USING MDBTOOLS")
-            else:
-                self.logger.debug("\nWINDOWS USING MDBTOOLS-WIN")
+            yield f
 
-            tmp_name_global, tmp_name_raw, tmp_name_stats = self._create_tmp_files(
-                table_name_global,
-                table_name_normal,
-                table_name_stats,
-                temp_dir,
-                temp_filename,
-            )
 
-            # use pandas to load in the data
-            global_data_df = pd.read_csv(tmp_name_global)
+def _get_user_name():
+    return "jepegit"
+
+
+def _get_pw(method):
+    if method == "ask":
+        return getpass.getpass()
+    elif method == "env":
+        return os.environ.get(GITHUB_PWD_VAR_NAME, None)
+
+    else:
+        return None
+
+
+def _pull(gdirpath="examples", rootpath=None, u=None, pw=None):
+    if rootpath is None:
+        rootpath = prmreader.prms.Paths.examplesdir
+
+    rootpath = pathlib.Path(rootpath)
+
+    ndirpath = rootpath / gdirpath
+
+    if pw is not None:
+        click.echo(" DEV MODE ".center(80, "-"))
+        u = _get_user_name()
+        if pw == "ask":
+            click.echo("   - ask for password")
+            pw = _get_pw(pw)
+        elif pw == "env":
+            click.echo("   - check environ for password ")
+            pw = _get_pw(pw)
+            click.echo("   - got something")
+            if pw is None:
+                click.echo("   - only None")
+                u = None
+
+    g = Github(u, pw)
+    repo = g.get_repo(REPO)
+
+    click.echo(f"[cellpy] (pull) pulling {gdirpath}")
+    click.echo(f"[cellpy] (pull) -> {ndirpath}")
+
+    if not ndirpath.is_dir():
+        click.echo(f"[cellpy] (pull) creating dir: {ndirpath}")
+        ndirpath.mkdir(parents=True)
+
+    for gfile in _parse_g_dir(repo, gdirpath):
+        gfilename = pathlib.Path(gfile.path)
+        nfilename = rootpath / gfilename
+
+        _download_g_blob(gfile, nfilename)
+
 
-        tests = global_data_df[self.headers_normal.test_id_txt]
+def _get_default_template():
+    template = "standard"
+    try:
+        template = prmreader.prms.Batch.template
+    except:
+        logging.debug("You dont have any default template defined in you .conf file")
+    return template
+
+
+def _read_local_templates(local_templates_path=None):
+    if local_templates_path is None:
+        local_templates_path = pathlib.Path(prmreader.prms.Paths.templatedir)
+    templates = {}
+    for p in list(local_templates_path.rglob("cellpy_cookie*.zip")):
+        label = p.stem.strip()[len("cellpy_cookie_") :]
+        templates[label] = (str(p), None)
+    logging.debug(f"Found the following templates: {templates}")
+    return templates
+
+
+# ----------------------- new ----------------------------------------
+@click.command()
+@click.option("--template", "-t", help="Provide template name.")
+@click.option("--directory", "-d", default=None, help="Create in custom directory.")
+@click.option(
+    "--project",
+    "-p",
+    default=None,
+    help="Provide project name (i.e. sub-directory name).",
+)
+@click.option(
+    "--experiment",
+    "-e",
+    default=None,
+    help="Provide experiment name (i.e. lookup-value).",
+)
+@click.option(
+    "--local-user-template",
+    "-u",
+    is_flag=True,
+    default=False,
+    help="Use local template from the templates directory.",
+)
+@click.option("--serve", "-s", "serve_", is_flag=True, help="Run Jupyter.")
+@click.option(
+    "--run",
+    "-r",
+    "run_",
+    is_flag=True,
+    help="Use PaperMill to run the notebook(s) from the template "
+    "(will only work properly if the notebooks can be sorted in correct run-order by 'sorted'.",
+)
+@click.option(
+    "--lab",
+    "-j",
+    is_flag=True,
+    help="Use Jupyter Lab instead of Notebook when serving.",
+)
+@click.option(
+    "--list", "-l", "list_", is_flag=True, help="List available templates and exit."
+)
+def new(
+    template,
+    directory,
+    project,
+    experiment,
+    local_user_template,
+    serve_,
+    run_,
+    lab,
+    list_,
+):
+    """Set up a batch experiment (might need git installed)."""
+    _new(
+        template,
+        directory=directory,
+        project_dir=project,
+        session_id=experiment,
+        local_user_template=local_user_template,
+        serve_=serve_,
+        run_=run_,
+        lab=lab,
+        list_=list_,
+    )
 
-        number_of_sets = len(tests)
-        self.logger.debug("number of datasets: %i" % number_of_sets)
-        self.logger.debug(f"datasets: {tests}")
-
-        if dataset_number is not None:
-            self.logger.info(f"Dataset number given: {dataset_number}")
-            self.logger.info(f"Available dataset numbers: {tests}")
-            test_nos = [dataset_number]
+
+def _new(
+    template: str,
+    directory: Union[Path, str, None] = None,
+    project_dir: Union[str, None] = None,
+    local_user_template: bool = False,
+    serve_: bool = False,
+    run_: bool = False,
+    lab: bool = False,
+    list_: bool = False,
+    session_id: str = "experiment_001",
+    no_input: bool = False,
+    cookie_directory: str = "",
+):
+    """Set up a batch experiment (might need git installed).
+
+    Args:
+        template: short-name of template.
+        directory: the directory for your cellpy projects.
+        local_user_template: use local template if True.
+        serve_: serve the notebook after creation if True.
+        run_: run the notebooks using papermill if True.
+        lab: use jupyter-lab instead of jupyter notebook if True.
+        list_: list all available templates and return if True.
+        project_dir: your project directory.
+        session_id: the lookup value.
+        no_input: accept defaults if True (only valid when providing project_dir and session_id)
+        cookie_directory: name of the directory for your cookie (inside the repository or zip file).
+    Returns:
+        None
+    """
+
+    from cellpy.parameters import prms
+
+    if list_:
+        click.echo(f"\n[cellpy] batch templates")
+
+        default_template = _get_default_template()
+        local_templates = _read_local_templates()
+        local_templates_path = prmreader.prms.Paths.templatedir
+        registered_templates = prms._registered_templates
+        click.echo(f"[cellpy] - default: {default_template}")
+        click.echo("[cellpy] - registered templates (on github):")
+        for label, link in registered_templates.items():
+            click.echo(f"\t\t{label:18s} {link}")
+
+        if local_templates:
+            click.echo(f"[cellpy] - local templates ({local_templates_path}):")
+            for label, link in local_templates.items():
+                click.echo(f"\t\t{label:18s} {link}")
         else:
-            test_nos = range(number_of_sets)
+            click.echo(f"[cellpy] - local templates ({local_templates_path}): none")
 
-        for counter, test_no in enumerate(test_nos):
-            if counter > 0:
-                self.logger.warning("** WARNING ** MULTI-TEST-FILE (not recommended)")
-                if not ALLOW_MULTI_TEST_FILE:
-                    break
-            data = self._init_data(file_name, global_data_df, test_no)
+        return
 
-            self.logger.debug("reading raw-data")
-            if not use_mdbtools:
-                # --------- read raw-data (normal-data) ------------------------
-                length_of_test, normal_df = self._load_res_normal_table(
-                    conn, data.test_ID, bad_steps
-                )
-                # --------- read stats-data (summary-data) ---------------------
-                sql = "select * from %s where %s=%s order by %s" % (
-                    table_name_stats,
-                    self.headers_normal.test_id_txt,
-                    data.test_ID,
-                    self.headers_normal.data_point_txt,
-                )
-                summary_df = pd.read_sql_query(sql, conn)
+    if project_dir is None or session_id is None:
+        no_input = False
 
-            else:
-                length_of_test, normal_df, summary_df = self._load_from_tmp_files(
-                    data, tmp_name_global, tmp_name_raw, tmp_name_stats, temp_filename
-                )
+    if not template:
+        template = _get_default_template()
 
-            if summary_df.empty and prms.Reader.use_cellpy_stat_file:
-                txt = "\nCould not find any summary (stats-file)!"
-                txt += "\n -> issue make_summary(use_cellpy_stat_file=False)"
-                logging.debug(txt)
-            # normal_df = normal_df.set_index("Data_Point")
-
-            data.summary = summary_df
-            if DEBUG_MODE:
-                mem_usage = normal_df.memory_usage()
-                logging.debug(
-                    f"memory usage for "
-                    f"loaded data: \n{mem_usage}"
-                    f"\ntotal: {humanize_bytes(mem_usage.sum())}"
-                )
-                logging.debug(f"time used: {(time.time() - time_0):2.4f} s")
+    if lab:
+        server = "lab"
+    else:
+        server = "notebook"
 
-            data.raw = normal_df
-            data.raw_data_files_length.append(length_of_test)
+    try:
+        import cookiecutter.exceptions
+        import cookiecutter.main
+        import cookiecutter.prompt
+
+    except ModuleNotFoundError:
+        click.echo("Could not import cookiecutter.")
+        click.echo("Try installing it, for example by writing:")
+        click.echo("\npip install cookiecutter\n")
+
+    click.echo(f"Template: {template}")
+    if local_user_template:
+        # forcing using local template
+        templates = _read_local_templates()
+
+        if not templates:
+            click.echo(
+                "You asked me to use a local template, but you have none. Aborting."
+            )
+            return
+    else:
+        templates = prms._registered_templates
+        if local_templates := _read_local_templates():
+            templates.update(local_templates)
+
+    if not template.lower() in templates.keys():
+        click.echo("This template does not exist. Aborting.")
+        return
+
+    if directory is None:
+        logging.debug("no dir given")
+        directory = prms.Paths.notebookdir
+
+    if not os.path.isdir(directory):
+        click.echo("Sorry. This did not work as expected!")
+        click.echo(f" - {directory} does not exist")
+        return
+
+    directory = Path(directory)
+    selected_project_dir = None
+
+    if project_dir:
+        selected_project_dir = directory / project_dir
+        if not selected_project_dir.is_dir():
+            if cookiecutter.prompt.read_user_yes_no(
+                f"{project_dir} does not exist. Create?", "yes"
+            ):
+                os.mkdir(selected_project_dir)
+                click.echo(f"Created {selected_project_dir}")
 
-            data = self._post_process(data)
-            data = self.identify_last_data_point(data)
+            else:
+                selected_project_dir = None
+                click.echo(f"Select another directory instead")
 
-            new_tests.append(data)
-
-        # COMBINE FROM HERE
-        new_tests = self._inspect(new_tests)
-
-        return new_tests
-
-    def _create_tmp_files(
-        self,
-        table_name_global,
-        table_name_normal,
-        table_name_stats,
-        temp_dir,
-        temp_filename,
-    ):
-        import subprocess
-
-        # creating tmp-filenames
-        temp_csv_filename_global = os.path.join(temp_dir, "global_tmp.csv")
-        temp_csv_filename_normal = os.path.join(temp_dir, "normal_tmp.csv")
-        temp_csv_filename_stats = os.path.join(temp_dir, "stats_tmp.csv")
-        # making the cmds
-        mdb_prms = [
-            (table_name_global, temp_csv_filename_global),
-            (table_name_normal, temp_csv_filename_normal),
-            (table_name_stats, temp_csv_filename_stats),
+    if not selected_project_dir:
+        project_dirs = [
+            d.name
+            for d in directory.iterdir()
+            if d.is_dir() and not d.name.startswith(".")
         ]
-        # executing cmds
-        for table_name, tmp_file in mdb_prms:
-            with open(tmp_file, "w") as f:
-                subprocess.call([sub_process_path, temp_filename, table_name], stdout=f)
-                self.logger.debug(f"ran mdb-export {str(f)} {table_name}")
-        return (
-            temp_csv_filename_global,
-            temp_csv_filename_normal,
-            temp_csv_filename_stats,
-        )
+        project_dirs.insert(0, "[create new dir]")
 
-    def _load_from_tmp_files(
-        self,
-        data,
-        temp_csv_filename_global,
-        temp_csv_filename_normal,
-        temp_csv_filename_stats,
-        temp_filename,
-    ):
-        normal_df = pd.read_csv(temp_csv_filename_normal)
-        # filter on test ID
-        normal_df = normal_df[
-            normal_df[self.headers_normal.test_id_txt] == data.test_ID
-        ]
-        # sort on data point
-        if prms._sort_if_subprocess:
-            normal_df = normal_df.sort_values(self.headers_normal.data_point_txt)
-        length_of_test = normal_df.shape[0]
-        summary_df = pd.read_csv(temp_csv_filename_stats)
-        # clean up
-        for f in [
-            temp_filename,
-            temp_csv_filename_stats,
-            temp_csv_filename_normal,
-            temp_csv_filename_global,
-        ]:
-            if os.path.isfile(f):
-                try:
-                    os.remove(f)
-                except WindowsError as e:
-                    self.logger.warning(f"could not remove tmp-file\n{f} {e}")
-        return length_of_test, normal_df, summary_df
-
-    def _init_data(self, file_name, global_data_df, test_no):
-        data = Cell()
-        data.cell_no = test_no
-        data.loaded_from = file_name
-        fid = FileID(file_name)
-        # name of the .res file it is loaded from:
-        # data.parent_filename = os.path.basename(file_name)
-        data.channel_index = int(
-            global_data_df[self.headers_global["channel_index_txt"]][test_no]
-        )
-        data.channel_number = int(
-            global_data_df[self.headers_global["channel_number_txt"]][test_no]
+        project_dir = cookiecutter.prompt.read_user_choice(
+            "project folder", project_dirs
         )
-        data.creator = global_data_df[self.headers_global["creator_txt"]][test_no]
-        data.item_ID = global_data_df[self.headers_global["item_id_txt"]][test_no]
-        data.schedule_file_name = global_data_df[
-            self.headers_global["schedule_file_name_txt"]
-        ][test_no]
-        data.start_datetime = global_data_df[self.headers_global["start_datetime_txt"]][
-            test_no
-        ]
-        data.test_ID = int(global_data_df[self.headers_normal.test_id_txt][test_no])
-        data.test_name = global_data_df[self.headers_global["test_name_txt"]][test_no]
-        data.raw_data_files.append(fid)
-        return data
-
-    def _normal_table_generator(self, **kwargs):
-        pass
-
-    def _load_res_normal_table(self, conn, test_ID, bad_steps):
-        # Note that this function is run each time you use the loader.
-        # This means that it is not ideal for
-        # handling generators etc
-
-        self.logger.debug("starting loading raw-data")
-        self.logger.debug(f"connection: {conn} test-ID: {test_ID}")
-        self.logger.debug(f"bad steps:  {bad_steps}")
-
-        table_name_normal = TABLE_NAMES["normal"]
-
-        if prms.Reader.load_only_summary:  # SETTING
-            warnings.warn("not implemented")
-
-        if prms.Reader.select_minimal:  # SETTING
-            columns = MINIMUM_SELECTION
-            columns_txt = ", ".join(["%s"] * len(columns)) % tuple(columns)
-        else:
-            columns_txt = "*"
 
-        sql_1 = "select %s " % columns_txt
-        sql_2 = "from %s " % table_name_normal
-        sql_3 = "where %s=%s " % (self.headers_normal.test_id_txt, test_ID)
-        sql_4 = ""
-
-        if bad_steps is not None:
-            if not isinstance(bad_steps, (list, tuple)):
-                bad_steps = [bad_steps]
-            for bad_cycle, bad_step in bad_steps:
-                self.logger.debug(f"bad_step def: [c={bad_cycle}, s={bad_step}]")
-                sql_4 += "AND NOT (%s=%i " % (
-                    self.headers_normal.cycle_index_txt,
-                    bad_cycle,
-                )
-                sql_4 += "AND %s=%i) " % (self.headers_normal.step_index_txt, bad_step)
+        if project_dir == "[create new dir]":
+            default_name = "cellpy_project"
+            temp_default_name = default_name
+            for j in range(999):
+                if temp_default_name in project_dirs:
+                    temp_default_name = default_name + str(j + 1).zfill(3)
+                else:
+                    default_name = temp_default_name
+                    break
 
-        if prms.Reader.limit_loaded_cycles:
-            if len(prms.Reader.limit_loaded_cycles) > 1:
-                sql_4 += "AND %s>%i " % (
-                    self.headers_normal.cycle_index_txt,
-                    prms.Reader.limit_loaded_cycles[0],
-                )
-                sql_4 += "AND %s<%i " % (
-                    self.headers_normal.cycle_index_txt,
-                    prms.Reader.limit_loaded_cycles[-1],
-                )
-            else:
-                sql_4 = "AND %s=%i " % (
-                    self.headers_normal.cycle_index_txt,
-                    prms.Reader.limit_loaded_cycles[0],
-                )
+            project_dir = cookiecutter.prompt.read_user_variable(
+                "New name", default_name
+            )
+            try:
+                os.mkdir(directory / project_dir)
+                click.echo(f"created {project_dir}")
+            except FileExistsError:
+                click.echo("OK - but this directory already exists!")
+        selected_project_dir = directory / project_dir
+
+    # get a list of all folders
+    existing_projects = os.listdir(selected_project_dir)
 
-        sql_5 = "order by %s" % self.headers_normal.data_point_txt
-        sql = sql_1 + sql_2 + sql_3 + sql_4 + sql_5
+    os.chdir(selected_project_dir)
+    cellpy_version = cellpy.__version__
 
-        self.logger.debug("INFO ABOUT LOAD RES NORMAL")
-        self.logger.debug("sql statement: %s" % sql)
+    try:
+        selected_template, cookie_dir = templates[template.lower()]
 
-        if DEBUG_MODE:
-            current_memory_usage = sys.getsizeof(self)
+        if cookie_directory:
+            cookie_dir = cookie_directory
+        if not cookie_dir:
+            cookie_dir = template.lower()
+
+        cookiecutter.main.cookiecutter(
+            selected_template,
+            extra_context={
+                "author_name": os.getlogin(),
+                "project_name": project_dir,
+                "cellpy_version": cellpy_version,
+                "session_id": session_id,
+            },
+            no_input=no_input,
+            directory=cookie_dir,
+        )
+    except cookiecutter.exceptions.OutputDirExistsException as e:
+        click.echo("Sorry. This did not work as expected!")
+        click.echo(" - cookiecutter refused to create the project")
+        click.echo(e)
+
+    if serve_:
+        os.chdir(directory)
+        _serve(server)
 
-        if not prms.Instruments.Arbin.chunk_size:
-            self.logger.debug("no chunk-size given")
-            # memory here
-            normal_df = pd.read_sql_query(sql, conn)
-            # memory here
-            length_of_test = normal_df.shape[0]
-            self.logger.debug(f"loaded to normal_df (length =  {length_of_test})")
-        else:
-            self.logger.debug(f"chunk-size: {prms.Instruments.Arbin.chunk_size}")
-            self.logger.debug("creating a pd.read_sql_query generator")
-            normal_df_reader = pd.read_sql_query(
-                sql, conn, chunksize=prms.Instruments.Arbin.chunk_size
+    if run_:
+        try:
+            import papermill as pm
+        except ImportError:
+            click.echo(
+                "[cellpy]: You need to install papermill for automatically execute the notebooks."
             )
-            normal_df = None
-            chunk_number = 0
-            self.logger.debug("created pandas sql reader")
-            self.logger.debug("iterating chunk-wise")
-            for i, chunk in enumerate(normal_df_reader):
-                self.logger.debug(f"iteration number {i}")
-                if prms.Instruments.Arbin.max_chunks:
-                    self.logger.debug(
-                        f"max number of chunks mode "
-                        f"({prms.Instruments.Arbin.max_chunks})"
-                    )
-                    if chunk_number < prms.Instruments.Arbin.max_chunks:
-                        normal_df = pd.concat([normal_df, chunk], ignore_index=True)
-                        self.logger.debug(
-                            f"chunk {i} of {prms.Instruments.Arbin.max_chunks}"
-                        )
-                    else:
-                        break
-                else:
-                    try:
-                        normal_df = pd.concat([normal_df, chunk], ignore_index=True)
-                        self.logger.debug("concatenated new chunk")
-                    except MemoryError:
-                        self.logger.error(
-                            " - Could not read complete file (MemoryError)."
-                        )
-                        self.logger.error(
-                            f"Last successfully loaded chunk " f"number: {chunk_number}"
-                        )
-                        self.logger.error(
-                            f"Chunk size: {prms.Instruments.Arbin.chunk_size}"
-                        )
-                        break
-                chunk_number += 1
-            length_of_test = normal_df.shape[0]
-            self.logger.debug(f"finished iterating (#rows: {length_of_test})")
-        return length_of_test, normal_df
+            click.echo("[cellpy]: You can install it using pip like this:")
+            click.echo(" >> pip install papermill")
+            return
+        new_existing_projects = os.listdir(selected_project_dir)
+        our_new_projects = list(set(new_existing_projects) - set(existing_projects))
+
+        if not len(our_new_projects):
+            click.echo(
+                "[cellpy]: Sorry, could not deiced what is the new project "
+                "- so I don't dare to try to execute automatically."
+            )
+            return
+        our_new_project = selected_project_dir / our_new_projects[0]
 
+        _run_project(our_new_project)
 
-if __name__ == "__main__":
-    import logging
 
-    from cellpy import log
+def _serve(server):
+    click.echo(f"serving with jupyter {server}")
+    subprocess.run(["jupyter", server], check=True)
+    click.echo("Finished serving.")
+
+
+# ----------------------- serve ---------------------------------------
+@click.command()
+@click.option("--lab", "-l", is_flag=True, help="Use Jupyter Lab instead of Notebook")
+@click.option("--directory", "-d", default=None, help="Start in custom directory DIR")
+def serve(lab, directory):
+    """Start a Jupyter server."""
+
+    from cellpy.parameters import prms
+
+    if directory is None:
+        directory = prms.Paths.notebookdir
+    elif directory == "home":
+        directory = Path().home()
+    elif directory == "here":
+        directory = Path(os.getcwd())
 
-    log.setup_logging(default_level="DEBUG")
+    if not os.path.isdir(directory):
+        click.echo("Sorry. This did not work as expected!")
+        click.echo(f" - {directory} does not exist")
+        return
+
+    if lab:
+        server = "lab"
+    else:
+        server = "notebook"
+
+    os.chdir(directory)
+    _serve(server)
+
+
+cli.add_command(setup)
+cli.add_command(info)
+cli.add_command(edit)
+cli.add_command(pull)
+cli.add_command(run)
+cli.add_command(new)
+cli.add_command(serve)
+
+
+# tests etc
+def _main_pull():
+    if sys.platform == "win32":
+        rootpath = pathlib.Path(r"C:\Temp\cellpy_user")
+    else:
+        rootpath = pathlib.Path("/Users/jepe/scripting/tmp/cellpy_test_user")
+    _pull_examples(rootpath, pw="env")
+    _pull_tests(rootpath, pw="env")
+    # _pull(gdirpath="examples", rootpath=rootpath, u="ask", pw="ask")
+    # _pull(gdirpath="tests", rootpath=rootpath, u="ask", pw="ask")
+    # _pull(gdirpath="testdata", rootpath=rootpath, u="ask", pw="ask")
+
+
+def _main():
+    file_name = prmreader.create_custom_init_filename()
+    click.echo(file_name)
+    user_directory, destination_file_name = prmreader.get_user_dir_and_dst(file_name)
+    click.echo(user_directory)
+    click.echo(destination_file_name)
+    click.echo("trying to save it")
+    save_prm_file(destination_file_name + "_dummy")
+
+    click.echo(" Testing setup ".center(80, "="))
+    setup(["--interactive", "--reset"])
+
+
+def _cli_setup_interactive():
+    from click.testing import CliRunner
+
+    if sys.platform == "win32":
+        root_dir = r"C:\Temp\cellpy_user"
+    else:
+        root_dir = "/Users/jepe/scripting/tmp/cellpy_test_user"
+    testuser = "tester"
+    init_filename = prmreader.create_custom_init_filename(testuser)
+    dst_file = get_dst_file(root_dir, init_filename)
+    init_file = pathlib.Path(dst_file)
+    opts = list()
+    opts.append("setup")
+    opts.append("-i")
+    # opts.append("-nr")
+    opts.append("-r")
+    opts.extend(["-d", root_dir])
+    opts.extend(["-t", testuser])
+
+    input_str = "\n"  # out
+    input_str += "\n"  # rawdatadir
+    input_str += "\n"  # cellpyfiles
+    input_str += "\n"  # log
+    input_str += "\n"  # examples
+    input_str += "\n"  # dbfolder
+    input_str += "\n"  # dbfile
+    runner = CliRunner()
+    result = runner.invoke(cli, opts, input=input_str)
+
+    click.echo(" out ".center(80, "."))
+    click.echo(result.output)
+    from pprint import pprint
+
+    pprint(prmreader.prms.Paths)
+    click.echo(" conf-file ".center(80, "."))
+    click.echo(init_file)
+    click.echo()
+    with init_file.open() as f:
+        for line in f.readlines():
+            click.echo(line.strip())
+
+
+def check_it(var=None):
+    import pathlib
+    import sys
+
+    p_env = pathlib.Path(sys.prefix)
+    print(p_env.name)
+    new(list_=True)
+
+
+if __name__ == "__main__":
+    u1 = os.getlogin()
+    u2 = os.path.expanduser("~")
+    u3 = os.environ.get("USERNAME")
+
+    print(u1)
+    print(u2)
+    print(u3)
+    # check_it()
+    # click.echo("\n\n", " RUNNING MAIN PULL ".center(80, "*"), "\n")
+    # _main_pull()
+    # click.echo("ok")
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/base.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/biologics_mpr.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,35 +1,52 @@
-"""
-When you make a new loader you have to subclass the Loader class.
-Remember also to register it in cellpy.cellreader.
-
-(for future development, not used very efficiently yet).
-"""
-
-import abc
-import logging
-import pathlib
+"""This file contains methods for importing Bio-Logic mpr-type files"""
+# This is based on the work by Chris Kerr
+# (https://github.com/chatcannon/galvani/blob/master/galvani/BioLogic.py)
+import datetime
 import shutil
 import tempfile
-from abc import ABC
-from typing import List, Union
+import time
+import warnings
+import logging
+import os
+from collections import OrderedDict
 
+import numpy as np
 import pandas as pd
 
-import cellpy.readers.core as core
-from cellpy.parameters.internal_settings import headers_normal
-from cellpy.readers.instruments.configurations import (
-    ModelParameters,
-    register_configuration_from_module,
-)
-from cellpy.readers.instruments.processors import post_processors, pre_processors
-from cellpy.readers.instruments.processors.post_processors import (
-    ORDERED_POST_PROCESSING_STEPS,
+from cellpy.parameters.internal_settings import get_headers_normal
+from cellpy.readers.core import Data, FileID, humanize_bytes
+from cellpy.readers.instruments.base import BaseLoader
+from cellpy.readers.instruments.loader_specific_modules.biologic_file_format import (
+    bl_dtypes,
+    bl_flags,
+    bl_log_pos_dtype,
+    hdr_dtype,
+    mpr_label,
 )
 
+OLE_TIME_ZERO = datetime.datetime(1899, 12, 30, 0, 0, 0)
+SEEK_SET = 0  # from start
+SEEK_CUR = 1  # from current position
+SEEK_END = 2  # from end of file
+
+
+def ole2datetime(oledt):
+    """converts from ole datetime float to datetime"""
+    return OLE_TIME_ZERO + datetime.timedelta(days=float(oledt))
+
+
+def datetime2ole(dt):
+    """converts from datetime object to ole datetime float"""
+    delta = dt - OLE_TIME_ZERO
+    delta_float = delta / datetime.timedelta(days=1)  # trick from SO
+    return delta_float
+
+
+# The columns to choose if minimum selection is selected
 MINIMUM_SELECTION = [
     "Data_Point",
     "Test_Time",
     "Step_Time",
     "DateTime",
     "Step_Index",
     "Cycle_Index",
@@ -37,610 +54,559 @@
     "Voltage",
     "Charge_Capacity",
     "Discharge_Capacity",
     "Internal_Resistance",
 ]
 
 
-# TODO: move this to another module (e.g. inside processors):
-def find_delimiter_and_start(
-    file_name,
-    separators=None,
-    checking_length_header=30,
-    checking_length_whole=200,
-):
-    """function to automatically detect the delimiter and what line the first data appears on.
-
-    Remark! This function is rather simple, it splits the data into to parts
-        (possible header part (checking_length_header) and the rest of the data). Then it counts the appearances of
-        the different possible delimiters in the rest of the data part, and then selects a delimiter if it has unique
-        counts for all the lines.
-
-        The first line is defined as where the delimiter is used same number of times (probably a header line).
-    """
-
-    if separators is None:
-        separators = [";", "\t", "|", ","]
-    logging.debug(f"checking internals of the file {file_name}")
-
-    empty_lines = 0
-    with open(file_name, "r") as fin:
-        lines = []
-        for j in range(checking_length_whole):
-            line = fin.readline()
-            if not line:
-                break
-            if len(line.strip()):
-                lines.append(line)
-            else:
-                empty_lines += 1
-
-    checking_length_whole -= empty_lines
-    if checking_length_header - empty_lines < 1:
-        checking_length_header = checking_length_whole // 2
-    separator, number_of_hits = _find_separator(
-        checking_length_whole - checking_length_header, lines, separators
-    )
-
-    if separator is None:
-        raise IOError(f"could not decide delimiter in {file_name}")
-
-    if separator == "\t":
-        logging.debug("seperator = TAB")
-    elif separator == " ":
-        logging.debug("seperator = SPACE")
-    else:
-        logging.debug(f"seperator = {separator}")
-
-    first_index = _find_first_line_whit_delimiter(
-        checking_length_header, lines, number_of_hits, separator
-    )
-    logging.debug(f"First line with delimiter: {first_index}")
-    return separator, first_index
-
-
-def _find_first_line_whit_delimiter(
-    checking_length_header, lines, number_of_hits, separator
-):
-    first_part = lines[:checking_length_header]
-    if number_of_hits is None:
-        # remark! if number of hits (i.e. how many separators pr line) is not given, we set it to the amount of
-        # separators we find in the third last line.
-        number_of_hits = lines[-3].count(separator)
-    return [
-        line_number
-        for line_number, line in enumerate(first_part)
-        if line.count(separator) == number_of_hits
-    ][0]
-
-
-def _find_separator(checking_length, lines, separators):
-    logging.debug("searching for separators")
-    separator = None
-    number_of_hits = None
-    last_part = lines[
-        checking_length:-1
-    ]  # don't include last line since it might be corrupted
-    check_sep = dict()
-
-    for i, v in enumerate(separators):
-        check_sep[i] = [line.count(v) for line in last_part]
-
-    unique_sep_counts = {i: set(v) for i, v in check_sep.items()}
-
-    for index, value in unique_sep_counts.items():
-        value_as_list = list(value)
-        number_of_hits = value_as_list[0]
-        if len(value_as_list) == 1 and number_of_hits > 0:
-            separator = separators[index]
-            break
-
-    return separator, number_of_hits
-
-
-def query_csv(
-    self,
-    name,
-    sep=None,
-    skiprows=None,
-    header=None,
-    encoding=None,
-    decimal=None,
-    thousands=None,
-):
-    logging.debug(f"parsing with pandas.read_csv: {name}")
-    sep = sep or self.sep
-    skiprows = skiprows or self.skiprows
-    header = header or self.header
-    encoding = encoding or self.encoding
-    decimal = decimal or self.decimal
-    thousands = thousands or self.thousands
-    logging.critical(f"{sep=}, {skiprows=}, {header=}, {encoding=}, {decimal=}")
-    data_df = pd.read_csv(
-        name,
-        sep=sep,
-        skiprows=skiprows,
-        header=header,
-        encoding=encoding,
-        decimal=decimal,
-        thousands=thousands,
-    )
-    return data_df
-
-
-class AtomicLoad:
-    """Atomic loading class"""
-
-    name = "atomic_loader"
-    pass
-
-
-class BaseLoader(AtomicLoad, metaclass=abc.ABCMeta):
-    """Main loading class"""
+def _read_modules(fileobj):
+    module_magic = fileobj.read(len(b"MODULE"))
+    hdr_bytes = fileobj.read(hdr_dtype.itemsize)
+    hdr = np.fromstring(
+        hdr_bytes, dtype=hdr_dtype, count=1
+    )  # TODO: change to frombuffer
+    hdr_dict = dict(((n, hdr[n][0]) for n in hdr_dtype.names))
+    hdr_dict["offset"] = fileobj.tell()
+    hdr_dict["data"] = fileobj.read(hdr_dict["length"])
+    fileobj.seek(hdr_dict["offset"] + hdr_dict["length"], SEEK_SET)
+    hdr_dict["end"] = fileobj.tell()
+    return hdr_dict
+
+
+class DataLoader(BaseLoader):
+    """Class for loading biologics-data from mpr-files."""
+
+    # Note: the class is sub-classing Loader. At the moment, Loader does
+    # not really contain anything...
+    instrument_name = "biologics_mpr"
+    raw_ext = "mpr"
 
-    name = "base_loader"
-
-    # TODO: should also include the functions for getting cellpy headers etc
-    #  here
+    def __init__(self, *args, **kwargs):
+        self.logger = logging.getLogger(__name__)
+        self.headers_normal = get_headers_normal()
+        self.current_chunk = 0  # use this to set chunks to load
+        self.mpr_data = None
+        self.mpr_log = None
+        self.mpr_settings = None
+        self.cellpy_headers = get_headers_normal()
 
     @staticmethod
-    @abc.abstractmethod
-    def get_raw_units() -> dict:
-        """Include the settings for the units used by the instrument. This is needed for example when
-        converting the capacity to a specific capacity. So far, it has been difficult to get any kind of
-        consensus on what the most optimal units are for storing cycling data. Therefore, cellpy implements three
-        levels of units: 1) the raw units that the data is loaded in already has and 2) the cellpy units used by cellpy
-        when generating summaries and related information, and 3) output units that can be set to get the data
-        in a specif unit when exporting or creating specific outputs such as ICA.
-
-        Comment 2022.09.11: still not sure if we should use raw units or cellpy units in the cellpy-files (.h5).
-            Currently, the summary is in cellpy units and the raw and step data is in raw units. If
-            you have any input on this topic, let us know. What (at least I, i.e. jepe) would like to implement is
-            to only use raw units in the cellpy-files. And rename the summary headers so that they don't contain
-            any reference to their units (e.g. renaming `discharge_capacity_u_mAh_g` to `specific_discharge_capacity`
-            or `discharge_capacity_specific`). This will be a breaking change.
+    def get_raw_units():
+        """Include the settings for the units used by the instrument.
 
-        The units are defined w.r.t. the SI units ('unit-fractions'; currently only units that are multiples of
-        Si units can be used). For example, for current defined in mA, the value for the
+        The units are defined w.r.t. the SI units ('unit-fractions';
+        currently only units that are multiples of
+        Si units can be used). For example, for current defined in mA,
+        the value for the
         current unit-fraction will be 0.001.
 
-        Returns: dictionary containing the unit-fractions for current, charge, and mass
-
-        Comments to me:
-            This will be used to convert all values to the internal set cellpy units (given in xxxx).
-            The units used by cellpy will be stored in the cellpy file. The original units will not be stored.
-        Example:
-
-            @staticmethod
-            def get_raw_units():
-                raw_units = {
-                    "current": 1.0, # A
-                    "charge": 1.0, # Ah
-                    "mass": 0.001, # g (i.e. units are given in mg)
-                    "voltage": 1.0,  # V
-                    "time": 1.0, # sec
-                    "resistance": 1.0, # Ohms
-                    "power": 1.0, # W
-                    "energy": 1.0, # Wh
-                    "length": 1.0, # m
-                    "area": 1.0, # m2
-                    "temperature": 1.0, # C
-                }
-                return raw_units
-
-        The internal cellpy units are given in the
-
-        """
-        raise NotImplementedError
-
-    @abc.abstractmethod
-    def get_raw_limits(self) -> dict:
-        """Include the settings for how to decide what kind of step you are examining here.
-
-        The raw limits are 'epsilons' used to check if the current and/or voltage is stable (for example
-        for galvanostatic steps, one would expect that the current is stable (constant) and non-zero).
-        If the (accumulated) change is less than 'epsilon', then cellpy interpret it to be stable.
-        It is expected that different instruments (with different resolution etc.) have different
-        resolutions and noice levels, thus different 'epsilons'.
-
-        Returns: the raw limits (dict)
+        Returns: dictionary containing the unit-fractions for current, charge,
+        and mass
 
         """
-        raise NotImplementedError
-
-    @classmethod
-    def get_params(cls, parameter: Union[str, None]) -> dict:
-        """Retrieves parameters needed for facilitating working with the
-        instrument without registering it.
-
-        Typically, it should include the name and raw_ext.
-
-        Return: parameters or a selected parameter
-        """
-
-        return getattr(cls, parameter)
 
-    @abc.abstractmethod
-    def loader(self, *args, **kwargs) -> list:
-        """Loads data into a Cell object and returns it"""
-        pass
+        raw_units = dict()
+        raw_units["current"] = "A"
+        raw_units["charge"] = "Ah"
+        raw_units["mass"] = "g"
+        raw_units["voltage"] = "V"
+        return raw_units
 
     @staticmethod
-    def identify_last_data_point(data: core.Cell) -> core.Cell:
-        """This method is used to find the last record in the data."""
-        return core.identify_last_data_point(data)
-
-
-class AutoLoader(BaseLoader):
-    """Main autoload class.
-
-    This class can be sub-classed if you want to make a data-reader for different type of "easily parsed" files
-    (for example csv-files). The subclass needs to have at least one
-    associated CONFIGURATION_MODULE defined and must have the following attributes as minimum:
-
-        default_model: str = NICK_NAME_OF_DEFAULT_CONFIGURATION_MODULE
-        supported_models: dict = SUPPORTED_MODELS
-
-    where SUPPORTED_MODELS is a dictionary with {NICK_NAME : CONFIGURATION_MODULE_NAME}  key-value pairs.
-    Remark! the NICK_NAME must be in upper-case!
-
-    It is also possible to set these in a custom pre_init method:
-
-        @classmethod
-        def pre_init(cls):
-            cls.default_model: str = NICK_NAME_OF_DEFAULT_CONFIGURATION_MODULE
-            cls.supported_models: dict = SUPPORTED_MODELS
-
-    or turn off automatic registering of configuration:
-        @classmethod
-        def pre_init(cls):
-            cls.auto_register_config = False  # defaults to True
-
-    During initialisation of the class, if auto_register_config == True,  it will dynamically load the definitions
-    provided in the CONFIGURATION_MODULE.py located in the cellpy.readers.instruments.configurations folder/package.
-
-    """
-
-    name = "auto_loader"
+    def get_raw_limits():
+        """Include the settings for how to decide what kind of
+        step you are examining here.
+
+        The raw limits are 'epsilons' used to check if the current
+        and/or voltage is stable (for example
+        for galvanostatic steps, one would expect that the current
+        is stable (constant) and non-zero).
+        It is expected that different instruments (with different
+        resolution etc.) have different
+        'epsilons'.
 
-    def __init__(self, *args, **kwargs):
-        """Attributes can be set during initialization of the class as **kwargs that are then handled by the
-        ``parse_formatter_parameters`` method.
+        Returns: the raw limits (dict)
 
-        Remark that some also can be provided as arguments to the ``loader`` method and will then automatically
-        be "transparent" to the ``cellpy.get`` function. So if you would like to give the user access to modify
-        these arguments, you should implement them in the ``parse_loader_parameters`` method.
         """
+        raw_limits = dict()
+        raw_limits["current_hard"] = 0.0000000000001
+        raw_limits["current_soft"] = 0.00001
+        raw_limits["stable_current_hard"] = 2.0
+        raw_limits["stable_current_soft"] = 4.0
+        raw_limits["stable_voltage_hard"] = 2.0
+        raw_limits["stable_voltage_soft"] = 4.0
+        raw_limits["stable_charge_hard"] = 2.0
+        raw_limits["stable_charge_soft"] = 5.0
+        raw_limits["ir_change"] = 0.00001
+        return raw_limits
+
+    def inspect(self, run_data):
+        """inspect the file."""
+        return run_data
 
-        self.auto_register_config = True
-        self.pre_init()
-
-        if not hasattr(self, "supported_models"):
-            raise AttributeError(
-                f"missing attribute in sub-class of TxtLoader: supported_models"
-            )
-        if not hasattr(self, "default_model"):
-            raise AttributeError(
-                f"missing attribute in sub-class of TxtLoader: default_model"
-            )
-
-        # in case model is given as argument
-        self.model = kwargs.pop("model", self.default_model)
-        if self.auto_register_config:
-            self.config_params = self.register_configuration()
-
-        self.name = None
-        self._file_path = None
-
-        self.parse_formatter_parameters(**kwargs)
-
-        self.pre_processors = kwargs.pop(
-            "pre_processors", self.config_params.pre_processors
-        )
-        self.post_processors = kwargs.pop(
-            "post_processors", self.config_params.post_processors
-        )
-        self.include_aux = kwargs.pop("include_aux", False)
-        self.keep_all_columns = kwargs.pop("keep_all_columns", False)
-        self.cellpy_headers_normal = (
-            headers_normal  # the column headers defined by cellpy
-        )
+    def repair(self, file_name):
+        """try to repair a broken/corrupted file"""
+        raise NotImplementedError
 
-    @abc.abstractmethod
-    def parse_formatter_parameters(self, **kwargs) -> None:
-        ...
-
-    @abc.abstractmethod
-    def parse_loader_parameters(self, **kwargs):
-        ...
-
-    @abc.abstractmethod
-    def query_file(self, file_path: Union[str, pathlib.Path]) -> pd.DataFrame:
-        ...
-
-    def pre_init(self) -> None:
-        ...
-
-    def register_configuration(self) -> ModelParameters:
-        """Register and load model configuration"""
-        if (
-            self.model is None
-        ):  # in case None was given as argument (model=None in initialisation)
-            self.model = self.default_model
-        model_module_name = self.supported_models.get(self.model.upper(), None)
-        if model_module_name is None:
-            raise Exception(
-                f"The model {self.model} does not have any defined configuration."
-                f"\nCurrent supported models are {[*self.supported_models.keys()]}"
-            )
-        return register_configuration_from_module(self.model, model_module_name)
+    def dump(self, file_name, path):
+        """Dumps the raw file to an intermediate hdf5 file.
 
-    def get_raw_units(self):
-        """Include the settings for the units used by the instrument.
+        This method can be used if the raw file is too difficult to load and it
+        is likely that it is more efficient to convert it to an hdf5 format
+        and then load it using the `from_intermediate_file` function.
 
-        The units are defined w.r.t. the SI units ('unit-fractions'; currently only units that are multiples of
-        Si units can be used). For example, for current defined in mA, the value for the
-        current unit-fraction will be 0.001.
+        Args:
+            file_name: name of the raw file
+            path: path to where to store the intermediate hdf5 file (optional)
 
-        Returns: dictionary containing the unit-fractions for current, charge, and mass
+        Returns:
+            full path to stored intermediate hdf5 file
+            information about the raw file (needed by the
+            `from_intermediate_file` function)
 
         """
-        return self.config_params.raw_units
-
-    def get_raw_limits(self):
-        """Include the settings for how to decide what kind of step you are examining here.
+        raise NotImplementedError
 
-        The raw limits are 'epsilons' used to check if the current and/or voltage is stable (for example
-        for galvanostatic steps, one would expect that the current is stable (constant) and non-zero).
-        It is expected that different instruments (with different resolution etc.) have different
-        'epsilons'.
+    def loader(self, file_name, bad_steps=None, **kwargs):
+        """Loads data from BioLogics mpr files.
 
-        Returns: the raw limits (dict)
+        Args:
+            file_name (str): path to .res file.
+            bad_steps (list of tuples): (c, s) tuples of steps s
+             (in cycle c) to skip loading.
 
+        Returns:
+            new test
         """
-        return self.config_params.raw_limits
+        print("bad steps: %s" % bad_steps)
+        print(f"kwargs: {kwargs}")
+        # self.name = file_name
+
+        # creating temporary file and connection
+        # self.copy_to_temporary()
+        temp_filename = self.temp_file_path
+
+        filesize = os.path.getsize(self.temp_file_path)
+        hfilesize = humanize_bytes(filesize)
+        txt = "File size: %i (%s)" % (filesize, hfilesize)
+        self.logger.debug(txt)
+
+        self.logger.debug("tmp file: %s" % temp_filename)
+        self.logger.debug("HERE WE LOAD THE DATA")
+
+        data = Data()
+        self.generate_fid()
+        data.raw_data_files.append(self.fid)
+
+        # div parameters and information (probably load this last)
+        data.loaded_from = self.name
+
+        # some overall prms
+        data.channel_index = None
+        data.creator = None
+        data.schedule_file_name = None
+        data.start_datetime = None
+        data.test_ID = None
+        data.test_name = None
+
+        # --------- read raw-data (normal-data) -------------------------
+        self.logger.debug("reading raw-data")
+        self.mpr_data = None
+        self.mpr_log = None
+        self.mpr_settings = None
+
+        self._load_mpr_data(temp_filename, bad_steps)
+        length_of_test = self.mpr_data.shape[0]
+        self.logger.debug(f"length of test: {length_of_test}")
+
+        self.logger.debug("renaming columns")
+        self._rename_headers()
+        # ---------  stats-data (summary-data) -------------------------
+        summary_df = self._create_summary_data()
+
+        if summary_df.empty:
+            txt = "\nCould not find any summary (stats-file)!"
+            txt += " (summary_df.empty = True)"
+            txt += "\n -> issue make_summary(use_cellpy_stat_file=False)"
+            warnings.warn(txt)
 
-    @staticmethod
-    def get_headers_aux(raw: pd.DataFrame) -> dict:
-        raise NotImplementedError(
-            f"missing method in sub-class of TxtLoader: get_headers_aux"
-        )
+        data.summary = summary_df
+        data.raw = self.mpr_data
 
-    def _pre_process(self):
-        # create a copy of the file and set file_path attribute
-        temp_dir = pathlib.Path(tempfile.gettempdir())
-        temp_filename = temp_dir / self.name.name
-        shutil.copy2(self.name, temp_dir)
-        logging.debug(f"tmp file: {temp_filename}")
-        self._file_path = temp_filename
-
-        for processor_name in self.pre_processors:
-            if self.pre_processors[processor_name]:
-                if hasattr(pre_processors, processor_name):
-                    logging.critical(f"running pre-processor: {processor_name}")
-                    processor = getattr(pre_processors, processor_name)
-                    self._file_path = processor(self._file_path)
-                else:
-                    raise NotImplementedError(
-                        f"{processor_name} is not currently supported - aborting!"
-                    )
-
-    def loader(self, name: Union[str, pathlib.Path], **kwargs: str) -> List[core.Cell]:
-        """returns a Cell object with loaded data.
+        data.raw_data_files_length.append(length_of_test)
+        self._clean_up(temp_filename)
+        return data
 
-        Loads data from Maccor txt file (csv-ish).
+    def _parse_mpr_log_data(self):
+        for value in bl_log_pos_dtype:
+            key, start, end, dtype = value
+            self.mpr_log[key] = np.frombuffer(  # replaced np.fromstring
+                self.mpr_log["data"][start:], dtype=dtype, count=1
+            )[0]
+            if "a" in dtype:
+                self.mpr_log[key] = self.mpr_log[key].decode("utf8")
+
+        # converting dates
+        date_datetime = ole2datetime(self.mpr_log["Acquisition started on"])
+        self.mpr_log["Start"] = date_datetime
+
+    def _parse_mpr_settings_data(self, settings_mod):
+        tm = time.strptime(settings_mod["date"].decode(), "%m.%d.%y")
+        startdate = datetime.date(tm.tm_year, tm.tm_mon, tm.tm_mday)
+
+        mpr_settings = dict()
+        mpr_settings["start_date"] = startdate
+        mpr_settings["length"] = settings_mod["length"]
+        mpr_settings["end"] = settings_mod["end"]
+        mpr_settings["offset"] = settings_mod["offset"]
+        mpr_settings["version"] = settings_mod["version"]
+        mpr_settings["data"] = settings_mod["data"]
+        self.mpr_settings = mpr_settings
+        return None
+
+    def _get_flag(self, flag_name):
+        if flag_name in self.flags_dict:
+            mask, dtype = self.flags_dict[flag_name]
+            # print(f"flag: {flag_name}, mask: {mask}, dtype: {dtype}")
+            return np.array(self.mpr_data["flags"] & mask, dtype=dtype)
+        # elif flag_name in self.flags2_dict:
+        #     mask, dtype = self.flags2_dict[flag_name]
+        #     return np.array(self.mpr_data['flags2'] & mask, dtype=dtype)
+        else:
+            raise AttributeError("Flag '%s' not present" % flag_name)
 
-        Args:
-            name (str, pathlib.Path): name of the file.
-            kwargs (dict): key-word arguments from raw_loader.
+    def _load_mpr_data(self, filename, bad_steps):
+        if bad_steps is not None:
+            warnings.warn("Exluding bad steps is not implemented")
+
+        stats_info = os.stat(filename)
+        mpr_modules = []
+
+        mpr_log = None
+        mpr_data = None
+        mpr_settings = None
+
+        file_obj = open(filename, mode="rb")
+        label = file_obj.read(len(mpr_label))
+        self.logger.debug(f"label: {label}")
+        counter = 0
+        while True:
+            counter += 1
+            new_module = _read_modules(file_obj)
+            position = int(new_module["end"])
+            mpr_modules.append(new_module)
+            if position >= stats_info.st_size:
+                txt = "-reached end of file"
+                if position == stats_info.st_size:
+                    txt += " --exactly at end of file"
+                self.logger.info(txt)
+                break
 
-        Returns:
-            new_tests (list of data objects)
-        """
-        self._file_path = pathlib.Path(name)
-        self.name = pathlib.Path(name)
-        pre_processor_hook = kwargs.pop("pre_processor_hook", None)
-        new_tests = []
-
-        if self.pre_processors:
-            self._pre_process()
-
-        self.parse_loader_parameters(**kwargs)
-
-        data_df = self.query_file(self._file_path)
-
-        if pre_processor_hook is not None:
-            logging.debug("running pre-processing-hook")
-            data_df = pre_processor_hook(data_df)
-
-        data = core.Cell()
-
-        # metadata
-        meta = self.parse_meta()
-        data.loaded_from = name
-        data.channel_index = meta.get("channel_index", None)
-        data.test_ID = meta.get("test_ID", None)
-        data.test_name = meta.get("test_name", None)
-        data.channel_number = meta.get("channel_number", None)
-        data.creator = meta.get("creator", None)
-        data.item_ID = meta.get("item_ID", None)
-        data.schedule_file_name = meta.get("schedule_file_name", None)
-        data.start_datetime = meta.get("start_datetime", None)
-
-        # Generating a FileID project:
-        fid = core.FileID(name)
-        data.raw_data_files.append(fid)
-
-        data.raw = data_df
-        data.raw_data_files_length.append(len(data_df))
-        data.summary = (
-            pd.DataFrame()
-        )  # creating an empty frame - loading summary is not implemented
-        data = self._post_process(data)
-        data = self.identify_last_data_point(data)
-        if data.start_datetime is None:
-            data.start_datetime = data.raw[headers_normal.datetime_txt].iat[0]
-
-        data = self.validate(data)
-        new_tests.append(data)
-        return new_tests
+        file_obj.close()
 
-    def validate(self, data: core.Cell) -> core.Cell:
-        """validation of the loaded data, should raise an appropriate exception if it fails."""
+        # ------------- set -----------------------------------
+        settings_mod = None
+        for m in mpr_modules:
+            if m["shortname"].strip().decode() == "VMP Set":
+                settings_mod = m
+                break
+        if settings_mod is None:
+            raise IOError("No settings-module found!")
 
-        logging.debug(f"no validation of defined in this sub-class of TxtLoader")
-        return data
+        self._parse_mpr_settings_data(settings_mod)
 
-    def parse_meta(self) -> dict:
-        """method that parses the data for meta-data (e.g. start-time, channel number, ...)"""
+        # ------------- data -----------------------------------
+        data_module = None
+        for m in mpr_modules:
+            if m["shortname"].strip().decode() == "VMP data":
+                data_module = m
+        if data_module is None:
+            raise IOError("No data module!")
+
+        data_version = data_module["version"]
+        n_data_points = np.fromstring(data_module["data"][:4], dtype="<u4")[0]
+        n_columns = np.fromstring(data_module["data"][4:5], dtype="u1")[0]
+        logging.debug(f"data (points, cols): {n_data_points}, {n_columns}")
+
+        if data_version == 0:
+            logging.debug("data version 0")
+            column_types = np.fromstring(
+                data_module["data"][5:], dtype="u1", count=n_columns
+            )
 
-        logging.debug(
-            f"no parsing method for meta-data defined in this sub-class of TxtLoader"
-        )
-        return dict()
+            remaining_headers = data_module["data"][5 + n_columns : 100]
+            main_data = data_module["data"][100:]
 
-    def _post_rename_headers(self, data):
-        if self.include_aux:
-            new_aux_headers = self.get_headers_aux(data.raw)
-            data.raw.rename(index=str, columns=new_aux_headers, inplace=True)
-        return data
+        elif data_version == 2:
+            logging.debug("data version 2")
+            column_types = np.fromstring(
+                data_module["data"][5:], dtype="<u2", count=n_columns
+            )
+            main_data = data_module["data"][405:]
+            remaining_headers = data_module["data"][5 + 2 * n_columns : 405]
 
-    def _post_process(self, data):
-        # ordered post-processing steps:
-        for processor_name in ORDERED_POST_PROCESSING_STEPS:
-            if processor_name in self.post_processors:
-                data = self._perform_post_process_step(data, processor_name)
-
-        # non-ordered post-processing steps
-        for processor_name in self.post_processors:
-            if processor_name not in ORDERED_POST_PROCESSING_STEPS:
-                data = self._perform_post_process_step(data, processor_name)
-        return data
+        else:
+            raise IOError("Unrecognised version for data module: %d" % data_version)
 
-    def _perform_post_process_step(self, data, processor_name):
-        if self.post_processors[processor_name]:
-            if hasattr(post_processors, processor_name):
-                logging.critical(f"running post-processor: {processor_name}")
-                processor = getattr(post_processors, processor_name)
-                data = processor(data, self.config_params)
-                if hasattr(self, f"_post_{processor_name}"):  # internal addon-function
-                    _processor = getattr(self, f"_post_{processor_name}")
-                    data = _processor(data)
-            else:
-                raise NotImplementedError(
-                    f"{processor_name} is not currently supported - aborting!"
-                )
-        return data
+        whats_left = remaining_headers.strip(b"\x00").decode("utf8")
+        if whats_left:
+            self.logger.debug("UPS! you have some columns left")
+            self.logger.debug(whats_left)
 
+        dtype_dict = OrderedDict()
+        flags_dict = OrderedDict()
 
-class TxtLoader(AutoLoader, ABC):
-    """Main txt loading class (for sub-classing).
+        for col in column_types:
+            if col in bl_flags.keys():
+                flags_dict[bl_flags[col][0]] = bl_flags[col][1]
 
-    The subclass of a TxtLoader gets its information by loading model specifications from its respective module
-    (``cellpy.readers.instruments.configurations.<module>``) or configuration file (yaml).
+            dtype_dict[bl_dtypes[col][1]] = bl_dtypes[col][0]
 
-    Remark that if you implement automatic loading of the formatter, the module / yaml-file must include all
-    the required formatter parameters (sep, skiprows, header, encoding, decimal, thousands).
+        self.dtype_dict = dtype_dict
+        self.flags_dict = flags_dict
 
-    If you need more flexibility, try using the CustomTxtLoader or subclass directly from AutoLoader or Loader.
-
-    Constructor **kwargs:
-        model (str): short name of the (already implemented) sub-model.
-        sep (str): delimiter.
-        skiprows (int): number of lines to skip.
-        header (int): number of the header lines.
-        encoding (str): encoding.
-        decimal (str): character used for decimal in the raw data, defaults to '.'.
-        processors (dict): pre-processing steps to take (before loading with pandas).
-        post_processors (dict): post-processing steps to make after loading the data, but before
-            returning them to the caller.
-        include_aux (bool): also parse so-called auxiliary columns / data. Defaults to False.
-        keep_all_columns (bool): load all columns, also columns that are not 100% necessary for ``cellpy`` to work.
-            Remark that the configuration settings for the sub-model must include a list of column header names
-            that should be kept if keep_all_columns is False (default).
-
-    Module - loader **kwargs:
-        sep (str): the delimiter (also works as a switch to turn on/off automatic detection of delimiter and
-            start of data (skiprows)).
-
-    """
-
-    name = "txt_loader"
-    raw_ext = "*"
-
-    # override this if needed
-    def parse_loader_parameters(self, **kwargs):
-        sep = kwargs.get("sep", None)
-        if sep is not None:
-            self.sep = sep
-        if self.sep is None:
-            self._auto_formatter()
-
-    # override this if needed
-    def parse_formatter_parameters(self, **kwargs):
-        logging.debug(f"model: {self.model}")
-        if not self.config_params.formatters:
-            # Setting defaults if formatter is not loaded
-            logging.debug("No formatter given - using default values.")
-            self.sep = kwargs.pop("sep", None)
-            self.skiprows = kwargs.pop("skiprows", 0)
-            self.header = kwargs.pop("header", 0)
-            self.encoding = kwargs.pop("encoding", "utf-8")
-            self.decimal = kwargs.pop("decimal", ".")
-            self.thousands = kwargs.pop("thousands", None)
+        dtype = np.dtype(list(dtype_dict.items()))
 
-        else:
-            # Remark! This will break if one of these parameters are missing
-            # (not a keyword argument and not within the configuration):
-            self.sep = kwargs.pop("sep", self.config_params.formatters["sep"])
-            self.skiprows = kwargs.pop(
-                "skiprows", self.config_params.formatters["skiprows"]
-            )
-            self.header = kwargs.pop("header", self.config_params.formatters["header"])
-            self.encoding = kwargs.pop(
-                "encoding", self.config_params.formatters["encoding"]
+        p = dtype.itemsize
+        if not p == (len(main_data) / n_data_points):
+            self.logger.info(
+                "WARNING! You have defined %i bytes, "
+                "but it seems it should be %i" % (p, len(main_data) / n_data_points)
             )
-            self.decimal = kwargs.pop(
-                "decimal", self.config_params.formatters["decimal"]
+        bulk = main_data
+        bulk_data = np.fromstring(bulk, dtype=dtype)
+        mpr_data = pd.DataFrame(bulk_data)
+
+        self.logger.debug(mpr_data.columns)
+        self.logger.debug(mpr_data.head())
+
+        # ------------- log  -----------------------------------
+        log_module = None
+        for m in mpr_modules:
+            if m["shortname"].strip().decode() == "VMP LOG":
+                log_module = m
+        if log_module is None:
+            txt = "error - no log module"
+            raise IOError(txt)
+
+        tm = time.strptime(log_module["date"].decode(), "%m.%d.%y")
+        enddate = datetime.date(tm.tm_year, tm.tm_mon, tm.tm_mday)
+
+        mpr_log = dict()
+        mpr_log["end_date"] = enddate
+        mpr_log["length2"] = log_module["length"]
+        mpr_log["end2"] = log_module["end"]
+        mpr_log["offset2"] = log_module["offset"]
+        mpr_log["version2"] = log_module["version"]
+        mpr_log["data"] = log_module[
+            "data"
+        ]  # Not sure if I will ever need it, but just in case....
+        self.mpr_log = mpr_log
+        self._parse_mpr_log_data()
+        self.mpr_data = mpr_data
+
+    def _rename_header(self, h_old, h_new):
+        try:
+            self.mpr_data.rename(
+                columns={h_new: self.cellpy_headers[h_old]}, inplace=True
             )
-            self.thousands = kwargs.pop(
-                "thousands", self.config_params.formatters["thousands"]
-            )
-        logging.debug(
-            f"Formatters: self.sep={self.sep} self.skiprows={self.skiprows} self.header={self.header} self.encoding={self.encoding}"
-        )
-        logging.debug(
-            f"Formatters (cont.): self.decimal={self.decimal} self.thousands={self.thousands}"
-        )
-
-    def _auto_formatter(self):
-        separator, first_index = find_delimiter_and_start(
-            self.name,
-            separators=None,
-            checking_length_header=100,
-            checking_length_whole=200,
-        )
-        self.encoding = "UTF-8"  # consider adding a find_encoding function
-        self.sep = separator
-        self.skiprows = first_index - 1
-        self.header = 0
+        except KeyError as e:
+            # warnings.warn(f"KeyError {e}")
+            self.logger.info(f"Problem during conversion to cellpy-format ({e})")
+
+    def _generate_cycle_index(self):
+        flag = "Ns changes"
+        n = self._get_flag(flag)
+        self.mpr_data[self.cellpy_headers["cycle_index_txt"]] = 1
+        ns_changes = self.mpr_data[n].index.values
+        for i in ns_changes:
+            self.mpr_data.loc[i:, self.cellpy_headers["cycle_index_txt"]] += 1
+
+    def _generate_datetime(self):
+        start_date = self.mpr_settings["start_date"]
+        start_datetime = self.mpr_log["Start"]
+        cellpy_header_txt = "datetime_txt"
+        date_format = "%Y-%m-%d %H:%M:%S"  # without microseconds
+        self.mpr_data[self.cellpy_headers[cellpy_header_txt]] = [
+            start_datetime + datetime.timedelta(seconds=n)
+            for n in self.mpr_data["time"].values
+        ]
+        # self.mpr_data[self.cellpy_headers[cellpy_header_txt]]
+        # .start_date.strftime(date_format)
+        # TODO: @jepe - currently storing as datetime object
+        # (while for arbindata it is stored as str)
+
+    def _generate_step_index(self):
+        # TODO: @jepe - check and optionally fix me
+        cellpy_header_txt = "step_index_txt"
+        biologics_header_txt = "flags2"
+        self._rename_header(cellpy_header_txt, biologics_header_txt)
+        self.mpr_data[self.cellpy_headers[cellpy_header_txt]] += 1
+
+    def _generate_step_time(self):
+        # TODO: @jepe - fix me
+        self.mpr_data[self.cellpy_headers["step_time_txt"]] = np.nan
+
+    def _generate_sub_step_time(self):
+        # TODO: @jepe - fix me
+        self.mpr_data[self.cellpy_headers["sub_step_time_txt"]] = np.nan
+
+    def _generate_capacities(self):
+        cap_col = self.mpr_data["QChargeDischarge"]
+        self.mpr_data[self.cellpy_headers["discharge_capacity_txt"]] = [
+            0.0 if x < 0 else x for x in cap_col
+        ]
+        self.mpr_data[self.cellpy_headers["charge_capacity_txt"]] = [
+            0.0 if x >= 0 else x for x in cap_col
+        ]
+
+    def _rename_headers(self):
+        # should ideally use the info from bl_dtypes, will do that later
+
+        self.mpr_data[self.cellpy_headers["internal_resistance_txt"]] = np.nan
+        self.mpr_data[self.cellpy_headers["data_point_txt"]] = np.arange(
+            1, self.mpr_data.shape[0] + 1, 1
+        )
+        self._generate_datetime()
+        self._generate_cycle_index()
+
+        self._generate_step_time()
+        self._generate_sub_step_time()
+        self._generate_step_index()
+        self._generate_capacities()
+
+        # simple renaming of column headers for the rest
+        self._rename_header("frequency_txt", "freq")
+        self._rename_header("voltage_txt", "Ewe")
+        self._rename_header("current_txt", "I")
+        self._rename_header("aci_phase_angle_txt", "phaseZ")
+        self._rename_header("amplitude_txt", "absZ")
+        self._rename_header("ref_voltage_txt", "Ece")
+        self._rename_header("ref_aci_phase_angle_txt", "phaseZce")
+        self._rename_header("test_time_txt", "time")
+
+        self.mpr_data[self.cellpy_headers["sub_step_index_txt"]] = self.mpr_data[
+            self.cellpy_headers["step_index_txt"]
+        ]
+
+    def _create_summary_data(self):
+        # Summary data should contain datapoint-number
+        # for last point in the cycle. It must also contain
+        # capacity
+        df_summary = pd.DataFrame()
+        mpr_log = self.mpr_log
+        mpr_settings = self.mpr_settings
+        # TODO: @jepe - finalise making summary of mpr-files
+        # after figuring out steps etc
+        warnings.warn(
+            "Creating summary data for biologics mpr-files" " is not implemented yet"
+        )
+        self.logger.info(mpr_settings)
+        self.logger.info(mpr_log)
+        start_date = mpr_settings["start_date"]
+        self.logger.info(start_date)
+        return df_summary
+
+    def __raw_export(self, filename, df):
+        filename_out = os.path.splitext(filename)[0] + "_test_out.csv"
+        print("\n--------EXPORTING----------------------------")
+        print(filename)
+        print("->")
+        print(filename_out)
+        df.to_csv(filename_out, sep=";")
+        print("------OK--------------------------------------")
+
+    def _clean_up(self, tmp_filename):
+        if os.path.isfile(tmp_filename):
+            try:
+                os.remove(tmp_filename)
+            except WindowsError as e:
+                self.logger.warning(
+                    "could not remove tmp-file\n%s %s" % (tmp_filename, e)
+                )
+        pass
 
-        logging.critical(
-            f"auto-formatting:\n  {self.sep=}\n  {self.skiprows=}\n  {self.header=}\n  {self.encoding=}\n"
-        )
 
-    # override this if using other query functions
-    def query_file(self, name):
-        logging.debug(f"parsing with pandas.read_csv: {name}")
-        logging.critical(
-            f"{self.sep=}, {self.skiprows=}, {self.header=}, {self.encoding=}, {self.decimal=}"
-        )
-        data_df = pd.read_csv(
-            name,
-            sep=self.sep,
-            skiprows=self.skiprows,
-            header=self.header,
-            encoding=self.encoding,
-            decimal=self.decimal,
-            thousands=self.thousands,
-        )
-        return data_df
+if __name__ == "__main__":
+    import logging
+    import os
+    import sys
+
+    from cellpy import cellreader, log
+
+    # -------- defining overall path-names etc ----------
+    current_file_path = os.path.dirname(os.path.realpath(__file__))
+    # relative_test_data_dir = "../cellpy/data_ex"
+    relative_test_data_dir = "../../../testdata"
+    relative_out_data_dir = "../../../dev_data"
+    test_data_dir = os.path.abspath(
+        os.path.join(current_file_path, relative_test_data_dir)
+    )
+    test_data_dir_out = os.path.abspath(
+        os.path.join(current_file_path, relative_out_data_dir)
+    )
+    test_data_dir_raw = os.path.join(test_data_dir, "data")
+    if not os.path.isdir(test_data_dir_raw):
+        print(f"Could not find {test_data_dir_raw}")
+        sys.exit(-23)
+    if not os.path.isdir(test_data_dir_out):
+        sys.exit(-24)
+
+    if not os.path.isdir(os.path.join(test_data_dir_out, "out")):
+        os.mkdir(os.path.join(test_data_dir_out, "out"))
+    test_data_dir_out = os.path.join(test_data_dir_out, "out")
+
+    test_raw_file = "biol.mpr"
+    test_raw_file_full = os.path.join(test_data_dir_raw, test_raw_file)
+
+    test_data_dir_cellpy = os.path.join(test_data_dir, "hdf5")
+    test_cellpy_file = "geis.h5"
+    test_cellpy_file_tmp = "tmpfile.h5"
+    test_cellpy_file_full = os.path.join(test_data_dir_cellpy, test_cellpy_file)
+    test_cellpy_file_tmp_full = os.path.join(test_data_dir_cellpy, test_cellpy_file_tmp)
+
+    raw_file_name = test_raw_file_full
+    print("\n======================mpr-dev===========================")
+    print(f"Test-file: {raw_file_name}")
+    log.setup_logging(default_level="DEBUG")
+    instrument = "biologics_mpr"
+    cellpy_data_instance = cellreader.CellpyCell()
+    cellpy_data_instance.set_instrument(instrument=instrument)
+    print("starting to load the file")
+    cellpy_data_instance.from_raw(raw_file_name)
+    print("printing cellpy instance:")
+    print(cellpy_data_instance)
+
+    print("---make step table")
+    cellpy_data_instance.make_step_table()
+
+    print("---make summary")
+    cellpy_data_instance.make_summary()
+
+    print("---saving to csv")
+    try:
+        temp_dir = tempfile.mkdtemp()
+        cellpy_data_instance.to_csv(datadir=temp_dir)
+        cellpy_data_instance.to_csv(datadir=test_data_dir_out)
+        print("---saving to hdf5")
+        print("NOT YET")
+    finally:
+        shutil.rmtree(temp_dir)
+
+
+# dtype = dtype([('flags', 'u1'), ('time/s', '<f8'), ('Ewe/V', '<f4'), ('dQ/mA.h', '<f8'),
+#        ('I/mA', '<f4'), ('Ece/V', '<f4'), ('(Q-Qo)/mA.h', '<f8'), ('20', '<f8'),
+#        ('freq/Hz', '<f4'), ('Phase(Z)/deg', '<f4'), ('|Z|/Ohm', '<f4'),
+#        ('I Range', '<u2'), ('74', '<f8'), ('96', '<f8'), ('98', '<f8'),
+#        ('99', '<f8'), ('100', '<f8'), ('101', '<f8'), ('123', '<f8'),
+#        ('124', '<f8'), ('Capacitance charge/F', '<f8'), ('Capacitance discharge/F', '<f8'),
+#        ('Ns', '<u2'), ('430', '<f8'), ('431', '<f8'), ('432', '<f8'), ('433', '<f8'),
+#        ('Q charge/discharge/mA.h', '<f8'), ('half cycle', '<u4'), ('469', '<f8'),
+#        ('471', '<f8')])
+#
+# flags = OrderedDict(
+#     [
+#         ('mode', (3, <class 'numpy.uint8'>)),('ox/red', (4, <class 'numpy.bool_'>)),
+#         ('error', (8, <class 'numpy.bool_'>)), ('control changes', (16, <class 'numpy.bool_'>)),
+#         ('Ns changes', (32, <class 'numpy.bool_'>)), ('counter inc.', (128, <class 'numpy.bool_'>))]
+# )
+# flags2 = OrderedDict()
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/configurations/__init__.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/configurations/__init__.py`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/configurations/maccor_txt_four.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/configurations/maccor_txt_two.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-# Works for data from KIT (SIMBA project)
-
-file_info = {"raw_extension": "txt"}
+# Works for data from KIT (SIMBA project) with ',' as decimal and dropping bad last rows.
 
 unit_labels = {
     "resistance": "Ohms",
     "time": "s",
     "current": "A",
     "voltage": "V",
     "power": "W",
     "capacity": "Ah",
     "energy": "Wh",
     "temperature": "C",
 }
 
+file_info = {"raw_extension": "txt"}
+
 normal_headers_renaming_dict = {
     "data_point_txt": "Rec",
     "cycle_index_txt": "Cycle C",
     "step_index_txt": "Step",
     "test_time_txt": "TestTime",
     "step_time_txt": "StepTime",
     "charge_capacity_txt": "Cap. [Ah]",
@@ -30,15 +30,15 @@
     "column_name": "Md",
     "charge_keys": ["C"],
     "discharge_keys": ["D"],
     "rest_keys": ["R"],
 }
 
 
-raw_units = {"current": 1.0, "charge": 1.0, "mass": 1.0, "voltage": 1.0}
+raw_units = {"current": "A", "charge": "Ah", "mass": "g", "voltage": "V"}
 
 raw_limits = {
     "current_hard": 0.000_000_000_000_1,
     "current_soft": 0.000_01,
     "stable_current_hard": 2.0,
     "stable_current_soft": 4.0,
     "stable_voltage_hard": 2.0,
@@ -63,9 +63,10 @@
 
 post_processors = {
     "split_capacity": True,
     "split_current": True,
     "set_index": True,
     "rename_headers": True,
     "remove_last_if_bad": True,
+    "set_cycle_number_not_zero": True,
     "convert_date_time_to_datetime": True,
 }
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/configurations/maccor_txt_one.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/configurations/maccor_txt_one.py`

 * *Files 3% similar despite different names*

```diff
@@ -85,15 +85,15 @@
 states = {
     "column_name": "State",
     "charge_keys": ["C"],
     "discharge_keys": ["D"],
     "rest_keys": ["R"],
 }
 
-raw_units = {"current": 1.0, "charge": 1.0, "mass": 1.0, "voltage": 1.0}
+raw_units = {"current": "A", "charge": "Ah", "mass": "g", "voltage": "V"}
 
 raw_limits = {
     "current_hard": 0.000_000_000_000_1,
     "current_soft": 0.000_01,
     "stable_current_hard": 2.0,
     "stable_current_soft": 4.0,
     "stable_voltage_hard": 2.0,
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/configurations/maccor_txt_three.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/configurations/maccor_txt_three.py`

 * *Files 11% similar despite different names*

```diff
@@ -37,15 +37,15 @@
 states = {
     "column_name": "State",
     "charge_keys": ["C"],
     "discharge_keys": ["D"],
     "rest_keys": ["R"],
 }
 
-raw_units = {"current": 1.0, "charge": 1.0, "mass": 1.0, "voltage": 1.0}
+raw_units = {"current": "A", "charge": "Ah", "mass": "g", "voltage": "V"}
 
 # raw_limits = {
 #     "current_hard": 0.000_000_000_000_1,
 #     "current_soft": 0.000_01,
 #     "stable_current_hard": 2.0,
 #     "stable_current_soft": 4.0,
 #     "stable_voltage_hard": 2.0,
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/configurations/maccor_txt_two.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/configurations/maccor_txt_four.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-# Works for data from KIT (SIMBA project) with ',' as decimal and dropping bad last rows.
+# Works for data from KIT (SIMBA project)
+
+file_info = {"raw_extension": "txt"}
 
 unit_labels = {
     "resistance": "Ohms",
     "time": "s",
     "current": "A",
     "voltage": "V",
     "power": "W",
     "capacity": "Ah",
     "energy": "Wh",
     "temperature": "C",
 }
 
-file_info = {"raw_extension": "txt"}
-
 normal_headers_renaming_dict = {
     "data_point_txt": "Rec",
     "cycle_index_txt": "Cycle C",
     "step_index_txt": "Step",
     "test_time_txt": "TestTime",
     "step_time_txt": "StepTime",
     "charge_capacity_txt": "Cap. [Ah]",
@@ -30,15 +30,15 @@
     "column_name": "Md",
     "charge_keys": ["C"],
     "discharge_keys": ["D"],
     "rest_keys": ["R"],
 }
 
 
-raw_units = {"current": 1.0, "charge": 1.0, "mass": 1.0, "voltage": 1.0}
+raw_units = {"current": "A", "charge": "Ah", "mass": "g", "voltage": "V"}
 
 raw_limits = {
     "current_hard": 0.000_000_000_000_1,
     "current_soft": 0.000_01,
     "stable_current_hard": 2.0,
     "stable_current_soft": 4.0,
     "stable_voltage_hard": 2.0,
@@ -63,10 +63,9 @@
 
 post_processors = {
     "split_capacity": True,
     "split_current": True,
     "set_index": True,
     "rename_headers": True,
     "remove_last_if_bad": True,
-    "set_cycle_number_not_zero": True,
     "convert_date_time_to_datetime": True,
 }
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/configurations/maccor_txt_zero.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/configurations/maccor_txt_zero.py`

 * *Files 8% similar despite different names*

```diff
@@ -85,15 +85,15 @@
 states = {
     "column_name": "State",
     "charge_keys": ["C"],
     "discharge_keys": ["D"],
     "rest_keys": ["R"],
 }
 
-raw_units = {"current": 1.0, "charge": 1.0, "mass": 1.0, "voltage": 1.0}
+raw_units = {"current": "A", "charge": "Ah", "mass": "g", "voltage": "V"}
 
 raw_limits = {
     "current_hard": 0.000_000_000_000_1,
     "current_soft": 0.000_01,
     "stable_current_hard": 2.0,
     "stable_current_soft": 4.0,
     "stable_voltage_hard": 2.0,
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/configurations/neware_txt_one.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/configurations/neware_txt_zero.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,39 +1,34 @@
-# TODO: Edit to CENATE format (put on hold until exact format has been decided)
+file_info = {"raw_extension": "csv"}
 
-file_info = {"raw_extension": "txt"}
-
-# TODO: rename this to raw_units for v1.0.0 and copy the file to master
-raw_unit_labels = {
+raw_units = {
     "current": "A",
     "charge": "Ah",
     "mass": "g",
     "voltage": "V",
     "energy": "Wh",
     "power": "W",
     "resistance": "Ohm",
 }
 
-raw_units = {"current": 1.0, "charge": 1.0, "mass": 1.0, "voltage": 1.0}
-
 normal_headers_renaming_dict = {
     "data_point_txt": f"DataPoint",
     "cycle_index_txt": f"Cycle Index",
     "step_index_txt": f"Step Index",
-    "current_txt": f"Current({raw_unit_labels['current']})",
-    "voltage_txt": f"Voltage({raw_unit_labels['voltage']})",
-    "charge_capacity_txt": f"Chg. Cap.({raw_unit_labels['charge']})",
-    "charge_energy_txt": f"Chg. Energy({raw_unit_labels['energy']})",
-    "discharge_capacity_txt": f"DChg. Cap.({raw_unit_labels['charge']})",
-    "discharge_energy_txt": f"DChg. Energy({raw_unit_labels['energy']})",
+    "current_txt": f"Current({raw_units['current']})",
+    "voltage_txt": f"Voltage({raw_units['voltage']})",
+    "charge_capacity_txt": f"Chg. Cap.({raw_units['charge']})",
+    "charge_energy_txt": f"Chg. Energy({raw_units['energy']})",
+    "discharge_capacity_txt": f"DChg. Cap.({raw_units['charge']})",
+    "discharge_energy_txt": f"DChg. Energy({raw_units['energy']})",
     "datetime_txt": f"Date",
     "step_time_txt": f"Time",
     "dq_dv_txt": f"dQ/dV(mAh/V)",
     "internal_resistance_txt": f"Contact resistance(mO)",
-    "power_txt": f"Power({raw_unit_labels['power']})",
+    "power_txt": f"Power({raw_units['power']})",
     "test_time_txt": f"Cumulative Time",
 }
 
 states = {
     "column_name": "Step Type",
     "charge_keys": ["CC Chg"],
     "discharge_keys": ["CC DChg"],
@@ -50,17 +45,17 @@
     "stable_voltage_soft": 4.0,
     "stable_charge_hard": 0.001,
     "stable_charge_soft": 5.0,
     "ir_change": 0.00001,
 }
 
 formatters = {
-    "skiprows": 0,
-    "sep": "\t",  #
-    "header": 5,  # will not be used since auto is on
+    "skiprows": 0,  # will not be used since auto is on
+    "sep": None,  # comma for UiO at the moment, but using auto instead
+    "header": 0,  # will not be used since auto is on
     "encoding": "ISO-8859-1",  # will not be used since auto is on
     "decimal": ".",
     "thousands": None,
 }
 
 
 post_processors = {
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/custom.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/custom.py`

 * *Files 1% similar despite different names*

```diff
@@ -22,15 +22,15 @@
     register_local_configuration_from_yaml_file,
 )
 
 
 class DataLoader(AutoLoader, ABC):
     """Class for loading data from txt files."""
 
-    name = "custom"
+    instrument_name = "custom"
     raw_ext = "*"
 
     def __init__(self, instrument_file=None, **kwargs):
         if instrument_file is None:
             logging.debug("No instrument_file provided - checking default")
             instrument_file = prms.Instruments.custom_instrument_definitions_file
         if not instrument_file:
@@ -246,21 +246,21 @@
 
 def _process_cellpy_object(name, c, out):
     import matplotlib.pyplot as plt
 
     pd.options.display.max_columns = 100
 
     print(f"loaded the file - now lets see what we got")
-    raw = c.cell.raw
+    raw = c.data.raw
     raw.to_clipboard()
     print(raw.head())
     c.make_step_table()
 
-    steps = c.cell.steps
-    summary = c.cell.summary
+    steps = c.data.steps
+    summary = c.data.summary
 
     raw.to_csv(out / "raw.csv", sep=";")
     steps.to_csv(out / "steps.csv", sep=";")
     summary.to_csv(out / "summary.csv", sep=";")
 
     fig_1, (ax1, ax2, ax3, ax4) = plt.subplots(
         4,
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/ext_nda_reader.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/ext_nda_reader.py`

 * *Files 14% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 import os
 
 import pandas as pd
 
 from cellpy import prms
 from cellpy.parameters.internal_settings import get_headers_normal
 from cellpy.readers.core import (
-    Cell,
+    Data,
     FileID,
     check64bit,
     humanize_bytes,
     xldate_as_datetime,
 )
 from cellpy.readers.instruments.base import BaseLoader
 
@@ -26,19 +26,22 @@
 # check
 
 
 def load_nda(*args, **kwargs):
     print("dummy function (mock)")
     print(args)
     print(kwargs)
+    return None, None
 
 
 class DataLoader(BaseLoader):
     """Class for using the NDA loader by Frederik Huld (Beyonder)."""
 
+    instrument_name = "neware_nda"
+
     def __init__(self, *args, **kwargs):
         """initiates the NdaLoader class"""
         # could use __init__(self, cellpydata_object) and
         # set self.logger = cellpydata_object.logger etc.
         # then remember to include that as prm in "out of class" functions
         # self.prms = prms
         self.logger = logging.getLogger(__name__)
@@ -78,59 +81,42 @@
         Returns: the raw limits (dict)
 
         """
         raise NotImplementedError
 
     def loader(self, file_name, *args, **kwargs):
         """Loads data into a DataSet object and returns it"""
-
-        new_tests = []
-
+        # self.name = file_name
+        # self.copy_to_temporary()
         test_no = 1
         channel_index = 1
-        channel_number = 1
         creator = "no name"
-        item_ID = 1
         schedule_file_name = "no name"
         start_datetime = "2020.02.24 14:58:00"
         test_ID = 1
         test_name = "no name"
 
-        if not os.path.isfile(file_name):
-            self.logger.info("Missing file_\n   %s" % file_name)
-            return None
-
         self.logger.debug("in loader")
         self.logger.debug("filename: %s" % file_name)
 
-        filesize = os.path.getsize(file_name)
-        hfilesize = humanize_bytes(filesize)
-        txt = "Filesize: %i (%s)" % (filesize, hfilesize)
-        self.logger.debug(txt)
-
-        data = Cell()
-        data.cell_no = test_no
+        data = Data()
         data.loaded_from = file_name
-        fid = FileID(file_name)
+        self.generate_fid()
+        data.raw_data_files.append(self.fid)
         data.channel_index = channel_index
-        data.channel_number = channel_number
         data.creator = creator
-        data.item_ID = item_ID
         data.schedule_file_name = schedule_file_name
         data.start_datetime = start_datetime
         data.test_ID = test_ID
         data.test_name = test_name
-        data.raw_data_files.append(fid)
 
         length_of_test, normal_df = load_nda()
 
         data.summary = pd.DataFrame()
 
         data.raw = normal_df
         data.raw_data_files_length.append(length_of_test)
 
         data = self._post_process(data)
         data = self.identify_last_data_point(data)
 
-        new_tests.append(data)
-
-        return new_tests
+        return data
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/loader_specific_modules/biologic_file_format.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/loader_specific_modules/biologic_file_format.py`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/local_instrument.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/local_instrument.py`

 * *Files 15% similar despite different names*

```diff
@@ -7,15 +7,15 @@
     register_local_configuration_from_yaml_file,
 )
 
 
 class DataLoader(TxtLoader):
     """Class for loading data from txt files."""
 
-    name = "local_instrument"
+    instrument_name = "local_instrument"
     raw_ext = "*"
 
     def __init__(self, instrument_file=None, **kwargs):
         self.local_instrument_file = instrument_file
         super().__init__()
 
     default_model = None
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/maccor_txt.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/maccor_txt.py`

 * *Files 2% similar despite different names*

```diff
@@ -40,15 +40,15 @@
     headers_normal.discharge_capacity_txt,
 ]
 
 
 class DataLoader(TxtLoader):
     """Class for loading data from Maccor txt files."""
 
-    name = "maccor_txt"
+    instrument_name = "maccor_txt"
     raw_ext = "txt"
 
     default_model = prms.Instruments.Maccor["default_model"]  # Required
     supported_models = SUPPORTED_MODELS  # Required
 
     @staticmethod
     def get_headers_aux(raw):
@@ -196,26 +196,26 @@
     datadir = pathlib.Path(
         r"C:\scripts\cellpy_dev_resources\2021_leafs_data\Charge-Discharge\Maccor series 4000"
     )
     name = datadir / "01_UBham_M50_Validation_0deg_01.txt"
     out = pathlib.Path(r"C:\scripts\notebooks\Div")
     print(f"Exists? {name.is_file()}")
 
-    c = cellreader.CellpyData()
+    c = cellreader.CellpyCell()
     c.set_instrument("maccor_txt", sep="\t")
 
     c.from_raw(name)
     c.set_mass(1000)
 
     c.make_step_table()
     c.make_summary()
 
-    raw = c.cell.raw
-    steps = c.cell.steps
-    summary = c.cell.summary
+    raw = c.data.raw
+    steps = c.data.steps
+    summary = c.data.summary
     raw.to_csv(r"C:\scripts\notebooks\Div\trash\raw.csv", sep=";")
     steps.to_csv(r"C:\scripts\notebooks\Div\trash\steps.csv", sep=";")
     summary.to_csv(r"C:\scripts\notebooks\Div\trash\summary.csv", sep=";")
 
     fig_1, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(6, 10))
     raw.plot(x="test_time", y="voltage", ax=ax1)
     raw.plot(x="test_time", y=["charge_capacity", "discharge_capacity"], ax=ax3)
@@ -273,17 +273,17 @@
     print(f"File exists? {name.is_file()}")
     if not name.is_file():
         print(f"could not find {name} ")
         return
 
     c = cellpy.get(filename=name, instrument="maccor_txt", model="one", mass=1.0)
     print("loaded")
-    raw = c.cell.raw
-    steps = c.cell.steps
-    summary = c.cell.summary
+    raw = c.data.raw
+    steps = c.data.steps
+    summary = c.data.summary
 
     raw.to_csv(r"C:\scripting\trash\raw.csv", sep=";")
     steps.to_csv(r"C:\scripting\trash\steps.csv", sep=";")
     summary.to_csv(r"C:\scripting\trash\summary.csv", sep=";")
 
     fig_1, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(6, 10))
     raw.plot(x="test_time", y="voltage", ax=ax1, title="voltage")
@@ -383,21 +383,21 @@
         print(f"could not find {name} ")
         return
 
     c = cellpy.get(
         filename=name, instrument=INSTRUMENT, model=MODEL, mass=1.0, auto_summary=False
     )
     print(f"loaded the file - now lets see what we got")
-    raw = c.cell.raw
+    raw = c.data.raw
     raw.to_clipboard()
     print(raw.head())
     c.make_step_table()
 
-    steps = c.cell.steps
-    summary = c.cell.summary
+    steps = c.data.steps
+    summary = c.data.summary
 
     raw.to_csv(out / "raw.csv", sep=";")
     steps.to_csv(out / "steps.csv", sep=";")
     summary.to_csv(out / "summary.csv", sep=";")
 
     fig_1, (ax1, ax2, ax3, ax4) = plt.subplots(
         4,
@@ -423,8 +423,8 @@
     outfile = out / "test_out"
     c.save(outfile)
 
 
 if __name__ == "__main__":
     # check_dev_loader2(model="two")
     # check_loader(number=2, model="two")
-    check_loader_from_outside_with_get2()
+    check_loader_from_outside_with_get()
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/neware_txt.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/neware_txt.py`

 * *Files 8% similar despite different names*

```diff
@@ -49,15 +49,14 @@
     headers_normal,
 )
 from cellpy.readers.instruments.base import TxtLoader
 
 
 SUPPORTED_MODELS = {
     "UIO": "neware_txt_zero",
-    "VAJEE": "neware_txt_one",
 }
 
 
 MUST_HAVE_RAW_COLUMNS = [
     headers_normal.test_time_txt,
     headers_normal.step_time_txt,
     headers_normal.current_txt,
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/old_custom.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/arbin_sql.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,561 +1,525 @@
+"""arbin MS SQL Server data"""
+import datetime
 import logging
 import os
-import pathlib
+import platform
+import shutil
+import sys
+import tempfile
+import time
+import warnings
 
+import numpy as np
 import pandas as pd
-from ruamel import yaml
+import pyodbc
+from dateutil.parser import parse
 
-from cellpy.exceptions import FileNotFound
-from cellpy.parameters import prms
-from cellpy.parameters.internal_settings import ATTRS_CELLPYFILE, get_headers_normal
-from cellpy.readers.core import Cell, FileID, check64bit, humanize_bytes
+from cellpy import prms
+from cellpy.parameters.internal_settings import HeaderDict, get_headers_normal
+from cellpy.readers.core import (
+    Data,
+    FileID,
+    check64bit,
+    humanize_bytes,
+    xldate_as_datetime,
+)
 from cellpy.readers.instruments.base import BaseLoader
 
-DEFAULT_CONFIG = {
-    "structure": {
-        "format": "csv",
-        "table_name": None,
-        "header_definitions": "labels",
-        "comment_chars": ("#", "!"),
-        "sep": ";",
-        "locate_start_data_by": "line_number",
-        "locate_end_data_by": "EOF",
-        "locate_vars_by": "key_value_pairs",
-        "start_data": 19,
-        "header_info_line": 1,
-        "start_data_offset": 0,
-        "header_info_parse": "in_key",
-        "header_info_splitter": ";",
-        "file_type_id_line": 0,
-        "file_type_id_match": None,
-    },
-    "variables": {
-        "mass": "mass",
-        "total_mass": "total_mass",
-        "schedule_file": "schedule_file",
-        "schedule": "schedule",
-        "creator": "operator",
-        "loaded_from": "loaded_from",
-        "channel_index": "channel_index",
-        "channel_number": "channel_number",
-        "item_ID": "instrument",
-        "test_ID": "test_name",
-        "cell_name": "cell",
-        "material": "material",
-        "counter_electrode": "counter",
-        "reference_electrode": "reference",
-        "start_datetime": "date",
-        "fid_last_modification_time": "last_modified",
-        "fid_size": "size",
-        "fid_last_accessed": "last_accessed",
-    },
-    "headers": {
-        "data_point_txt": "index",
-        "charge_capacity_txt": "charge_capacity",
-        "current_txt": "current",
-        "cycle_index_txt": "cycle",
-        "datetime_txt": "date_stamp",
-        "discharge_capacity_txt": "discharge_Capacity",
-        "step_index_txt": "step",
-        "step_time_txt": "step_time",
-        "test_time_txt": "test_time",
-        "voltage_txt": "voltage",
-    },
-    "units": {"current": 0.001, "charge": 0.001, "mass": 0.001, "specific": 1.0},
-    "limits": {
-        "current_hard": 0.0000000000001,
-        "current_soft": 0.00001,
-        "stable_current_hard": 2.0,
-        "stable_current_soft": 4.0,
-        "stable_voltage_hard": 2.0,
-        "stable_voltage_soft": 4.0,
-        "stable_charge_hard": 0.9,
-        "stable_charge_soft": 5.0,
-        "ir_change": 0.00001,
-    },
+# TODO: @muhammad - get more meta data from the SQL db
+# TODO: @jepe - update the batch functionality (including filefinder)
+# TODO: @muhammad - make routine for "setting up the SQL Server" so that it is accessible and document it
+
+DEBUG_MODE = prms.Reader.diagnostics  # not used
+ALLOW_MULTI_TEST_FILE = prms._allow_multi_test_file  # not used
+ODBC = prms._odbc
+SEARCH_FOR_ODBC_DRIVERS = prms._search_for_odbc_driver  # not used
+SQL_SERVER = prms.Instruments.Arbin["SQL_server"]
+SQL_UID = prms.Instruments.Arbin["SQL_UID"]
+SQL_PWD = prms.Instruments.Arbin["SQL_PWD"]
+SQL_DRIVER = prms.Instruments.Arbin["SQL_Driver"]
+
+
+# Names of the tables in the SQL Server db that is used by cellpy
+
+# Not used anymore - maybe use a similar dict for the SQL table names (they are hard-coded at the moment)
+TABLE_NAMES = {
+    "normal": "Channel_Normal_Table",
+    "global": "Global_Table",
+    "statistic": "Channel_Statistic_Table",
+    "aux_global": "Aux_Global_Data_Table",
+    "aux": "Auxiliary_Table",
 }
 
+# Contains several headers not encountered yet in the Arbin SQL Server tables
+summary_headers_renaming_dict = {
+    "test_id_txt": "Test_ID",
+    "data_point_txt": "Data_Point",
+    "vmax_on_cycle_txt": "Vmax_On_Cycle",
+    "charge_time_txt": "Charge_Time",
+    "discharge_time_txt": "Discharge_Time",
+}
 
-class DataLoader(BaseLoader):
-    """Class for loading cell data from custom formatted files.
+# Contains several headers not encountered yet in the Arbin SQL Server tables
+normal_headers_renaming_dict = {
+    "aci_phase_angle_txt": "ACI_Phase_Angle",
+    "ref_aci_phase_angle_txt": "Reference_ACI_Phase_Angle",
+    "ac_impedance_txt": "AC_Impedance",
+    "ref_ac_impedance_txt": "Reference_AC_Impedance",
+    "charge_capacity_txt": "Charge_Capacity",
+    "charge_energy_txt": "Charge_Energy",
+    "current_txt": "Current",
+    "cycle_index_txt": "Cycle_ID",
+    "data_point_txt": "Data_Point",
+    "datetime_txt": "Date_Time",
+    "discharge_capacity_txt": "Discharge_Capacity",
+    "discharge_energy_txt": "Discharge_Energy",
+    "internal_resistance_txt": "Internal_Resistance",
+    "is_fc_data_txt": "Is_FC_Data",
+    "step_index_txt": "Step_ID",
+    "sub_step_index_txt": "Sub_Step_Index",  # new
+    "step_time_txt": "Step_Time",
+    "sub_step_time_txt": "Sub_Step_Time",  # new
+    "test_id_txt": "Test_ID",
+    "test_time_txt": "Test_Time",
+    "voltage_txt": "Voltage",
+    "ref_voltage_txt": "Reference_Voltage",  # new
+    "dv_dt_txt": "dV/dt",
+    "frequency_txt": "Frequency",  # new
+    "amplitude_txt": "Amplitude",  # new
+    "channel_id_txt": "Channel_ID",  # new Arbin SQL Server
+    "data_flag_txt": "Data_Flags",  # new Arbin SQL Server
+    "test_name_txt": "Test_Name",  # new Arbin SQL Server
+}
 
-    The file that contains the description of the custom data file
-    should be given by issuing the
-    pick_definition_file or given in the config file
-    (prms.Instruments.custom_instrument_definitions_file)
-
-    The format of the custom data file should be on the form
-
-    ...
-        # comment
-        # ...
-        variable sep value
-        variable sep value
-        ...
-        header1 sep header2 sep ...
-        value1  sep value2  sep ...
-        ...
-
-    where sep is either defined in the description file or the
-    config file.
-
-    The definition file should use the YAML format and it
-    must contain
 
-    xxx
-    xxx
+# Arbin SQL Server table headers (for both data df and stats df)
+# --------------------------------------------------------------
+# Test_ID
+# Channel_ID
+# Date_Time
+# Data_Point
+# Test_Time
+# Step_Time
+# Cycle_ID
+# Step_ID
+# Current
+# Voltage
+# Charge_Capacity
+# Discharge_Capacity
+# Charge_Energy
+# Discharge_Energy
+# Data_Flags
+# Test_Name
+# (all these headers are now implemented in the internal_settings
+
+
+def from_arbin_to_datetime(n):
+    if isinstance(n, int):
+        n = str(n)
+    ms_component = n[-7:]
+    date_time_component = n[:-7]
+    temp = f"{date_time_component}.{ms_component}"
+    datetime_object = datetime.datetime.fromtimestamp(float(temp))
+    time_in_str = datetime_object.strftime("%y-%m-%d %H:%M:%S:%f")
+    return time_in_str
 
-    [doc not finished yet]
-    """
 
-    name = "old_custom"
-    raw_ext = "*"
+class DataLoader(BaseLoader):
+    """Class for loading arbin-data from MS SQL server."""
+
+    instrument_name = "arbin_sql"
+    _is_db = True
 
     def __init__(self, *args, **kwargs):
-        """initiates the class"""
+        """initiates the ArbinSQLLoader class"""
+        self.arbin_headers_normal = (
+            get_headers_normal()
+        )  # the column headers defined by Arbin
+        self.cellpy_headers_normal = (
+            get_headers_normal()
+        )  # the column headers defined by cellpy
+        self.arbin_headers_global = self.get_headers_global()
+        self.arbin_headers_aux_global = self.get_headers_aux_global()
+        self.arbin_headers_aux = self.get_headers_aux()
+        self.current_chunk = 0  # use this to set chunks to load
+        self.server = SQL_SERVER
 
-        self.logger = logging.getLogger(__name__)
-        self.headers_normal = get_headers_normal()
-        self.definition_file = self._pick_definition_file()
-        self.units = None
-        self.limits = None
-        self.headers = None
-        self.variables = None
-        self.structure = None
-        self._parse_definition_file()
+    @staticmethod
+    def get_headers_normal():
+        """Defines the so-called normal column headings for Arbin SQL Server"""
+        # covered by cellpy at the moment
+        return get_headers_normal()
 
     @staticmethod
-    def _pick_definition_file():
-        logging.debug(
-            f"instrument file: {prms.Instruments.custom_instrument_definitions_file}"
-        )
-        return prms.Instruments.custom_instrument_definitions_file
+    def get_headers_aux():
+        """Defines the so-called auxiliary table column headings for Arbin SQL Server"""
+        # not in use (yet)
+        headers = HeaderDict()
+        # - aux column headings (specific for Arbin)
+        headers["test_id_txt"] = "Test_ID"
+        headers["data_point_txt"] = "Data_Point"
+        headers["aux_index_txt"] = "Auxiliary_Index"
+        headers["data_type_txt"] = "Data_Type"
+        headers["x_value_txt"] = "X"
+        headers["x_dt_value"] = "dX_dt"
+        return headers
 
     @staticmethod
-    def _load_definition_file():
-        definitions_file = pathlib.Path(
-            prms.Instruments.custom_instrument_definitions_file
-        )
-        if not definitions_file.is_file():
-            raise FileNotFound(
-                f"Custom definitions file not found ({definitions_file})"
-            )
-        yml = yaml.YAML()
-        with open(definitions_file, "r") as ff:
-            settings = yml.load(ff.read())
-        return settings
-
-    # TODO: @jepe - create yaml file example (from DEFAULT_CONFIG)
-
-    def _parse_definition_file(self):
-        if self.definition_file is None:
-            logging.info("no definition file for custom format")
-            logging.info("using default settings")
-            settings = DEFAULT_CONFIG
-        else:
-            logging.info("loading definition file for custom format")
-            settings = self._load_definition_file()
-
-        self.units = settings["units"]
-        self.limits = settings["limits"]
-        self.headers = settings["headers"]
-        self.variables = settings["variables"]
-        self.structure = settings["structure"]
-
-    def get_raw_units(self):
-        return self.units
-
-    def get_raw_limits(self):
-        return self.limits
-
-    def _find_data_start(self, file_name, sep):
-        if self.structure["locate_start_data_by"] != "line_number":
-            raise NotImplementedError
-        if not self.structure["start_data"] is None:
-            return self.structure["start_data"] + self.structure["start_data_offset"]
-
-        else:
-            logging.debug("searching for line where data starts")
-            header_info_line = self.structure["header_info_line"]
-            header_info_parse = self.structure["header_info_parse"]
-            header_info_splitter = self.structure["header_info_splitter"]
-            header_info_line = self.structure["header_info_line"]
-
-            with open(file_name, "rb") as fp:
-                for i, line_ in enumerate(fp):
-                    if i == header_info_line:
-                        line = line_.strip()
-                        line = line.decode()
-                        break
-
-            if header_info_parse == "in_key":
-                _, v = line.split(header_info_splitter)
-            else:
-                _, v = line.split(sep)
-            v = int(v)
-            return v
+    def get_headers_aux_global():
+        """Defines the so-called auxiliary global column headings for Arbin SQL Server"""
+        # not in use yet
+        headers = HeaderDict()
+        # - aux global column headings (specific for Arbin)
+        headers["channel_index_txt"] = "Channel_Index"
+        headers["aux_index_txt"] = "Auxiliary_Index"
+        headers["data_type_txt"] = "Data_Type"
+        headers["aux_name_txt"] = "Nickname"
+        headers["aux_unit_txt"] = "Unit"
+        return headers
 
-    def loader(self, file_name, **kwargs):
-        """Custom loader method
+    @staticmethod
+    def get_headers_global():
+        """Defines the so-called global column headings for Arbin SQL Server"""
+        # not in use yet
+        headers = HeaderDict()
+        # - global column headings (specific for Arbin)
+        headers["applications_path_txt"] = "Applications_Path"
+        headers["channel_index_txt"] = "Channel_Index"
+        headers["channel_number_txt"] = "Channel_Number"
+        headers["channel_type_txt"] = "Channel_Type"
+        headers["comments_txt"] = "Comments"
+        headers["creator_txt"] = "Creator"
+        headers["daq_index_txt"] = "DAQ_Index"
+        headers["item_id_txt"] = "Item_ID"
+        headers["log_aux_data_flag_txt"] = "Log_Aux_Data_Flag"
+        headers["log_chanstat_data_flag_txt"] = "Log_ChanStat_Data_Flag"
+        headers["log_event_data_flag_txt"] = "Log_Event_Data_Flag"
+        headers["log_smart_battery_data_flag_txt"] = "Log_Smart_Battery_Data_Flag"
+        headers["mapped_aux_conc_cnumber_txt"] = "Mapped_Aux_Conc_CNumber"
+        headers["mapped_aux_di_cnumber_txt"] = "Mapped_Aux_DI_CNumber"
+        headers["mapped_aux_do_cnumber_txt"] = "Mapped_Aux_DO_CNumber"
+        headers["mapped_aux_flow_rate_cnumber_txt"] = "Mapped_Aux_Flow_Rate_CNumber"
+        headers["mapped_aux_ph_number_txt"] = "Mapped_Aux_PH_Number"
+        headers["mapped_aux_pressure_number_txt"] = "Mapped_Aux_Pressure_Number"
+        headers["mapped_aux_temperature_number_txt"] = "Mapped_Aux_Temperature_Number"
+        headers["mapped_aux_voltage_number_txt"] = "Mapped_Aux_Voltage_Number"
+        headers[
+            "schedule_file_name_txt"
+        ] = "Schedule_File_Name"  # KEEP FOR CELLPY FILE FORMAT
+        headers["start_datetime_txt"] = "Start_DateTime"
+        headers["test_id_txt"] = "Test_ID"  # KEEP FOR CELLPY FILE FORMAT
+        headers["test_name_txt"] = "Test_Name"  # KEEP FOR CELLPY FILE FORMAT
+        return headers
+
+    @staticmethod
+    def get_raw_units():
+        raw_units = dict()
+        raw_units["current"] = "A"
+        raw_units["charge"] = "Ah"
+        raw_units["mass"] = "g"
+        raw_units["voltage"] = "V"
+        return raw_units
+
+    @staticmethod
+    def get_raw_limits():
+        """returns a dictionary with resolution limits"""
+        raw_limits = dict()
+        raw_limits["current_hard"] = 0.000_000_000_000_1
+        raw_limits["current_soft"] = 0.000_01
+        raw_limits["stable_current_hard"] = 2.0
+        raw_limits["stable_current_soft"] = 4.0
+        raw_limits["stable_voltage_hard"] = 2.0
+        raw_limits["stable_voltage_soft"] = 4.0
+        raw_limits["stable_charge_hard"] = 0.001
+        raw_limits["stable_charge_soft"] = 5.0
+        raw_limits["ir_change"] = 0.00001
+        return raw_limits
+
+    # TODO: rename this (for all instruments) to e.g. load
+    # TODO: implement more options (bad_cycles, ...)
+    def loader(self, name, **kwargs):
+        """returns a Data object with loaded data.
+
+        Loads data from arbin SQL server db.
 
         Args:
-            file_name (path): name of the file with the raw data
-            **kwargs: optional key-word arguments
-                pre_processor_hook (callable): function to apply before returning
-                    the data (run after renaming the cols but before changing the structure)
+            name (str): name of the test
 
         Returns:
-            List of cellpy Cell objects.
+            new_tests (list of data objects)
         """
-        pre_processor_hook = kwargs.pop("pre_processor_hook", None)
-        new_tests = []
-        var_dict = dict()
-
-        cycle_index_hdr = self.headers_normal["cycle_index_txt"]
-        charge_cap_hdr = self.headers_normal["charge_capacity_txt"]
-        discharge_cap_hdr = self.headers_normal["discharge_capacity_txt"]
-        datetime_hdr = self.headers_normal["datetime_txt"]
-        data_point = self.headers_normal["data_point_txt"]
-        step_time_hdr = self.headers_normal["step_time_txt"]
-        test_time_hdr = self.headers_normal["test_time_txt"]
-
-        if not os.path.isfile(file_name):
-            self.logger.info("Missing file_\n   %s" % file_name)
-            return
-
-        # find out strategy (based on structure)
-        if self.structure["format"] not in ["csv", "xlsx", "xls"]:
-            print(self.structure["format"])
-            raise NotImplementedError
-        else:
-            logging.debug(self.structure["format"])
-
-        if self.structure["format"] == "csv":
-            sep = self.structure.get("sep", prms.Reader.sep)
-
-            locate_vars_by = self.structure.get("locate_vars_by", "key_value_pairs")
-            comment_chars = self.structure.get("comment_chars", ["#", "!"])
-            header_row = self.structure.get("start_data", None)
-            if header_row is None:
-                header_row = self._find_data_start(file_name, sep)
-
-            # parsing the top part of the file, looking for variables
-            var_lines = []
-            with open(file_name, "rb") as fp:
-                for i, line in enumerate(fp):
-                    if i < header_row:
-                        line = line.strip()
-                        try:
-                            line = line.decode()
-                        except UnicodeDecodeError:
-                            logging.debug(
-                                "UnicodeDecodeError: " "skipping this line: " f"{line}"
-                            )
-                        else:
-                            if line.startswith(comment_chars):
-                                logging.debug(f"Comment: {line}")
-                            else:
-                                var_lines.append(line)
-                    else:
-                        break
-
-            if locate_vars_by == "key_value_pairs":
-                for line in var_lines:
-                    parts = line.split(sep)
+        # self.name = name
+        self.is_db = True
+        data_df, stat_df = self._query_sql(self.name)
+        aux_data_df = None  # Needs to be implemented
+        meta_data = None  # Should be implemented
+
+        # init data
+
+        # selecting only one value (might implement multi-channel/id use later)
+        test_id = data_df["Test_ID"].iloc[0]
+        id_name = f"{SQL_SERVER}:{name}:{test_id}"
+
+        channel_id = data_df["Channel_ID"].iloc[0]
+
+        data = Data()
+        data.loaded_from = id_name
+        data.channel_index = channel_id
+        data.test_ID = test_id
+        data.test_name = name
+
+        # The following meta data is not implemented yet for SQL loader:
+        data.creator = None
+        data.schedule_file_name = None
+        data.start_datetime = None
+
+        # Generating a FileID project:
+        self.generate_fid()
+        data.raw_data_files.append(self.fid)
+
+        data.raw = data_df
+        data.raw_data_files_length.append(len(data_df))
+        data.summary = stat_df
+        data = self._post_process(data)
+        data = self.identify_last_data_point(data)
+
+        return data
+
+    def _post_process(self, data, **kwargs):
+        # TODO: move this to parent
+
+        fix_datetime = kwargs.pop("fix_datetime", True)
+        set_index = kwargs.pop("set_index", True)
+        rename_headers = kwargs.pop("rename_headers", True)
+        extract_start_datetime = kwargs.pop("extract_start_datetime", True)
+
+        # TODO:  insert post-processing and div tests here
+        #    - check dtypes
+
+        # Remark that we also set index during saving the file to hdf5 if
+        #   it is not set.
+        from pprint import pprint
+
+        if rename_headers:
+            columns = {}
+            for key in self.arbin_headers_normal:
+                old_header = normal_headers_renaming_dict.get(key, None)
+                new_header = self.cellpy_headers_normal[key]
+                if old_header:
+                    columns[old_header] = new_header
+                logging.debug(
+                    f"processing cellpy normal header key '{key}':"
+                    f" old_header='{old_header}' -> new_header='{new_header}'"
+                )
+            logging.debug(f"renaming dict: {columns}")
+            data.raw.rename(index=str, columns=columns, inplace=True)
+            try:
+                columns = {}
+                for key, old_header in summary_headers_renaming_dict.items():
                     try:
-                        var_dict[parts[0]] = parts[1]
-                    except IndexError as e:
-                        logging.debug(f"{e}\ncould not split var-value\n{line}")
-
-            else:
-                raise NotImplementedError
-
-        data = Cell()
-        data.loaded_from = file_name
-        fid = self._generate_fid(file_name, var_dict)
-
-        # parsing cellpydata attributes
-        for attribute in ATTRS_CELLPYFILE:
-            key = self.variables.get(attribute, None)
-            val = var_dict.pop(key, None)
-            # print(f"{attribute} -> {key}:{val}")
-            if val:
-                if key in ["mass"]:
-                    val = float(val)
-                # print(f"mass, {attribute}: {val}")
-                setattr(data, attribute, val)
-
-        data.raw_data_files.append(fid)
-
-        # setting optional attributes (will be implemented later I hope)
-        key = self.variables.get("total_mass", None)
-        if key:
-            total_mass = var_dict.pop(key, None)
-            logging.debug("total_mass is given, but not propagated")
-
-        logging.debug(f"unused vars: {var_dict}")
-        if self.structure["format"] == "csv":
-            raw = self._parse_csv_data(file_name, sep, header_row)
-
-        elif self.structure["format"] == "xlsx":
-            raw = self._parse_xlsx_data(file_name)
-
-        elif self.structure["format"] == "xls":
-            raw = self._parse_xls_data(file_name)
-
-        raw = self._rename_cols(raw)
-        if pre_processor_hook is not None:
-            logging.debug("running pre-processing-hook")
-            raw = pre_processor_hook(raw)
-
-        capacity_structure = self.structure.get("capacity_structure", "cellpy")
-        if capacity_structure == "cellpy":
-            logging.debug(
-                "standard cellpy structure - no additional processing of capacity columns needed"
-            )
-        elif capacity_structure == "one_col_state":
-            # TODO: make this a function or method
-            # TODO: currently, the user needs to assign the name of the capacity column to the charge_capacity
-            #  variable in the yaml file. Should improve this later.
-            delimiter = "::"
-            cap_col = "charge_capacity"
-            col = self.structure["state_col"]
-            charge_key = self.structure["charge_key"]
-            charge_key = charge_key.split(delimiter)
-            discharge_key = self.structure["discharge_key"]
-            discharge_key = discharge_key.split(delimiter)
-
-            raw["new_c"] = 0
-            raw["new_d"] = 0
-
-            cycle_numbers = raw[cycle_index_hdr].unique()
-            # cell_type = prms.Reader.cycle_mode
-            good_cycles = []
-            bad_cycles = []
-            for i in cycle_numbers:
-                try:
-                    charge_cap = raw.loc[
-                        (raw[col].isin(charge_key)) & (raw[cycle_index_hdr] == i),
-                        [data_point, cap_col],
-                    ]
-                    discharge_cap = raw.loc[
-                        (raw[col].isin(discharge_key)) & (raw[cycle_index_hdr] == i),
-                        [data_point, cap_col],
-                    ]
-
-                    if not charge_cap.empty:
-                        charge_cap_last_index, charge_cap_last_val = charge_cap.iloc[-1]
-                        raw["new_c"].update(charge_cap[cap_col])
-
-                        raw.loc[
-                            (raw[data_point] > charge_cap_last_index)
-                            & (raw[cycle_index_hdr] == i),
-                            "new_c",
-                        ] = charge_cap_last_val
-
-                    if not discharge_cap.empty:
-                        (
-                            discharge_cap_last_index,
-                            discharge_cap_last_val,
-                        ) = discharge_cap.iloc[-1]
-                        raw["new_d"].update(discharge_cap[cap_col])
-
-                        raw.loc[
-                            (raw[data_point] > discharge_cap_last_index)
-                            & (raw[cycle_index_hdr] == i),
-                            "new_d",
-                        ] = discharge_cap_last_val
-
-                    good_cycles.append(i)
-
-                except Exception:
-                    bad_cycles.append(i)
-
-            raw[charge_cap_hdr] = raw["new_c"]
-            raw[discharge_cap_hdr] = raw["new_d"]
-            raw.drop(["new_c", "new_d"], axis=1)
-            if bad_cycles:
-                logging.critical(f"The data contains bad cycles: {bad_cycles}")
-        else:
-            raise NotImplementedError(f"{capacity_structure} is not yet supported")
-
-        step_time_conversion = self.structure.get("time_conversion_step_time", None)
-        test_time_conversion = self.structure.get("time_conversion_test_time", None)
-        date_time_conversion = self.structure.get("time_conversion_date_time", None)
-
-        if date_time_conversion:
-            if date_time_conversion.lower() == "test_time":
-                raw[datetime_hdr] = raw[test_time_hdr]
-                self.headers["datetime_txt"] = datetime_hdr
-            else:
-                raise NotImplementedError(
-                    f"date_time conversion method not implemented ({date_time_conversion})"
+                        columns[old_header] = self.cellpy_headers_normal[key]
+                    except KeyError:
+                        columns[old_header] = old_header.lower()
+                data.summary.rename(index=str, columns=columns, inplace=True)
+            except Exception as e:
+                logging.debug(f"Could not rename summary df ::\n{e}")
+
+        if fix_datetime:
+            h_datetime = self.cellpy_headers_normal.datetime_txt
+            logging.debug("converting to datetime format")
+
+            data.raw[h_datetime] = data.raw[h_datetime].apply(from_arbin_to_datetime)
+
+            h_datetime = h_datetime
+            if h_datetime in data.summary:
+                data.summary[h_datetime] = data.summary[h_datetime].apply(
+                    from_arbin_to_datetime
                 )
 
-        if test_time_conversion:
-            if test_time_conversion.lower() == "date_time_to_sec":
-                start_time = raw[test_time_hdr].iloc[0]
-                raw[test_time_hdr] = (
-                    raw[test_time_hdr] - start_time
-                ).dt.total_seconds()  # Warning: replaces original column
-                raw[test_time_hdr] = raw[test_time_hdr]
-            else:
-                raise NotImplementedError(
-                    f"test_time conversion method not implemented ({test_time_conversion})"
-                )
+        if set_index:
+            hdr_data_point = self.cellpy_headers_normal.data_point_txt
+            if data.raw.index.name != hdr_data_point:
+                data.raw = data.raw.set_index(hdr_data_point, drop=False)
+
+        if extract_start_datetime:
+            hdr_date_time = self.arbin_headers_normal.datetime_txt
+            data.start_datetime = parse("20" + data.raw[hdr_date_time].iat[0][:-7])
 
-        if step_time_conversion:
-            if step_time_conversion.lower() == "time_to_sec":
-                raw[step_time_hdr] = pd.to_timedelta(
-                    raw[step_time_hdr]
-                ).dt.total_seconds()
-            else:
-                raise NotImplementedError(
-                    f"step_time conversion method not implemented ({step_time_conversion})"
-                )
-        raw = self._select_cols(raw)
-        raw = self._check_cycleno_stepno(raw)
+        return data
 
-        data.raw_data_files_length.append(raw.shape[0])
-        data.summary = None
-        data.raw = raw
-        new_tests.append(data)
-        return new_tests
-
-    def _select_cols(self, raw):
-        selected = [
-            self.headers_normal[col_def]
-            for col_def in self.headers
-            if self.headers_normal[col_def] in raw.columns
-        ]
-        raw = raw[selected]
-        return raw
-
-    def _parse_xls_data(self, file_name):
-        sheet_name = self.structure["table_name"]
-
-        raw_frame = pd.read_excel(
-            file_name, engine="xlrd", sheet_name=None
-        )  # TODO: replace this with pd.ExcelReader
-        matching = [s for s in raw_frame.keys() if s.startswith(sheet_name)]
-        if matching:
-            return raw_frame[matching[0]]
-
-    def _parse_xlsx_data(self, file_name):
-        sheet_name = self.structure["table_name"]
-        raw_frame = pd.read_excel(
-            file_name, engine="openpyxl", sheet_name=None
-        )  # TODO: replace this with pd.ExcelReader
-        matching = [s for s in raw_frame.keys() if s.startswith(sheet_name)]
-        if matching:
-            return raw_frame[matching[0]]
-
-    def _parse_csv_data(self, file_name, sep, header_row):
-        raw = pd.read_csv(file_name, sep=sep, header=header_row, skip_blank_lines=False)
-        return raw
-
-    def _rename_cols(self, raw):
-        rename_col_dict = dict()
-
-        for col_def in self.headers:
-            new_name = self.headers_normal[col_def]
-            old_name = self.headers[col_def]
-            if old_name in raw.columns:
-                rename_col_dict[old_name] = new_name
-
-        raw = raw.rename(columns=rename_col_dict)
-        return raw
+    def _query_sql(self, name):
+        # TODO: refactor and include optional SQL arguments
+        name_str = f"('{name}', '')"
+        con_str = (
+            f"Driver={{{SQL_DRIVER}}};" + f"Server={SQL_SERVER};Trusted_Connection=yes;"
+        )
 
-    # TODO: @jepe - finalize the _check sub-modules
+        # TODO: use variable for the name of the main db (ArbinPro8....)
+        # TODO: consider making a function that searches for correct ArbinPro version
+        master_q = (
+            "SELECT Database_Name, Test_Name FROM "
+            "ArbinPro8MasterInfo.dbo.TestList_Table WHERE "
+            f"ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name IN {name_str}"
+        )
 
-    def _check_cycleno_stepno(self, raw):
-        return raw
+        conn = pyodbc.connect(con_str)
+        sql_query = pd.read_sql_query(master_q, conn)
 
-    def _convert_to_cellpy_units(self, data):
-        return data
+        datas_df = []
+        stats_df = []
 
-    def _check_columns(self, data):
-        return data
+        for index, row in sql_query.iterrows():
+            # TODO: use variables - see above
+            # TODO: consider to use f-strings
+            data_query = (
+                "SELECT "
+                + str(row["Database_Name"])
+                + ".dbo.IV_Basic_Table.*, ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name "
+                "FROM " + str(row["Database_Name"]) + ".dbo.IV_Basic_Table "
+                "JOIN ArbinPro8MasterInfo.dbo.TestList_Table "
+                "ON "
+                + str(row["Database_Name"])
+                + ".dbo.IV_Basic_Table.Test_ID = ArbinPro8MasterInfo.dbo.TestList_Table.Test_ID "
+                "WHERE ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name IN "
+                + str(name_str)
+            )
 
-    def _check_dtypes(self, data):
-        return data
+            stat_query = (
+                "SELECT "
+                + str(row["Database_Name"])
+                + ".dbo.StatisticData_Table.*, ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name "
+                "FROM " + str(row["Database_Name"]) + ".dbo.StatisticData_Table "
+                "JOIN ArbinPro8MasterInfo.dbo.TestList_Table "
+                "ON "
+                + str(row["Database_Name"])
+                + ".dbo.StatisticData_Table.Test_ID = ArbinPro8MasterInfo.dbo.TestList_Table.Test_ID "
+                "WHERE ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name IN "
+                + str(name_str)
+            )
 
-    def _generate_fid(self, file_name, var_dict):
-        fid = FileID()
-        last_modified = var_dict.get(self.variables["fid_last_modification_time"], None)
-        size = var_dict.get(self.variables["fid_size"], None)
-        last_accessed = var_dict.get(self.variables["fid_last_accessed"], None)
-
-        if any([last_modified, size, last_accessed]):
-            fid.name = os.path.abspath(file_name)
-            fid.full_name = file_name
-            fid.location = os.path.dirname(file_name)
-
-            fid.size = size
-            fid.last_modified = last_modified
-            fid.last_accessed = last_accessed
-            fid.last_info_changed = last_accessed
-        else:
-            fid.populate(file_name)
-
-        return fid
-
-    def inspect(self, data):
-        data = self._convert_to_cellpy_units(data)
-        data = self._check_columns(data)
-        data = self._check_dtypes(data)
-        return data
+            datas_df.append(pd.read_sql_query(data_query, conn))
+            stats_df.append(pd.read_sql_query(stat_query, conn))
 
-    def load(self, file_name):
-        """Load a raw data-file
+        data_df = pd.concat(datas_df, axis=0)
+        stat_df = pd.concat(stats_df, axis=0)
 
-        Args:
-            file_name (path)
+        return data_df, stat_df
 
-        Returns:
-            loaded test
-        """
 
-        new_rundata = self.loader(file_name)
-        new_rundata = self.inspect(new_rundata)
-        return new_rundata
+def check_sql_loader(server: str = None, tests: list = None):
+    test_name = tuple(tests) + ("",)  # neat trick :-)
+    print(f"** test str: {test_name}")
+    con_str = "Driver={SQL Server};Server=" + server + ";Trusted_Connection=yes;"
+    master_q = (
+        "SELECT Database_Name, Test_Name FROM "
+        "ArbinPro8MasterInfo.dbo.TestList_Table WHERE "
+        f"ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name IN {test_name}"
+    )
+
+    conn = pyodbc.connect(con_str)
+    print("** connected to server")
+    sql_query = pd.read_sql_query(master_q, conn)
+    print("** SQL query:")
+    print(sql_query)
+    for index, row in sql_query.iterrows():
+        # Muhammad, why is it a loop here?
+        print(f"** index: {index}")
+        print(f"** row: {row}")
+        data_query = (
+            "SELECT "
+            + str(row["Database_Name"])
+            + ".dbo.IV_Basic_Table.*, ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name "
+            "FROM " + str(row["Database_Name"]) + ".dbo.IV_Basic_Table "
+            "JOIN ArbinPro8MasterInfo.dbo.TestList_Table "
+            "ON "
+            + str(row["Database_Name"])
+            + ".dbo.IV_Basic_Table.Test_ID = ArbinPro8MasterInfo.dbo.TestList_Table.Test_ID "
+            "WHERE ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name IN "
+            + str(test_name)
+        )
 
+        stat_query = (
+            "SELECT "
+            + str(row["Database_Name"])
+            + ".dbo.StatisticData_Table.*, ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name "
+            "FROM " + str(row["Database_Name"]) + ".dbo.StatisticData_Table "
+            "JOIN ArbinPro8MasterInfo.dbo.TestList_Table "
+            "ON "
+            + str(row["Database_Name"])
+            + ".dbo.StatisticData_Table.Test_ID = ArbinPro8MasterInfo.dbo.TestList_Table.Test_ID "
+            "WHERE ArbinPro8MasterInfo.dbo.TestList_Table.Test_Name IN "
+            + str(test_name)
+        )
+        print(f"** data query: {data_query}")
+        print(f"** stat query: {stat_query}")
 
-def load_paal():
-    import pathlib
-    from pprint import pprint
+        # if looping, maybe these should be concatenated?
+        data_df = pd.read_sql_query(data_query, conn)
+        stat_df = pd.read_sql_query(stat_query, conn)
 
-    elkem_tester = pathlib.Path(
-        r"C:\scripts\tasks\sibanode\2021_08_11_round_robbin\elkem_format.yml"
-    )
-    prms.Instruments.custom_instrument_definitions_file = elkem_tester
-    print("running this")
-    loader = DataLoader()
-    # loader.pick_definition_file()
-    datadir = r"C:\scripts\tasks\sibanode\2021_08_11_round_robbin\data\raw\elkem\elkem"
-    datadir = pathlib.Path(datadir)
-    my_file_name = datadir / "240014-1-1-1621.xls"
-    # print(help(loader.get_raw_units))
-    # print(help(loader.get_raw_limits))
-    # print(f"Trying to load {my_file_name}")
-    loader.load(my_file_name)
+    return data_df, stat_df
 
 
-def load_example_file():
+def check_query():
     import pathlib
-    from pprint import pprint
 
-    print("running this")
-    loader = DataLoader()
-    # loader.pick_definition_file()
-    datadir = "/Users/jepe/scripting/cellpy/test_data"
-    datadir = pathlib.Path(datadir)
-    my_file_name = datadir / "custom_data_001.csv"
-    print(help(loader.get_raw_units))
-    print(help(loader.get_raw_limits))
-    print(f"Trying to load {my_file_name}")
-    loader.load(my_file_name)
+    name = ["20201106_HC03B1W_1_cc_01"]
+    dd, ds = check_sql_loader(SQL_SERVER, name)
+    out = pathlib.Path(r"C:\scripts\notebooks\Div")
+    dd.to_clipboard()
+    input("x")
+    ds.to_clipboard()
+
+
+def check_loader():
+    print(" Testing connection to arbin sql server ".center(80, "-"))
+
+    sql_loader = DataLoader()
+    name = "20201106_HC03B1W_1_cc_01"
+    cell = sql_loader.loader(name)
+
+    return cell
+
+
+def check_loader_from_outside():
+    import matplotlib.pyplot as plt
+
+    from cellpy import cellreader
+
+    name = "20200820_CoFBAT_slurry07B_01_cc_01"
+    c = cellreader.CellpyCell()
+    c.set_instrument("arbin_sql")
+    # print(c)
+    c.from_raw(name)
+    # print(c)
+    c.make_step_table()
+    c.make_summary()
+    # print(c)
+    raw = c.data.raw
+    steps = c.data.steps
+    summary = c.data.summary
+    raw.to_csv(r"C:\scripting\trash\raw.csv", sep=";")
+    steps.to_csv(r"C:\scripting\trash\steps.csv", sep=";")
+    summary.to_csv(r"C:\scripting\trash\summary.csv", sep=";")
+
+    n = c.get_number_of_cycles()
+    print(f"number of cycles: {n}")
+
+    cycle = c.get_cap(1, method="forth")
+    print(cycle.head())
+    cycle.plot(x="capacity", y="voltage")
+    plt.show()
+
+
+def check_get():
+    import cellpy
+
+    name = "20200820_CoFBAT_slurry07B_01_cc_01"
+    c = cellpy.get(name, instrument="arbin_sql")
+    print(c)
 
 
 if __name__ == "__main__":
-    load_paal()
+    # test_query()
+    # cell = test_loader()
+    check_get()
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/pec_csv.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/pec_csv.py`

 * *Files 12% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 from datetime import datetime
 
 import numpy as np
 import pandas as pd
 from dateutil.parser import parse
 
 from cellpy.parameters.internal_settings import get_headers_normal
-from cellpy.readers.core import Cell, FileID, humanize_bytes
+from cellpy.readers.core import Data, FileID, humanize_bytes
 from cellpy.readers.instruments.base import BaseLoader
 
 pec_headers_normal = dict()
 
 pec_headers_normal["step_index_txt"] = "Step"
 pec_headers_normal["cycle_index_txt"] = "Cycle"
 pec_headers_normal["test_time_txt"] = "Total_Time_Seconds"  # This might change
@@ -34,35 +34,34 @@
 #  2. convert cycle and step numbers so that they start with 1 and not 0
 #  3. find user-defined variables
 
 
 class DataLoader(BaseLoader):
     """Main loading class"""
 
-    name = "pec_csv"
+    instrument_name = "pec_csv"
     raw_ext = "csv"
 
     def __init__(self, *args, **kwargs):
         self.headers_normal = (
             get_headers_normal()
-        )  # should consider to move this to the Loader class
+        )  # should consider moving this to the Loader class
         self.current_chunk = 0  # use this to set chunks to load
         self.pec_data = None
         self.pec_log = None
         self.pec_settings = None
         self.variable_header_keywords = [
             "Voltage (V)",
             "Current (A)",
         ]  # The unit of these will be read from file
         self.fake_header_length = [
             "#RESULTS CHECK\n",
             "#END RESULTS CHECK\n",
         ]  # Ignores number of delimiters in between
         self.pec_file_delimiter = ","
-        self.filename = None
         self.number_of_header_lines = None  # Number of header lines is not constant
         self.cellpy_headers = (
             get_headers_normal()
         )  # should consider to move this to the Loader class
 
     # @staticmethod
     # def _get_pec_units():
@@ -81,15 +80,17 @@
 
         # Adding the non-variable units to the return value
         pec_units = {"charge": 0.001, "mass": 0.001, "energy": 0.001}  # Ah  # g  # Wh
 
         # A list with all the variable keywords without any prefixes, used as search terms
         header = self.variable_header_keywords
 
-        data = pd.read_csv(self.filename, skiprows=self.number_of_header_lines, nrows=1)
+        data = pd.read_csv(
+            self.temp_file_path, skiprows=self.number_of_header_lines, nrows=1
+        )
 
         # Searching for the prefix for all the variable units
         for item in data.keys():
             for unit in header:
                 x = unit.find("(") - len(unit)
                 if unit[: x + 1] in item:
                     y = item[x].replace("(", "")
@@ -108,15 +109,17 @@
         units = {
             "(Hours in hh:mm:ss.xxx)": self.timestamp_to_seconds,
             "(Decimal Hours)": 3600,
             "(Minutes)": 60,
             "(Seconds)": 1,
         }
 
-        data = pd.read_csv(self.filename, skiprows=self.number_of_header_lines, nrows=0)
+        data = pd.read_csv(
+            self.temp_file_path, skiprows=self.number_of_header_lines, nrows=0
+        )
         pec_times = dict()
 
         # Adds the time variables and their units to the pec_times dictonary return value
         # Also updates the column headers in pec_headers_normal with the correct name
         for item in data.keys():
             for unit in units:
                 if unit in item:
@@ -143,23 +146,34 @@
         current unit-fraction will be 0.001.
 
         Returns: dictionary containing the unit-fractions for current, charge, and mass
 
         """
 
         raw_units = dict()
-        raw_units["voltage"] = 1.0  # V
-        raw_units["current"] = 1.0  # A
-        raw_units["charge"] = 1.0  # Ah
-        raw_units["mass"] = 0.001  # g
-        raw_units["energy"] = 1.0  # Wh
-        raw_units["time"] = 1.0  # s
+        raw_units["current"] = "A"
+        raw_units["charge"] = "Ah"
+        raw_units["mass"] = "mg"
+        raw_units["voltage"] = "V"
+        raw_units["energy"] = "Wh"
+        raw_units["time"] = "s"
 
         return raw_units
 
+    @staticmethod
+    def _raw_units_for_internal_calculations():
+        raw_units = dict()
+        raw_units["current"] = 1.0
+        raw_units["charge"] = 1.0
+        raw_units["mass"] = 0.001
+        raw_units["voltage"] = 1.0
+        raw_units["energy"] = 1.0
+        raw_units["time"] = 1.0
+        return raw_units
+
     def get_raw_limits(self):
         """Include the settings for how to decide what kind of step you are examining here.
 
         The raw limits are 'epsilons' used to check if the current and/or voltage is stable (for example
         for galvanostatic steps, one would expect that the current is stable (constant) and non-zero).
         It is expected that different instruments (with different resolution etc.) have different
         'epsilons'.
@@ -177,67 +191,54 @@
         raw_limits["stable_voltage_soft"] = 4.0
         raw_limits["stable_charge_hard"] = 2.0
         raw_limits["stable_charge_soft"] = 5.0
         raw_limits["ir_change"] = 0.00001
         return raw_limits
 
     def loader(self, file_name, bad_steps=None, **kwargs):
-        new_tests = []
-        if not os.path.isfile(file_name):
-            self.logger.info("Missing file_\n   %s" % file_name)
-            return None
-
-        self.filename = file_name
+        # self.name = file_name
+        # self.copy_to_temporary()
         self.number_of_header_lines = self._find_header_length()
-        filesize = os.path.getsize(file_name)
-        hfilesize = humanize_bytes(filesize)
-        txt = "Filesize: %i (%s)" % (filesize, hfilesize)
-        logging.debug(txt)
-
-        data = Cell()
-        fid = FileID(file_name)
+        data = Data()
+        self.generate_fid()
+        data.raw_data_files.append(self.fid)
 
         # div parameters and information (probably load this last)
-        test_no = 1
-        data.cell_no = test_no
-        data.loaded_from = file_name
+        data.loaded_from = self.name
 
         # some overall prms
         data.channel_index = None
-        data.channel_number = None
         data.creator = None
-        data.item_ID = None
         data.schedule_file_name = None
         data.test_ID = None
         data.test_name = None
-        data.raw_data_files.append(fid)
 
         # --------- read raw-data (normal-data) -------------------------
-
-        self._load_pec_data(file_name, bad_steps)
+        self._load_pec_data(bad_steps)
         data.start_datetime = self.pec_settings["start_time"]
         length_of_test = self.pec_data.shape[0]
         logging.debug(f"length of test: {length_of_test}")
 
         logging.debug("renaming columns")
         self._rename_headers()
         self._convert_units()
 
         # cycle indices should not be 0
         if 0 in self.pec_data["cycle_index"]:
             self.pec_data["cycle_index"] += 1
 
         data.raw = self.pec_data
-
         data.raw_data_files_length.append(length_of_test)
-        new_tests.append(data)
 
-        return new_tests
+        return data
 
-    def _load_pec_data(self, file_name, bad_steps):
+    def _load_pec_data(self, bad_steps):
+        if bad_steps is not None:
+            warnings.warn("bad_steps is not implemented yet for this instrument")
+        file_name = self.temp_file_path
         number_of_header_lines = self.number_of_header_lines
 
         # ----------------- reading the data ---------------------
         df = pd.read_csv(file_name, skiprows=number_of_header_lines)
 
         # get rid of unnamed columns
         df = df.loc[:, ~df.columns.str.contains("^Unnamed")]
@@ -271,15 +272,14 @@
                 lines.append(line)
         self._extract_variables(lines)
 
     def _extract_variables(self, lines):
         header_comments = dict()
         comment_loop = False
         for line_number, line in enumerate(lines):
-
             if line.startswith("#"):
                 if not comment_loop:
                     comment_loop = True
                 else:
                     comment_loop = False
 
             else:
@@ -335,15 +335,15 @@
         )
 
         self.pec_data["Rack"] = self.pec_data["Rack"].astype("category")
 
         logging.debug("- cellpy units")
         pec_units = self._get_pec_units()
         pec_times = self._get_pec_times()
-        raw_units = self.get_raw_units()
+        raw_units = self._raw_units_for_internal_calculations()
         self._rename_headers()  # Had to run this again after fixing the headers, might be a better way to fix this
 
         _v = pec_units["voltage"] / raw_units["voltage"]
         _i = pec_units["current"] / raw_units["current"]
         _c = pec_units["charge"] / raw_units["charge"]
         _w = pec_units["energy"] / raw_units["energy"]
 
@@ -390,15 +390,15 @@
         except KeyError as e:
             logging.info(f"Problem during conversion to cellpy-format ({e})")
 
     def _find_header_length(self):
         skiprows = 0
         resultscheck = False  # Ignore number of delimiters inside RESULTS CHECK
 
-        with open(self.filename, "r") as header:
+        with open(self.temp_file_path, "r") as header:
             for line in header:
                 if line in self.fake_header_length:
                     resultscheck = not resultscheck
                 if (
                     line.count(self.pec_file_delimiter) > 1 and not resultscheck
                 ):  # End when there are >2 columns
                     break
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/processors/post_processors.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/processors/post_processors.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,29 +1,30 @@
 """Post-processing methods for instrument loaders.
 
-All methods must implement the following parameters/arguments:
-    data: Cell object
+All methods must implement the following parameters/arguments::
+
+    data: Data object
     config_params: ModelParameters
 
-All methods should return the modified Cell object.
+All methods should return the modified Data object.
 
 You can access the individual parameters for the post processor from
 the config_params.post_processor[<name of post processor>].
 
 """
 import datetime
 import logging
 import sys
 
 import pandas as pd
 import numpy as np
 
 from cellpy.parameters.internal_settings import headers_normal
 from cellpy.parameters.prms import _minimum_columns_to_keep_for_raw_if_exists
-from cellpy.readers.core import Cell
+from cellpy.readers.core import Data
 from cellpy.readers.instruments.configurations import ModelParameters
 
 ORDERED_POST_PROCESSING_STEPS = [
     "get_column_names",
     "rename_headers",
     "select_columns_to_keep",
     "remove_last_if_bad",
@@ -42,24 +43,25 @@
 #  3. the old custom loader has methods for parsing csv, xls,
 #     and xlsx - implement them if needed. [DONE]
 #  4. Consider adding x_time_conversion key-words and methods from old custom.
 #  5. Check if CustomTxtLoader properly implements inspect() and
 #     _generate_fid().
 
 
-def remove_last_if_bad(data: Cell, config_params: ModelParameters) -> Cell:
+def remove_last_if_bad(data: Data, config_params: ModelParameters) -> Data:
     """Drop the last row if it contains more NaNs than second to last."""
     number_of_nans_2nd_last = data.raw.iloc[-2].isna().sum()
     number_of_nans_last = data.raw.iloc[-1].isna().sum()
     if number_of_nans_last > number_of_nans_2nd_last:
         data.raw = data.raw.drop(data.raw.tail(1).index)  # drop last row
     return data
 
 
-def convert_units(data: Cell, config_params: ModelParameters) -> Cell:
+def convert_units(data: Data, config_params: ModelParameters) -> Data:
+    raise Exception("THIS FUNCTION NEEDS TO BE UPDATED")
     # TODO: implement all
     if x := config_params.raw_units.get("voltage", None):
         logging.debug(f"converting voltage ({x})")
         data.raw[headers_normal.voltage_txt] = data.raw[headers_normal.voltage_txt] * x
 
     if config_params.raw_units.get("current", None):
         logging.debug("converting current - not implemented yet")
@@ -70,36 +72,37 @@
     # TODO: add time as raw unit
     if config_params.raw_units.get("time", None):
         logging.debug("converting time - not implemented yet")
 
     return data
 
 
-def set_cycle_number_not_zero(data: Cell, config_params: ModelParameters) -> Cell:
+def set_cycle_number_not_zero(data: Data, config_params: ModelParameters) -> Data:
     data.raw[headers_normal.cycle_index_txt] += 1
     return data
 
 
-def select_columns_to_keep(data: Cell, config_params: ModelParameters) -> Cell:
+def select_columns_to_keep(data: Data, config_params: ModelParameters) -> Data:
     config_params.columns_to_keep.extend(
         headers_normal[h] for h in _minimum_columns_to_keep_for_raw_if_exists
     )
     if config_params.states:
         config_params.columns_to_keep.append(config_params.states["column_name"])
     config_params.columns_to_keep = list(set(config_params.columns_to_keep))
     columns_to_keep = [
         col for col in config_params.columns_to_keep if col in data.raw.columns
     ]
     data.raw = data.raw[columns_to_keep]
     return data
 
 
-def get_column_names(data: Cell, config_params: ModelParameters) -> Cell:
+def get_column_names(data: Data, config_params: ModelParameters) -> Data:
     # TODO: add custom "splitter"
     # TODO: test
+    raise Exception("THIS FUNCTION NEEDS TO BE UPDATED")
     if not config_params.prefixes:
         config_params.prefixes = {
             "G": 1000_000_000,
             "M": 1000_000,
             "k": 1000.0,
             "h": 100.0,
             "d": 10.0,
@@ -142,72 +145,72 @@
         for k in DEFAULT_RAW_UNITS:
             config_params.raw_units[k] = config_params.raw_units.get(
                 k, DEFAULT_RAW_UNITS[k]
             )
     return data
 
 
-def convert_date_time_to_datetime(data: Cell, config_params: ModelParameters) -> Cell:
+def convert_date_time_to_datetime(data: Data, config_params: ModelParameters) -> Data:
     hdr_date_time = headers_normal.datetime_txt
     data.raw[hdr_date_time] = pd.to_datetime(data.raw[hdr_date_time])
     return data
 
 
-def date_time_from_test_time(data: Cell, config_params: ModelParameters) -> Cell:
+def date_time_from_test_time(data: Data, config_params: ModelParameters) -> Data:
     """add a date_time column (based on the test_time column)."""
     hdr_date_time = headers_normal.datetime_txt
     hdr_test_time = headers_normal.test_time_txt
 
     # replace this with something that can parse a date-string if implementing start_date in config_params.
     # currently, it will always use current date-time as start date.
     start_date = config_params.meta_keys.get("start_date", datetime.datetime.now())
     start_time = data.raw[hdr_test_time].iloc[0]
     data.raw[hdr_date_time] = (
         pd.to_timedelta(data.raw[hdr_test_time] - start_time) + start_date
     )
     return data
 
 
-def convert_step_time_to_timedelta(data: Cell, config_params: ModelParameters) -> Cell:
+def convert_step_time_to_timedelta(data: Data, config_params: ModelParameters) -> Data:
     hdr_step_time = headers_normal.step_time_txt
     if data.raw[hdr_step_time].dtype == "datetime64[ns]":
         logging.debug("already datetime64[ns] - need to convert to back first")
         data.raw[hdr_step_time] = data.raw[hdr_step_time].view("int64")
         data.raw[hdr_step_time] = (
             data.raw[hdr_step_time] - data.raw[hdr_step_time].iloc[0]
         )
 
     data.raw[hdr_step_time] = pd.to_timedelta(
         data.raw[hdr_step_time]
     ).dt.total_seconds()
     return data
 
 
-def convert_test_time_to_timedelta(data: Cell, config_params: ModelParameters) -> Cell:
+def convert_test_time_to_timedelta(data: Data, config_params: ModelParameters) -> Data:
     hdr_test_time = headers_normal.test_time_txt
     if data.raw[hdr_test_time].dtype == "datetime64[ns]":
         logging.debug("already datetime64[ns] - need to convert to back first")
         data.raw[hdr_test_time] = data.raw[hdr_test_time].view("int64")
         data.raw[hdr_test_time] = (
             data.raw[hdr_test_time] - data.raw[hdr_test_time].iloc[0]
         )
     data.raw[hdr_test_time] = pd.to_timedelta(
         data.raw[hdr_test_time]
     ).dt.total_seconds()
     return data
 
 
-def set_index(data: Cell, config_params: ModelParameters) -> Cell:
+def set_index(data: Data, config_params: ModelParameters) -> Data:
     hdr_data_point = headers_normal.data_point_txt
     if data.raw.index.name != hdr_data_point:
         data.raw = data.raw.set_index(hdr_data_point, drop=False)
     return data
 
 
-def cumulate_capacity_within_cycle(data: Cell, config_params: ModelParameters) -> Cell:
+def cumulate_capacity_within_cycle(data: Data, config_params: ModelParameters) -> Data:
     """Cumulates the capacity within each cycle"""
     # state_column = config_params.states["column_name"]
     # is_charge = config_params.states["charge_keys"]
     # is_discharge = config_params.states["discharge_keys"]
     cycles = data.raw.groupby("cycle_index")
     cumulated = []
     charge_hdr = "charge_capacity"
@@ -223,21 +226,21 @@
             last_charge = step.at[step.index[-1], charge_hdr]
             last_discharge = step.at[step.index[-1], discharge_hdr]
             cumulated.append(step)
     data.raw = pd.concat(cumulated)
     return data
 
 
-def replace(data: Cell, config_params: ModelParameters) -> Cell:
+def replace(data: Data, config_params: ModelParameters) -> Data:
     print("NOT IMPLEMENTED")
     print("input:")
     print(config_params.post_processors["replace"])
 
 
-def rename_headers(data: Cell, config_params: ModelParameters) -> Cell:
+def rename_headers(data: Data, config_params: ModelParameters) -> Data:
     columns = {}
     renaming_dict = config_params.normal_headers_renaming_dict
     # ---- special cases ----
     # 1. datetime_txt and test_time_txt same column
     if "datetime_txt" in renaming_dict and "test_time_txt" in renaming_dict:
         datetime_hdr = renaming_dict["datetime_txt"]
         test_time_hdr = renaming_dict["test_time_txt"]
@@ -369,15 +372,15 @@
     raw = raw.drop([temp_col_name_charge], axis=1)
     if temp_col_name_charge != temp_col_name_discharge:
         raw[new_col_name_discharge] = raw[temp_col_name_discharge]
         raw = raw.drop([temp_col_name_discharge], axis=1)
     return raw
 
 
-def split_current(data: Cell, config_params: ModelParameters) -> Cell:
+def split_current(data: Data, config_params: ModelParameters) -> Data:
     """Split current into positive and negative"""
     data.raw = _state_splitter(
         data.raw,
         base_col_name="current",
         n_charge=1,
         n_discharge=-1,
         temp_col_name_charge="tmp_charge",
@@ -386,15 +389,15 @@
         temp_col_name_discharge="tmp_charge",
         propagate=False,
         states=config_params.states,
     )
     return data
 
 
-def split_capacity(data: Cell, config_params: ModelParameters) -> Cell:
+def split_capacity(data: Data, config_params: ModelParameters) -> Data:
     """split capacity into charge and discharge"""
     data.raw = _state_splitter(
         data.raw,
         base_col_name=headers_normal.charge_capacity_txt,
         n_charge=1,
         n_discharge=1,
         new_col_name_charge=headers_normal.charge_capacity_txt,
```

### Comparing `cellpy-0.4.3a3/cellpy/readers/instruments/processors/pre_processors.py` & `cellpy-1.0.0a0/cellpy/readers/instruments/processors/pre_processors.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 """Pre-processing methods for instrument loaders.
 
-All methods must implement the following parameters/arguments:
+All methods must implement the following parameters/arguments::
+
     filename: Union[str, pathlib.Path], *args: str, **kwargs: str
 
 All methods should return None (i.e. nothing).
 
 """
 
 import logging
@@ -39,12 +40,11 @@
     if not filename.is_file():
         raise IOError(f"Could not find the file ({filename})")
     out_file_name = filename.parent / (str(uuid.uuid4()) + ".txt")
 
     with open(filename, "r+") as file:
         with open(out_file_name, "w") as out_file:
             for line in file.readlines():
-
                 if line.strip():
                     out_file.write(f"{line.strip()}\n")
 
     return out_file_name
```

### Comparing `cellpy-0.4.3a3/cellpy/utils/batch.py` & `cellpy-1.0.0a0/cellpy/utils/batch.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 import cellpy.exceptions
 from cellpy import log, prms
 from cellpy.parameters.internal_settings import (
     headers_journal,
     headers_step_table,
     headers_summary,
 )
+from cellpy.internals.core import OtherPath
 from cellpy.utils.batch_tools.batch_analyzers import (
     BaseSummaryAnalyzer,
     OCVRelaxationAnalyzer,
 )
 from cellpy.utils.batch_tools.batch_core import Data
 from cellpy.utils.batch_tools.batch_experiments import CyclingExperiment
 from cellpy.utils.batch_tools.batch_exporters import CSVExporter
@@ -37,15 +38,22 @@
     headers_journal.total_mass,
     headers_journal.loading,
     headers_journal.nom_cap,
 ]
 
 
 class Batch:
-    """A convenient class for running batch procedures."""
+    """A convenient class for running batch procedures.
+
+    The Batch class contains among other things:
+        - iterator protocol
+        - a journal with info about the different cells where the
+        main information is accessible as a pandas.DataFrame through the `.pages` attribute
+        - a data lookup accessor `.data` that behaves similarly as a dict.
+    """
 
     def __init__(self, *args, **kwargs):
         """Initialize the Batch class.
 
         The initialization accepts arbitrary arguments and keyword arguments.
         It first looks for the file_name and db_reader keyword arguments.
 
@@ -139,14 +147,23 @@
         self.plotter.assign(self.experiment)
         self._journal_name = self.journal_name
         self.headers_step_table = headers_step_table
 
     def __str__(self):
         return str(self.experiment)
 
+    def _repr_html_(self):
+        txt = f"<h2>Batch-object</h2> id={hex(id(self))}"
+        txt += f"<h3>batch.journal</h3>"
+        txt += f"<blockquote>{self.journal._repr_html_()}</blockquote>"
+        txt += f"<h3>batch.experiment</h3>"
+        txt += f"<blockquote>{self.experiment._repr_html_()}</blockquote>"
+
+        return txt
+
     def __len__(self):
         return len(self.experiment)
 
     def __iter__(self):
         return self.experiment.__iter__()
 
     def show_pages(self, number_of_rows=5):
@@ -163,80 +180,89 @@
     @property
     def name(self):
         return self.experiment.journal.name
 
     def _check_cell_raw(self, cell_id):
         try:
             c = self.experiment.data[cell_id]
-            return len(c.cell.raw)
-        except Exception:
+            return len(c.data.raw)
+        except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
             return None
 
     def _check_cell_steps(self, cell_id):
         try:
             c = self.experiment.data[cell_id]
-            return len(c.cell.steps)
-        except Exception:
+            return len(c.data.steps)
+        except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
             return None
 
     def _check_cell_summary(self, cell_id):
         try:
             c = self.experiment.data[cell_id]
-            return len(c.cell.summary)
+            return len(c.data.summary)
         except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
             return None
 
     def _check_cell_max_cap(self, cell_id):
         try:
             c = self.experiment.data[cell_id]
-            s = c.cell.summary
-            return s[headers_summary.charge_capacity].max()
+            s = c.data.summary
+            return s[headers_summary["charge_capacity_gravimetric"]].max()
 
         except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
             return None
 
     def _check_cell_min_cap(self, cell_id):
         try:
             c = self.experiment.data[cell_id]
-            s = c.cell.summary
-            return s[headers_summary.charge_capacity].min()
+            s = c.data.summary
+            return s[headers_summary["charge_capacity_gravimetric"]].min()
 
         except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
             return None
 
     def _check_cell_avg_cap(self, cell_id):
         try:
             c = self.experiment.data[cell_id]
-            s = c.cell.summary
-            return s[headers_summary.charge_capacity].mean()
+            s = c.data.summary
+            return s[headers_summary["charge_capacity_gravimetric"]].mean()
 
         except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
             return None
 
     def _check_cell_std_cap(self, cell_id):
         try:
             c = self.experiment.data[cell_id]
-            s = c.cell.summary
-            return s[headers_summary.charge_capacity].std()
+            s = c.data.summary
+            return s[headers_summary["charge_capacity_gravimetric"]].std()
 
         except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
             return None
 
     def _check_cell_empty(self, cell_id):
         try:
             c = self.experiment.data[cell_id]
             return c.empty
-        except Exception:
+        except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
             return None
 
     def _check_cell_cycles(self, cell_id):
         try:
             c = self.experiment.data[cell_id]
-            return c.cell.steps[self.headers_step_table.cycle].max()
-        except Exception:
+            return c.data.steps[self.headers_step_table.cycle].max()
+        except Exception as e:
+            logging.debug(f"Exception ignored: {e}")
             return None
 
     def drop(self, cell_label=None):
         """Drop cells from the journal.
 
         If ``cell_label`` is not given, ``cellpy`` will look into the journal for session
         info about bad cells, and if it finds it, it will remove those from the
@@ -382,28 +408,33 @@
         print(
             "Label-based look-up is not supported yet. Performing cell-name based look-up instead."
         )
         return self.experiment.cell_names
 
     @property
     def cells(self) -> Data:
-        """access cells as a Data object (attribute lookup and automatic loading)"""
+        """Access cells as a Data object (attribute lookup and automatic loading)
+
+        Usage (at least in jupyter notebook):
+            Write `b.cells.x` and press <TAB>. Then a pop-up will appear, and you can choose the
+            cell you would like to retrieve.
+        """
         return self.experiment.data
 
     @property
     def cell_summary_headers(self) -> Index:
-        return self.experiment.data[self.experiment.cell_names[0]].cell.summary.columns
+        return self.experiment.data[self.experiment.cell_names[0]].data.summary.columns
 
     @property
     def cell_raw_headers(self) -> Index:
-        return self.experiment.data[self.experiment.cell_names[0]].cell.raw.columns
+        return self.experiment.data[self.experiment.cell_names[0]].data.raw.columns
 
     @property
     def cell_step_headers(self) -> Index:
-        return self.experiment.data[self.experiment.cell_names[0]].cell.steps.columns
+        return self.experiment.data[self.experiment.cell_names[0]].data.steps.columns
 
     @property
     def pages(self) -> pd.DataFrame:
         return self.experiment.journal.pages
 
     @pages.setter
     def pages(self, df: pd.DataFrame):
@@ -423,14 +454,57 @@
         # self.experiment.journal = new
         raise NotImplementedError(
             "Setting a new journal object directly on a "
             "batch object is not allowed at the moment. Try modifying "
             "the journal.pages instead."
         )
 
+    def old_duplicate_journal(self, folder=None) -> None:
+        """Copy the journal to folder.
+
+        Args:
+            folder (str or pathlib.Path): folder to copy to (defaults to the
+            current folder).
+        """
+
+        logging.debug(f"duplicating journal to folder {folder}")
+        journal_name = pathlib.Path(self.experiment.journal.file_name)
+        if not journal_name.is_file():
+            logging.info("No journal saved")
+            return
+        new_journal_name = journal_name.name
+        if folder is not None:
+            new_journal_name = pathlib.Path(folder) / new_journal_name
+        try:
+            shutil.copy(journal_name, new_journal_name)
+        except shutil.SameFileError:
+            logging.debug("same file exception encountered")
+
+    def duplicate_journal(self, folder=None) -> None:
+        """Copy the journal to folder.
+
+        Args:
+            folder (str or pathlib.Path): folder to copy to (defaults to the
+            current folder).
+        """
+        self.experiment.journal.duplicate_journal(folder)
+        #
+        # logging.debug(f"duplicating journal to folder {folder}")
+        # journal_name = pathlib.Path(self.experiment.journal.file_name)
+        # if not journal_name.is_file():
+        #     logging.info("No journal saved")
+        #     return
+        # new_journal_name = journal_name.name
+        # if folder is not None:
+        #     new_journal_name = pathlib.Path(folder) / new_journal_name
+        # try:
+        #     shutil.copy(journal_name, new_journal_name)
+        # except shutil.SameFileError:
+        #     logging.debug("same file exception encountered")
+
     def create_journal(self, description=None, from_db=True, **kwargs):
         """Create journal pages.
 
             This method is a wrapper for the different Journal methods for making
             journal pages (Batch.experiment.journal.xxx). It is under development. If you
             want to use 'advanced' options (i.e. not loading from a db), please consider
             using the methods available in Journal for now.
@@ -477,35 +551,53 @@
                 file_list (list of str): perform the search within a given list
                     of filenames instead of searching the folder(s). The list should
                     not contain the full filepath (only the actual file names). If
                     you want to provide the full path, you will have to modify the
                     file_name_format or reg_exp accordingly.
                 pre_path (path or str): path to prepend the list of files selected
                      from the file_list.
+            kwargs -> journal.to_file:
+                duplicate_to_local_folder (bool): default True.
+
 
         Returns:
             info_dict
 
         """
 
         # TODO (jepe): create option to update journal without looking for files
 
         logging.debug("Creating a journal")
         logging.debug(f"description: {description}")
         logging.debug(f"from_db: {from_db}")
         logging.info(f"name: {self.experiment.journal.name}")
         logging.info(f"project: {self.experiment.journal.project}")
+        to_project_folder = kwargs.pop("to_project_folder", True)
+        duplicate_to_local_folder = kwargs.pop("duplicate_to_local_folder", True)
 
         if description is not None:
             from_db = False
+        else:
+            if self.experiment.journal.pages is not None:
+                warnings.warn(
+                    "You created a journal - but you already have a "
+                    "journal. Hope you know what you are doing!"
+                )
 
         if from_db:
             self.experiment.journal.from_db(**kwargs)
-            self.experiment.journal.to_file()
-            self.duplicate_journal(prms.Paths.batchfiledir)
+            self.experiment.journal.to_file(
+                duplicate_to_local_folder=duplicate_to_local_folder
+            )
+
+            # TODO: remove these:
+            if duplicate_to_local_folder:
+                self.experiment.journal.duplicate_journal()
+            if to_project_folder:
+                self.duplicate_journal(prms.Paths.batchfiledir)
 
         else:
             is_str = isinstance(description, str)
             is_file = False
 
             if is_str and pathlib.Path(description).is_file():
                 description = pathlib.Path(description)
@@ -519,15 +611,14 @@
                 logging.info(f"loading file {description}")
                 if description.suffix in [".json", ".xlsx"]:
                     self.experiment.journal.from_file(description)
                 else:
                     warnings.warn("unknown file extension")
 
             else:
-
                 if is_str and description.lower() == "empty":
                     logging.debug("creating empty journal pages")
 
                     self.experiment.journal.pages = (
                         self.experiment.journal.create_empty_pages()
                     )
 
@@ -559,15 +650,14 @@
                     )
                     for k in self.experiment.journal.pages.columns:
                         try:
                             value = description[k]
                         except KeyError:
                             warnings.warn(f"missing key: {k}")
                         else:
-
                             if not isinstance(value, list):
                                 warnings.warn("encountered item that is not a list")
                                 logging.debug(f"converting '{k}' to list-type")
                                 value = [value]
                             if k == "raw_file_names":
                                 if not isinstance(value[0], list):
                                     warnings.warn(
@@ -597,15 +687,17 @@
                         "an unknown type or a file not found"
                     )
                     logging.info(
                         "did not understand the option - creating empty journal pages"
                     )
 
             # finally
-            self.experiment.journal.to_file()
+            self.experiment.journal.to_file(
+                duplicate_to_local_folder=duplicate_to_local_folder
+            )
             self.experiment.journal.generate_folder_names()
             self.experiment.journal.paginate()
             self.duplicate_journal(prms.Paths.batchfiledir)
 
     def create_folder_structure(self) -> None:
         warnings.warn("Deprecated - use paginate instead.", DeprecationWarning)
         self.experiment.journal.paginate()
@@ -638,35 +730,14 @@
         if filename is None:
             filename = self.experiment.journal.file_name
         filename = pathlib.Path(filename).with_suffix(".xlsx")
         self.experiment.journal.to_file(
             file_name=filename, to_project_folder=False, paginate=False
         )
 
-    def duplicate_journal(self, folder=None) -> None:
-        """Copy the journal to folder.
-
-        Args:
-            folder (str or pathlib.Path): folder to copy to (defaults to the
-            current folder).
-        """
-
-        logging.debug(f"duplicating journal to folder {folder}")
-        journal_name = pathlib.Path(self.experiment.journal.file_name)
-        if not journal_name.is_file():
-            logging.info("No journal saved")
-            return
-        new_journal_name = journal_name.name
-        if folder is not None:
-            new_journal_name = pathlib.Path(folder) / new_journal_name
-        try:
-            shutil.copy(journal_name, new_journal_name)
-        except shutil.SameFileError:
-            logging.debug("same file exception encountered")
-
     def duplicate_cellpy_files(
         self, location: str = "standard", selector: dict = None, **kwargs
     ) -> None:
         """Copy the cellpy files and make a journal with the new names available in
         the current folder.
 
         Args:
@@ -683,15 +754,15 @@
             kwargs: sent to update if selector is provided
 
         Returns:
             The updated journal pages.
         """
 
         pages = self.experiment.journal.pages
-        cellpy_file_dir = pathlib.Path(prms.Paths.cellpydatadir)
+        cellpy_file_dir = OtherPath(prms.Paths.cellpydatadir)
 
         if location == "standard":
             batch_data_dir = pathlib.Path("data") / "interim"
 
         elif location == "here":
             batch_data_dir = pathlib.Path(".")
 
@@ -752,25 +823,25 @@
             self.summary_collector.do(reset=True)
 
     def load(self) -> None:
         # does the same as update
         warnings.warn("Deprecated - use update instead.", DeprecationWarning)
         self.experiment.update()
 
-    def update(self, **kwargs) -> None:
+    def update(self, pool=False, **kwargs) -> None:
         """Updates the selected datasets.
 
-        Args:
+        Keyword Args (to experiment-instance):
             all_in_memory (bool): store the `cellpydata` in memory (default
                 False)
             cell_specs (dict of dicts): individual arguments pr. cell. The `cellspecs` key-word argument
                 dictionary will override the **kwargs and the parameters from the journal pages
                 for the indicated cell.
 
-        kwargs:
+        Additional kwargs:
             transferred all the way to the instrument loader, if not
             picked up earlier. Remark that you can obtain the same pr. cell by
             providing a `cellspecs` dictionary. The kwargs have precedence over the
             parameters given in the journal pages, but will be overridden by parameters
             given by `cellspecs`.
 
             Merging:
@@ -778,23 +849,39 @@
                     cycle numbers etc. when merging several data-sets.
             Loading:
                 selector (dict): selector-based parameters sent to the cellpy-file loader (hdf5) if
                 loading from raw is not necessary (or turned off).
 
         """
         self.experiment.errors["update"] = []
-        self.experiment.update(**kwargs)
+        if pool:
+            self.experiment.parallel_update(**kwargs)
+        else:
+            self.experiment.update(**kwargs)
 
     def export_cellpy_files(self, path=None, **kwargs) -> None:
         if path is None:
             path = pathlib.Path(".").resolve()
         self.experiment.errors["export_cellpy_files"] = []
         self.experiment.export_cellpy_files(path=path, **kwargs)
 
     def recalc(self, **kwargs) -> None:
+        """Run make_step_table and make_summary on all cells.
+
+        Keyword Args:
+            save (bool): Save updated cellpy-files if True (defaults to True).
+            step_opts (dict): parameters to inject to make_steps (defaults to None).
+            summary_opts (dict): parameters to inject to make_summary (defaults to None).
+            indexes (list): Only recalculate for given indexes (i.e. list of cell-names) (defaults to None).
+            calc_steps (bool): Run make_steps before making the summary (defaults to True).
+            testing (bool): Only for testing purposes (defaults to False).
+
+        Returns:
+            None
+        """
         self.experiment.errors["recalc"] = []
         self.experiment.recalc(**kwargs)
 
     def make_summaries(self) -> None:
         warnings.warn("Deprecated - use combine_summaries instead.", DeprecationWarning)
 
         self.exporter.do()
@@ -860,18 +947,17 @@
         >>> empty_batch = Batch.init(db_reader=None)
         >>> batch_from_file = Batch.init(file_name="cellpy_batch_my_experiment.json")
         >>> normal_init_of_batch = Batch.init()
     """
     # TODO: add option for setting max cycle number (experiment.last_cycle)
     # set up cellpy logger
     default_log_level = kwargs.pop("default_log_level", None)
-    testing = kwargs.pop("testing", None)
+    testing = kwargs.pop("testing", False)
     file_name = kwargs.pop("file_name", None)
     frame = kwargs.pop("frame", None)
-
     log.setup_logging(
         default_level=default_log_level, testing=testing, reset_big_log=True
     )
 
     logging.debug(f"returning Batch(kwargs: {kwargs})")
     if file_name is not None:
         kwargs.pop("db_reader", None)
@@ -879,18 +965,18 @@
     if frame is not None:
         kwargs.pop("db_reader", None)
         return Batch(*args, file_name=None, db_reader=None, frame=frame, **kwargs)
 
     return Batch(*args, **kwargs)
 
 
-def from_journal(journal_file, autolink=True) -> Batch:
+def from_journal(journal_file, autolink=True, testing=False) -> Batch:
     """Create a Batch from a journal file"""
     # TODO: add option for setting max cycle number (experiment.last_cycle)
-    b = init(db_reader=None, file_name=journal_file)
+    b = init(db_reader=None, file_name=journal_file, testing=testing)
     if autolink:
         b.link()
     return b
 
 
 def load_pages(file_name) -> pd.DataFrame:
     """Retrieve pages from a Journal file.
@@ -946,16 +1032,18 @@
     dpi = kwargs.pop("dpi", 300)
 
     default_log_level = kwargs.pop("default_log_level", "CRITICAL")
     if len(args) == 1:
         file_name = args[0]
     else:
         file_name = kwargs.pop("file_name", None)
-
-    log.setup_logging(default_level=default_log_level, reset_big_log=True)
+    testing = kwargs.pop("testing", False)
+    log.setup_logging(
+        default_level=default_log_level, reset_big_log=True, testing=testing
+    )
     logging.debug(f"creating Batch(kwargs: {kwargs})")
 
     if file_name is not None:
         kwargs.pop("db_reader", None)
         b = Batch(*args, file_name=file_name, db_reader=None, **kwargs)
         b.create_journal(file_name)
     else:
```

### Comparing `cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_analyzers.py` & `cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_analyzers.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,197 +1,196 @@
-import logging
-
-import pandas as pd
-
-from cellpy.exceptions import UnderDefined
-from cellpy.utils.batch_tools.batch_core import BaseAnalyzer
-from cellpy.utils.batch_tools.dumpers import ram_dumper
-from cellpy.utils.batch_tools.engines import summary_engine
-from cellpy.utils.ocv_rlx import select_ocv_points
-
-
-class BaseSummaryAnalyzer(BaseAnalyzer):
-    def __init__(self):
-        super().__init__()
-        self._assign_engine(summary_engine)
-        self._assign_dumper(ram_dumper)
-
-
-class ICAAnalyzer(BaseAnalyzer):
-    def __init__(self):
-        super().__init__()
-
-
-class EISAnalyzer(BaseAnalyzer):
-    def __init__(self):
-        super().__init__()
-
-
-class OCVRelaxationAnalyzer(BaseAnalyzer):
-    """Analyze open curcuit relaxation curves.
-
-    This analyzer is still under development.
-    (Partly) implented so far: select_ocv_points -> farms.
-    To get the DataFrames from the farms, you can use
-    >>> ocv_point_frames = OCVRelaxationAnalyzer.last
-
-    Attributes:
-        selection_method: criteria for selecting points
-            (martin: select first and last, and then last/2, last/2/2 etc. until you have reached the wanted number of points; fixed_time: select first, and same interval; defaults to "martin")
-        number_of_points: number of points you want.
-            defaults to 5
-        interval: interval between each point (in use only for methods
-            where interval makes sense). If it is a list, then
-            number_of_points will be calculated as len(interval) + 1 (and
-            override the set number_of_points).
-            defaults to 10
-        relative_voltage: set to True if you would like the voltage to be
-            relative to the voltage before starting the ocv rlx step.
-            Defaults to False. Remark that for the initial rxl step (when
-            you just have put your cell on the tester) does not have any
-            prior voltage. The relative voltage will then be versus the
-            first measurement point.
-            defaults to False
-        report_times: also report the ocv rlx total time if True (defaults
-            to False)
-        direction ("up", "down" or "both"): select "up" if you would like
-            to process only the ocv rlx steps where the voltage is relaxing
-            upwards and vize versa. Defaults to "both
-
-    Notes:
-        This analyzer is not working as intended yet. Todos:
-
-        - include better engine-dumper methodology and dump
-          stuff to both memory and file(s)
-          (should add this to BaseAnalyser)
-        - recieve settings and parameters
-        - option (dumper) for plotting?
-        - automatic fitting of OCV rlx data?
-    """
-
-    def __init__(self):
-        super().__init__()
-        self.engines = []
-        self.dumpers = []
-        self.current_engine = None
-        self._assign_engine(self.ocv_points_engine)
-        # self._assign_dumper(self.screen_dumper)
-        # prms for select_ocv_points
-        self.selection_method = "martin"
-        self.number_of_points = 5
-        self.interval = 10
-        self.relative_voltage = False
-        self.report_times = False
-        self.include_times = True
-        self.direction = None
-
-    def _assign_engine(self, engine):
-        self.engines.append(engine)
-
-    def _assign_dumper(self, dumper):
-        self.dumpers.append(dumper)
-
-    def screen_dumper(self, **kwargs):
-        for farm in self.farms:
-            print(farm)
-
-    @property
-    def last(self) -> list:
-        return self.farms[-1]
-
-    @property
-    def dframe(self) -> pd.DataFrame:
-        return self.farms[-1][-1]
-
-    def run_engine(self, engine):
-        logging.debug(f"start engine::{engine.__name__}]")
-
-        self.current_engine = engine
-
-        self.farms, self.barn = engine(experiments=self.experiments, farms=self.farms)
-        logging.debug("::engine ended")
-
-    def run_dumper(self, dumper):
-        logging.debug(f"start dumper::{dumper.__name__}]")
-        dumper(
-            experiments=self.experiments,
-            farms=self.farms,
-            barn=self.barn,
-            engine=self.current_engine,
-        )
-        logging.debug("::dumper ended")
-
-    def do(self):
-        if not self.experiments:
-            raise UnderDefined("cannot run until you have assigned an experiment")
-
-        for engine in self.engines:
-            self.empty_the_farms()
-            logging.debug(f"running - {str(engine)}")
-            self.run_engine(engine)
-
-            for dumper in self.dumpers:
-                logging.debug(f"exporting - {str(dumper)}")
-                self.run_dumper(dumper)
-
-    def ocv_points_engine(self, **kwargs):
-        experiments = kwargs["experiments"]
-        farms = kwargs["farms"]
-        barn = None
-        for experiment, farm in zip(experiments, farms):
-            dframes = []
-            for cell_label in experiment.cell_data_frames:
-
-                logging.info(f"Analyzing {cell_label}")
-                if experiment.all_in_memory:
-                    logging.debug("CellpyData picked from memory")
-                    cell = experiment.cell_data_frames[cell_label]
-                    if cell.empty:
-                        logging.warning("Oh-no! Empty CellpyData-object")
-                else:
-                    logging.debug("CellpyData loaded from Cellpy-file")
-                    cell = experiment.data[cell_label]
-                    if cell.empty:
-                        logging.warning("Oh-no! Empty CellpyData-object")
-                df = select_ocv_points(
-                    cell,
-                    cell_label=cell_label,
-                    include_times=self.include_times,
-                    selection_method=self.selection_method,
-                    number_of_points=self.number_of_points,
-                    interval=self.interval,
-                    relative_voltage=self.relative_voltage,
-                    report_times=self.report_times,
-                    direction=self.direction,
-                )
-                dframes.append(df)
-            concat_df = pd.concat(dframes)
-            farm.append(concat_df)
-        return farms, barn
-
-    def do2(self):
-        for experiment, farm in zip(self.experiments, self.farms):
-            for cell_label in experiment.cell_data_frames:
-                logging.info(f"Analyzing {cell_label}")
-                if experiment.all_in_memory:
-                    logging.debug("CellpyData picked from memory")
-                    cell = experiment.cell_data_frames[cell_label]
-                    if cell.empty:
-                        logging.warning("Oh-no! Empty CellpyData-object")
-                else:
-                    logging.debug("CellpyData loaded from Cellpy-file")
-                    cell = experiment.data[cell_label]
-                    if cell.empty:
-                        logging.warning("Oh-no! Empty CellpyData-object")
-
-                df = select_ocv_points(
-                    cell,
-                    cell_label=cell_label,
-                    selection_method=self.selection_method,
-                    number_of_points=self.number_of_points,
-                    interval=self.interval,
-                    relative_voltage=self.relative_voltage,
-                    report_times=self.report_times,
-                    direction=self.direction,
-                )
-                farm.append(df)
-        return self.farms
+import logging
+
+import pandas as pd
+
+from cellpy.exceptions import UnderDefined
+from cellpy.utils.batch_tools.batch_core import BaseAnalyzer
+from cellpy.utils.batch_tools.dumpers import ram_dumper
+from cellpy.utils.batch_tools.engines import summary_engine
+from cellpy.utils.ocv_rlx import select_ocv_points
+
+
+class BaseSummaryAnalyzer(BaseAnalyzer):
+    def __init__(self):
+        super().__init__()
+        self._assign_engine(summary_engine)
+        self._assign_dumper(ram_dumper)
+
+
+class ICAAnalyzer(BaseAnalyzer):
+    def __init__(self):
+        super().__init__()
+
+
+class EISAnalyzer(BaseAnalyzer):
+    def __init__(self):
+        super().__init__()
+
+
+class OCVRelaxationAnalyzer(BaseAnalyzer):
+    """Analyze open curcuit relaxation curves.
+
+    This analyzer is still under development.
+    (Partly) implented so far: select_ocv_points -> farms.
+    To get the DataFrames from the farms, you can use
+    >>> ocv_point_frames = OCVRelaxationAnalyzer.last
+
+    Attributes:
+        selection_method: criteria for selecting points
+            (martin: select first and last, and then last/2, last/2/2 etc. until you have reached the wanted number of points; fixed_time: select first, and same interval; defaults to "martin")
+        number_of_points: number of points you want.
+            defaults to 5
+        interval: interval between each point (in use only for methods
+            where interval makes sense). If it is a list, then
+            number_of_points will be calculated as len(interval) + 1 (and
+            override the set number_of_points).
+            defaults to 10
+        relative_voltage: set to True if you would like the voltage to be
+            relative to the voltage before starting the ocv rlx step.
+            Defaults to False. Remark that for the initial rxl step (when
+            you just have put your cell on the tester) does not have any
+            prior voltage. The relative voltage will then be versus the
+            first measurement point.
+            defaults to False
+        report_times: also report the ocv rlx total time if True (defaults
+            to False)
+        direction ("up", "down" or "both"): select "up" if you would like
+            to process only the ocv rlx steps where the voltage is relaxing
+            upwards and vize versa. Defaults to "both
+
+    Notes:
+        This analyzer is not working as intended yet. Todos:
+
+        - include better engine-dumper methodology and dump
+          stuff to both memory and file(s)
+          (should add this to BaseAnalyser)
+        - recieve settings and parameters
+        - option (dumper) for plotting?
+        - automatic fitting of OCV rlx data?
+    """
+
+    def __init__(self):
+        super().__init__()
+        self.engines = []
+        self.dumpers = []
+        self.current_engine = None
+        self._assign_engine(self.ocv_points_engine)
+        # self._assign_dumper(self.screen_dumper)
+        # prms for select_ocv_points
+        self.selection_method = "martin"
+        self.number_of_points = 5
+        self.interval = 10
+        self.relative_voltage = False
+        self.report_times = False
+        self.include_times = True
+        self.direction = None
+
+    def _assign_engine(self, engine):
+        self.engines.append(engine)
+
+    def _assign_dumper(self, dumper):
+        self.dumpers.append(dumper)
+
+    def screen_dumper(self, **kwargs):
+        for farm in self.farms:
+            print(farm)
+
+    @property
+    def last(self) -> list:
+        return self.farms[-1]
+
+    @property
+    def dframe(self) -> pd.DataFrame:
+        return self.farms[-1][-1]
+
+    def run_engine(self, engine):
+        logging.debug(f"start engine::{engine.__name__}]")
+
+        self.current_engine = engine
+
+        self.farms, self.barn = engine(experiments=self.experiments, farms=self.farms)
+        logging.debug("::engine ended")
+
+    def run_dumper(self, dumper):
+        logging.debug(f"start dumper::{dumper.__name__}]")
+        dumper(
+            experiments=self.experiments,
+            farms=self.farms,
+            barn=self.barn,
+            engine=self.current_engine,
+        )
+        logging.debug("::dumper ended")
+
+    def do(self):
+        if not self.experiments:
+            raise UnderDefined("cannot run until you have assigned an experiment")
+
+        for engine in self.engines:
+            self.empty_the_farms()
+            logging.debug(f"running - {str(engine)}")
+            self.run_engine(engine)
+
+            for dumper in self.dumpers:
+                logging.debug(f"exporting - {str(dumper)}")
+                self.run_dumper(dumper)
+
+    def ocv_points_engine(self, **kwargs):
+        experiments = kwargs["experiments"]
+        farms = kwargs["farms"]
+        barn = None
+        for experiment, farm in zip(experiments, farms):
+            dframes = []
+            for cell_label in experiment.cell_data_frames:
+                logging.info(f"Analyzing {cell_label}")
+                if experiment.all_in_memory:
+                    logging.debug("CellpyData picked from memory")
+                    cell = experiment.cell_data_frames[cell_label]
+                    if cell.empty:
+                        logging.warning("Oh-no! Empty CellpyData-object")
+                else:
+                    logging.debug("CellpyData loaded from Cellpy-file")
+                    cell = experiment.data[cell_label]
+                    if cell.empty:
+                        logging.warning("Oh-no! Empty CellpyData-object")
+                df = select_ocv_points(
+                    cell,
+                    cell_label=cell_label,
+                    include_times=self.include_times,
+                    selection_method=self.selection_method,
+                    number_of_points=self.number_of_points,
+                    interval=self.interval,
+                    relative_voltage=self.relative_voltage,
+                    report_times=self.report_times,
+                    direction=self.direction,
+                )
+                dframes.append(df)
+            concat_df = pd.concat(dframes)
+            farm.append(concat_df)
+        return farms, barn
+
+    def do2(self):
+        for experiment, farm in zip(self.experiments, self.farms):
+            for cell_label in experiment.cell_data_frames:
+                logging.info(f"Analyzing {cell_label}")
+                if experiment.all_in_memory:
+                    logging.debug("CellpyData picked from memory")
+                    cell = experiment.cell_data_frames[cell_label]
+                    if cell.empty:
+                        logging.warning("Oh-no! Empty CellpyData-object")
+                else:
+                    logging.debug("CellpyData loaded from Cellpy-file")
+                    cell = experiment.data[cell_label]
+                    if cell.empty:
+                        logging.warning("Oh-no! Empty CellpyData-object")
+
+                df = select_ocv_points(
+                    cell,
+                    cell_label=cell_label,
+                    selection_method=self.selection_method,
+                    number_of_points=self.number_of_points,
+                    interval=self.interval,
+                    relative_voltage=self.relative_voltage,
+                    report_times=self.report_times,
+                    direction=self.direction,
+                )
+                farm.append(df)
+        return self.farms
```

### Comparing `cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_core.py` & `cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_core.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 import abc
 import collections
 import logging
 import os
+import random
 
 from cellpy import cellreader, prms
 from cellpy.exceptions import UnderDefined
 from cellpy.parameters.internal_settings import get_headers_journal
 from cellpy.utils.batch_tools import batch_helpers as helper
 
 #  import box
@@ -57,22 +58,35 @@
 
         args = self._validate_base_experiment_type(args)
         if args:
             self.experiments.extend(args)
             self.farms.append(empty_farm)
 
     def _assign_engine(self, engine):
-
         self.engines.append(engine)
 
     def _assign_dumper(self, dumper):
         self.dumpers.append(dumper)
 
     @abc.abstractmethod
     def run_engine(self, engine, **kwargs):
+        """Set the current_engine and run it.
+
+        The method sets and engages the engine (callable) and provide
+        appropriate binding to at least the class attributes self.farms and
+        self.barn.
+
+        Example:
+            self.current_engine = engine
+            self.farms, self.barn = engine(experiments=self.experiments, farms=self.farms, **kwargs)
+
+        Args:
+            engine (callable): the function that should be called.
+            **kwargs: additional keyword arguments sent to the callable.
+        """
         pass
 
     @abc.abstractmethod
     def run_dumper(self, dumper):
         pass
 
     def __str__(self):
@@ -131,21 +145,22 @@
 class Data(collections.UserDict):
     """Class that is used to access the experiment.journal.pages DataFrame.
 
     The Data class loads the complete cellpy-file if raw-data is not already
     loaded in memory. In future version, it could be that the Data object
     will return a link allowing querying instead to save memory usage...
 
-    Remark that some cellpy (cellreader.CellpyData) function might not work if
+    Remark that some cellpy (cellreader.CellpyCell) function might not work if
     you have the raw-data in memory, but not summary data (if the cellpy function
     requires summary data or other settings not set as default).
     """
 
     # TODO (jepe): decide if we should included querying functionality here.
     # TODO (jepe): implement experiment.last_cycle
+    # TODO (jepe): consider renaming for v1.0.0 (Cell will be renamed to Data).
 
     def __init__(self, experiment, *args):
         super().__init__(*args)
         self.experiment = experiment
         self.query_mode = False
         self.accessor_pre = "x_"
         self.accessors = {}
@@ -197,48 +212,67 @@
             item = self._create_cell_label(item)
             return self.__getitem__(item)
         else:
             return super().__getattribute__(item)
 
     def __look_up__(self, cell_id):
         try:
-            if not self.experiment.cell_data_frames[cell_id].cell.raw.empty:
+            if not self.experiment.cell_data_frames[cell_id].data.raw.empty:
                 return self.experiment.cell_data_frames[cell_id]
             else:
                 raise AttributeError
 
         except AttributeError:
             logging.debug("Need to do a look-up from the cellpy file")
             # last_cycle = self.experiment.max_cycle
             pages = self.experiment.journal.pages
             info = pages.loc[cell_id, :]
-            cellpy_file = info[hdr_journal.cellpy_file_name]
+            cellpy_file = str(info[hdr_journal.cellpy_file_name])
+
             # linking (query_mode) not implemented yet - loading whole file in mem instead
             if not self.query_mode:
                 # TODO: modify _load_cellpy_file so that it can select parts of the data (max_cycle etc)
-                cell = self.experiment._load_cellpy_file(cellpy_file)
+                cell = self.experiment._load_cellpy_file(cellpy_file)  # noqa
                 self.experiment.cell_data_frames[cell_id] = cell
                 # trick for making tab-completion work:
                 self.accessors[
                     self._create_accessor_label(cell_id)
                 ] = self.experiment.cell_data_frames[cell_id]
                 return cell
             else:
                 raise NotImplementedError
 
+    def sample(self):
+        """Pick out one random cell from the batch"""
+        cell_labels = self.experiment.journal.pages.index
+        cell_id = random.choice(cell_labels)
+        return self.__look_up__(cell_id)
+
+    def first(self):
+        """Pick out first cell from the batch"""
+        cell_labels = self.experiment.journal.pages.index
+        cell_id = cell_labels[0]
+        return self.__look_up__(cell_id)
+
+    def last(self):
+        """Pick out last cell from the batch"""
+        cell_labels = self.experiment.journal.pages.index
+        cell_id = cell_labels[-1]
+        return self.__look_up__(cell_id)
+
 
 class BaseExperiment(metaclass=abc.ABCMeta):
     """An experiment contains experimental data and meta-data."""
 
     def __init__(self, *args):
         self.journal = None
         self.summary_frames = None
         self.cell_data_frames = dict()
         self.memory_dumped = dict()
-        self.parent_level = "CellpyData"
+        self.parent_level = "CellpyCell"
         self.log_level = "CRITICAL"
         self._data = None
         self._store_data_object = True
         self._cellpy_object = None
         self.limit = 10
         self._max_cycle = None
 
@@ -281,38 +315,39 @@
                     self._link_cellpy_file(cell_label)
                     cellpy_object = self.data[cell_label]
                 except (IOError, KeyError, UnderDefined):
                     raise StopIteration
             return cellpy_object
 
     def _link_cellpy_file(self, cell_label, max_cycle=None):
+        # creates a CellpyCell object and loads only the step-table
         logging.debug("linking cellpy file")
         cellpy_file_name = self.journal.pages.loc[
             cell_label, hdr_journal.cellpy_file_name
         ]
         if not os.path.isfile(cellpy_file_name):
             raise IOError
 
-        cellpy_object = cellreader.CellpyData(initialize=True)
+        cellpy_object = cellreader.CellpyCell(initialize=True)
         step_table = helper.look_up_and_get(
             cellpy_file_name, prms._cellpyfile_step, max_cycle=max_cycle
         )
         if step_table.empty:
             raise UnderDefined
         if max_cycle:
             cellpy_object.overwrite_able = False
             self.max_cycle = max_cycle
-        cellpy_object.cell.steps = step_table
+        cellpy_object.data.steps = step_table
         self._data = None
         self.cell_data_frames[cell_label] = cellpy_object
 
     def _load_cellpy_file(self, file_name):
         # TODO: modify this so that it can select parts of the data (max_cycle etc)
         selector = dict()
-        cellpy_data = cellreader.CellpyData()
+        cellpy_data = cellreader.CellpyCell()
         if self.max_cycle:
             cellpy_data.overwrite_able = False
             selector["max_cycle"] = self.max_cycle
         cellpy_data.load(file_name, self.parent_level, selector=selector)
         logging.info(f" <- grabbing ( {file_name} )")
         return cellpy_data
 
@@ -382,15 +417,15 @@
     packable = ["name", "project", "time_stamp", "project_dir", "batch_dir", "raw_dir"]
 
     def __init__(self):
         self.pages = None  # pandas.DataFrame
         self.session = None  # dictionary
         self.name = None
         self.project = None
-        self.file_name = None
+        self.file_name = None  # This is the file-path to the "True" journal file
         self.time_stamp = None
         self.project_dir = None
         self.batch_dir = None
         self.raw_dir = None
 
     def __str__(self):
         return (
@@ -421,15 +456,15 @@
                     logging.debug(f"unknown variable encountered: {p}")
 
     def from_db(self):
         """Make journal pages by looking up a database.
 
         Default to using the simple excel "database" provided by cellpy.
 
-        If you dont have a database or you dont know how to make and use one,
+        If you don't have a database, or you don't know how to make and use one,
         look in the cellpy documentation for other solutions
         (e.g. manually create a file that can be loaded by the ``from_file``
         method).
         """
         logging.debug("not implemented")
 
     def from_file(self, file_name):
@@ -460,15 +495,15 @@
     """An exporter exports your data to a given format."""
 
     def __init__(self, *args):
         super().__init__(*args)
         self._use_dir = None
         self.current_engine = None
 
-    def run_engine(self, engine):
+    def run_engine(self, engine, **kwargs):
         logging.debug(f"start engine::{engine.__name__}")
         self.current_engine = engine
         self.farms, self.barn = engine(experiments=self.experiments, farms=self.farms)
         logging.debug("::engine ended")
 
     def run_dumper(self, dumper):
         logging.debug(f"start dumper::{dumper.__name__}")
```

### Comparing `cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_experiments.py` & `cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_experiments.py`

 * *Files 22% similar despite different names*

```diff
@@ -4,17 +4,19 @@
 import pathlib
 import sys
 import warnings
 
 import pandas as pd
 from tqdm.auto import tqdm
 
+import cellpy
 from cellpy import prms
 from cellpy.parameters.internal_settings import get_headers_journal, get_headers_summary
 from cellpy.readers import cellreader
+from cellpy.internals.core import OtherPath
 from cellpy.utils.batch_tools import batch_helpers as helper
 from cellpy.utils.batch_tools.batch_core import BaseExperiment
 from cellpy.utils.batch_tools.batch_journals import LabJournal
 
 hdr_journal = get_headers_journal()
 hdr_summary = get_headers_summary()
 
@@ -80,35 +82,68 @@
         self.last_cycle = None
         self.nom_cap = None
         self.instrument = None
         self.custom_data_folder = None
 
         self.selected_summaries = None
 
+    def _repr_html_(self):
+        txt = f"<h2>CyclingExperiment-object</h2> id={hex(id(self))}"
+        txt += "<h3>Main attributes</h3>"
+        txt += f"""
+        <table>
+            <thead>
+                <tr>
+                    <th>Attribute</th>
+                    <th>Value</th>
+                </tr>
+            </thead>
+            <tbody>
+                <tr><td><b>force_cellpy</b></td><td>{self.force_cellpy}</td></tr>
+                <tr><td><b>force_raw</b></td><td>{self.force_raw}</td></tr>
+                <tr><td><b>force_recalc</b></td><td>{self.force_recalc}</td></tr>
+                <tr><td><b>save_cellpy</b></td><td>{self.save_cellpy}</td></tr>
+                <tr><td><b>accept_errors</b></td><td>{self.accept_errors}</td></tr>
+                <tr><td><b>all_in_memory</b></td><td>{self.all_in_memory}</td></tr>
+                <tr><td><b>export_cycles</b></td><td>{self.export_cycles}</td></tr>
+                <tr><td><b>shifted_cycles</b></td><td>{self.shifted_cycles}</td></tr>
+                <tr><td><b>export_raw</b></td><td>{self.export_raw}</td></tr>
+                <tr><td><b>export_ica</b></td><td>{self.export_ica}</td></tr>
+                <tr><td><b>last_cycle</b></td><td>{self.last_cycle}</td></tr>
+                <tr><td><b>nom_cap</b></td><td>{self.nom_cap}</td></tr>
+                <tr><td><b>instrument</b></td><td>{self.instrument}</td></tr>
+                <tr><td><b>custom_data_folder</b></td><td>{self.custom_data_folder}</td></tr>
+                <tr><td><b>selected_summaries</b></td><td>{self.selected_summaries}</td></tr>
+            </tbody>
+        </table>
+        """
+        txt += "<h3>Cells</h3>"
+        txt += f"<p><b>data</b>: contains {len(self)} cells.</p>"
+        return txt
+
     @staticmethod
     def _get_cell_spec_from_page(indx: int, row: pd.Series) -> dict:
         # Edit this if we decide to make "argument families", e.g. loader_split or merger_recalc.
 
         PRM_SPLITTER = ";"
         EQUAL_SIGN = "="
 
         def _arg_parser(text: str) -> None:
             individual_specs = text.split(PRM_SPLITTER)
             for p in individual_specs:
                 p, a = p.split(EQUAL_SIGN)
 
-        logging.debug(f"getting cell_spec from journal pages ({indx}: {row})")
+        logging.debug(f"getting cell_spec from journal pages ({indx})")
         try:
             cell_spec = row[hdr_journal.argument]
             logging.debug(cell_spec)
             if not isinstance(cell_spec, dict):
                 raise TypeError("the cell spec argument is not a dictionary")
         except Exception as e:
             logging.warning(f"could not get cell spec for {indx}")
-            logging.warning(f"row: {row}")
             logging.warning(f"error message: {e}")
             return {}
 
         # converting from str if needed
         for spec in cell_spec:
             if isinstance(cell_spec[spec], str):
                 if cell_spec[spec].lower() == "true":
@@ -126,23 +161,24 @@
                     except ValueError as e:
                         logging.warning(
                             f"ERROR! Could not convert from str to python object!"
                         )
                         logging.debug(e)
         return cell_spec
 
-    def update(self, all_in_memory=None, cell_specs=None, **kwargs):
+    def update(self, all_in_memory=None, cell_specs=None, logging_mode=None, **kwargs):
         """Updates the selected datasets.
 
         Args:
             all_in_memory (bool): store the `cellpydata` in memory (default
                 False)
             cell_specs (dict of dicts): individual arguments pr. cell. The `cellspecs` key-word argument
                 dictionary will override the **kwargs and the parameters from the journal pages
                 for the indicated cell.
+            logging_mode (str): sets the logging mode for the loader(s).
 
             kwargs:
                 transferred all the way to the instrument loader, if not
                 picked up earlier. Remark that you can obtain the same pr. cell by
                 providing a `cellspecs` dictionary. The kwargs have precedence over the
                 parameters given in the journal pages, but will be overridden by parameters
                 given by `cellspecs`.
@@ -150,27 +186,38 @@
                 Merging:
                     recalc (Bool): set to False if you don't want automatic "recalc" of
                         cycle numbers etc. when merging several data-sets.
                 Loading:
                     selector (dict): selector-based parameters sent to the cellpy-file loader (hdf5) if
                     loading from raw is not necessary (or turned off).
 
+                Debugging:
+                    debug (Bool): set to True if you want to run in debug mode (should never be used by non-developers).
+
+        Debug-mode:
+                 - runs only for the first item in your journal
+
         Examples:
             >>> # Don't perform recalculation of cycle numbers etc. when merging
             >>> # All cells:
             >>> b.update(recalc=False)
             >>> # For specific cell(s):
             >>> cell_specs_cell_01 = {"name_of_cell_01": {"recalc": False}}
             >>> b.update(cell_specs=cell_specs_cell_01)
 
         """
 
         # TODO: implement experiment.last_cycle
 
+        debugging = kwargs.pop("debug", False)
+        testing = kwargs.pop("testing", False)
+
+        # --- cleaning up attributes / arguments etc ---
         force_cellpy = kwargs.pop("force_cellpy", self.force_cellpy)
+        force_raw = kwargs.pop("force_raw", self.force_raw)
 
         logging.info("[update experiment]")
         if all_in_memory is not None:
             self.all_in_memory = all_in_memory
 
         logging.info(f"Additional keyword arguments: {kwargs}")
         selector = kwargs.get("selector", None)
@@ -195,173 +242,168 @@
                 "Future version will require instrument in the journal pages."
             )
             pages[hdr_journal.instrument] = x
 
         if pages.empty:
             raise Exception("your journal is empty")
 
+        # --- init ---
         summary_frames = dict()
         cell_data_frames = dict()
         number_of_runs = len(pages)
+        logging.debug(f"You have {number_of_runs} cells in your journal")
         counter = 0
         errors = []
 
         pbar = tqdm(list(pages.iterrows()), file=sys.stdout, leave=False)
 
-        for indx, row in pbar:
+        if debugging:
+            pbar = tqdm(list(pages.iterrows())[0:1], file=sys.stdout, leave=False)
+
+        # --- iterating ---
+        # TODO: create a multiprocessing pool and get one statusbar pr cell
+        for index, row in pbar:
             counter += 1
-            h_txt = f"{indx}"
+            h_txt = f"{index}"
             n_txt = f"loading {counter}"
-            l_txt = f"starting to process file # {counter} ({indx})"
-
-            # TO BE IMPLEMENTED (parameters already in the journal pages):
-            cell_spec_page = self._get_cell_spec_from_page(indx, row)
+            l_txt = f"starting to process file # {counter} ({index})"
+            pbar.set_description(n_txt, refresh=True)
+            cell_spec_page = self._get_cell_spec_from_page(index, row)
 
             if cell_specs is not None:
-                cell_spec = cell_specs.get(indx, dict())
+                cell_spec = cell_specs.get(index, dict())
             else:
                 cell_spec = dict()
 
             cell_spec = {**cell_spec_page, **kwargs, **cell_spec}
-
             l_txt += f" cell_spec: {cell_spec}"
             logging.debug(l_txt)
-            pbar.set_description(n_txt)
-            pbar.set_postfix_str(s=h_txt, refresh=True)
 
-            if not row[hdr_journal.raw_file_names] and not force_cellpy:
+            # --- UPDATING ARGUMENTS ---
+            filename = None
+            instrument = None
+            cellpy_file = OtherPath(row[hdr_journal.cellpy_file_name])
+            _cellpy_file = None
+            if not force_raw and cellpy_file.is_file():
+                _cellpy_file = cellpy_file
+                logging.debug(f"Got cellpy file: {_cellpy_file}")
+            if not force_cellpy:
+                filename = row[hdr_journal.raw_file_names]
+                instrument = row[hdr_journal.instrument]
+
+            cycle_mode = row[hdr_journal.cell_type]
+            mass = row[hdr_journal.mass]
+            nom_cap = row[hdr_journal.nom_cap]
+
+            loading = None
+            area = None
+            if hdr_journal.loading in row:
+                loading = row[hdr_journal.loading]
+            if hdr_journal.area in row:
+                area = row[hdr_journal.area]
+
+            summary_kwargs = {
+                "use_cellpy_stat_file": prms.Reader.use_cellpy_stat_file,
+            }
+
+            step_kwargs = {}
+            if not filename and not force_cellpy:
                 logging.info(
-                    f"Raw file(s) not given in the journal.pages for index={indx}"
+                    f"Raw file(s) not given in the journal.pages for index={index}"
                 )
-                errors.append(indx)
-                h_txt += " [-]"
-                pbar.set_postfix_str(s=h_txt, refresh=True)
+                errors.append(index)
                 continue
 
-            else:
-                logging.info(f"Processing {indx}")
+            elif not cellpy_file and force_cellpy:
+                logging.info(
+                    f"Cellpy file not given in the journal.pages for index={index}"
+                )
+                errors.append(index)
+                continue
 
-            cell_data = cellreader.CellpyData()
+            else:
+                logging.info(f"Processing {index}")
 
             logging.info("loading cell")
-            if not force_cellpy:
-                if self.force_raw:
-                    h_txt += " (r)"
-                    pbar.set_postfix_str(s=h_txt, refresh=True)
-                logging.debug("not forcing to load cellpy-file instead of raw file.")
-
-                try:
-                    # TODO: replace 'loadcell' with its individual parts instead - this
-                    #   will make refactoring much much easier
-                    cell_data.loadcell(
-                        raw_files=row[hdr_journal.raw_file_names],
-                        cellpy_file=row[hdr_journal.cellpy_file_name],
-                        mass=row[hdr_journal.mass],
-                        summary_on_raw=True,
-                        force_raw=self.force_raw,
-                        use_cellpy_stat_file=prms.Reader.use_cellpy_stat_file,
-                        nom_cap=row[hdr_journal.nom_cap],
-                        cell_type=row[hdr_journal.cell_type],
-                        instrument=row[hdr_journal.instrument],
-                        selector=selector,
-                        **cell_spec,
-                    )
-
-                except Exception as e:
-                    logging.info("Failed to load: " + str(e))
-                    errors.append("loadcell:" + str(indx))
-                    h_txt += " [-]"
-                    pbar.set_postfix_str(s=h_txt, refresh=True)
-                    if not self.accept_errors:
-                        raise e
-                    continue
+            try:
+                logging.debug("inside try: running cellpy.get")
+                cell_data = cellpy.get(
+                    filename=filename,
+                    instrument=instrument,
+                    cellpy_file=_cellpy_file,
+                    cycle_mode=cycle_mode,
+                    mass=mass,
+                    nom_cap=nom_cap,
+                    loading=loading,
+                    area=area,
+                    step_kwargs=step_kwargs,
+                    summary_kwargs=summary_kwargs,
+                    selector=selector,
+                    logging_mode=logging_mode,
+                    testing=testing,
+                    **cell_spec,
+                )
+                logging.info("loaded cell")
 
-            else:
-                logging.info("forcing")
-                h_txt += " (f)"
+            except Exception as e:
+                logging.info("Failed to load: " + str(e))
+                errors.append("update:" + str(index))
+                h_txt += " [-]"
                 pbar.set_postfix_str(s=h_txt, refresh=True)
-                try:
-                    cell_data.load(
-                        row[hdr_journal.cellpy_file_name],
-                        parent_level=self.parent_level,
-                        selector=selector,
-                    )
-                except Exception as e:
-                    logging.info(
-                        f"Critical exception encountered {type(e)} "
-                        "- skipping this file"
-                    )
-                    logging.debug("Failed to load. Error-message: " + str(e))
-                    errors.append("load:" + str(indx))
-                    h_txt += " [-]"
-                    pbar.set_postfix_str(s=h_txt, refresh=True)
-                    if not self.accept_errors:
-                        raise e
-                    continue
+                if not self.accept_errors:
+                    raise e
+                continue
 
-            if not cell_data.check():
+            if cell_data.empty:
                 logging.info("...not loaded...")
-                logging.debug("Did not pass check(). Could not load cell!")
-                errors.append("check:" + str(indx))
+                logging.debug("Data is empty. Could not load cell!")
+                errors.append("check:" + str(index))
                 h_txt += " [-]"
                 pbar.set_postfix_str(s=h_txt, refresh=True)
                 continue
 
             logging.info("...loaded successfully...")
             h_txt += " [OK]"
             pbar.set_postfix_str(s=h_txt, refresh=True)
-            summary_tmp = cell_data.cell.summary
+            summary_tmp = cell_data.data.summary
             logging.info("Trying to get summary_data")
 
-            if cell_data.cell.steps is None or self.force_recalc:
+            if cell_data.data.steps is None or self.force_recalc:
                 logging.info("Running make_step_table")
                 n_txt = f"steps {counter}"
                 pbar.set_description(n_txt, refresh=True)
                 cell_data.make_step_table()
 
             if summary_tmp is None or self.force_recalc:
                 logging.info("Running make_summary")
                 n_txt = f"summary {counter}"
                 pbar.set_description(n_txt, refresh=True)
                 cell_data.make_summary(find_end_voltage=True, find_ir=True)
 
-            # if summary_tmp.index.name == b"Cycle_Index":
-            #     logging.debug("Strange: 'Cycle_Index' is a byte-string")
-            #     summary_tmp.index.name = "Cycle_Index"
-
+            # some clean-ups (might not be needed anymore):
             if not summary_tmp.index.name == hdr_summary.cycle_index:
-                # TODO: Why did I do this? Does not make any sense. It seems like
-                #    batch forces Summary to have "Cycle_Index" as index, but
-                #    files not processed by batch will not have.
-                #    I think I should choose what should and what should not have
-                #    a measurement col as index. Current:
-                #    steps - not sure
-                #    raw - data_point (already implemented I think)
-                #    summary - not sure
                 logging.debug("Setting index to Cycle_Index")
                 # check if it is a byte-string
                 if b"Cycle_Index" in summary_tmp.columns:
                     logging.debug("Seems to be a byte-string in the column-headers")
                     summary_tmp.rename(
                         columns={b"Cycle_Index": "Cycle_Index"}, inplace=True
                     )
-                # TODO: check if drop=False works [#index]
                 try:
                     summary_tmp.set_index("cycle_index", inplace=True)
                 except KeyError:
                     logging.debug("cycle_index already an index")
 
-            summary_frames[indx] = summary_tmp
+            summary_frames[index] = summary_tmp
 
             if self.all_in_memory:
-                cell_data_frames[indx] = cell_data
+                cell_data_frames[index] = cell_data
             else:
-                cell_data_frames[indx] = cellreader.CellpyData(initialize=True)
-                cell_data_frames[indx].cell.steps = cell_data.cell.steps
-                # cell_data_frames[indx].dataset.steps_made = True
+                cell_data_frames[index] = cellreader.CellpyCell(initialize=True)
+                cell_data_frames[index].data.steps = cell_data.data.steps
 
             if self.save_cellpy:
                 logging.info("saving to cellpy-format")
                 n_txt = f"saving {counter}"
                 pbar.set_description(n_txt, refresh=True)
                 if self.custom_data_folder is not None:
                     print("Save to custom data-folder not implemented yet")
@@ -412,23 +454,336 @@
                         savedir=self.journal.raw_dir,
                         sep=prms.Reader.sep,
                         last_cycle=self.last_cycle,
                     )
                 except Exception as e:
                     logging.error("Could not make/export dq/dv data")
                     logging.debug(
-                        "Failed to make/export " "dq/dv data (%s): %s" % (indx, str(e))
+                        "Failed to make/export " "dq/dv data (%s): %s" % (index, str(e))
+                    )
+                    errors.append("ica:" + str(index))
+
+        self.errors["update"] = errors
+        self.summary_frames = summary_frames
+        self.cell_data_frames = cell_data_frames
+
+    def parallel_update(
+        self, all_in_memory=None, cell_specs=None, logging_mode=None, **kwargs
+    ):
+        """Updates the selected datasets in parallel.
+
+        Args:
+            all_in_memory (bool): store the `cellpydata` in memory (default
+                False)
+            cell_specs (dict of dicts): individual arguments pr. cell. The `cellspecs` key-word argument
+                dictionary will override the **kwargs and the parameters from the journal pages
+                for the indicated cell.
+            logging_mode (str): sets the logging mode for the loader(s).
+
+            kwargs:
+                transferred all the way to the instrument loader, if not
+                picked up earlier. Remark that you can obtain the same pr. cell by
+                providing a `cellspecs` dictionary. The kwargs have precedence over the
+                parameters given in the journal pages, but will be overridden by parameters
+                given by `cellspecs`.
+
+                Merging:
+                    recalc (Bool): set to False if you don't want automatic "recalc" of
+                        cycle numbers etc. when merging several data-sets.
+                Loading:
+                    selector (dict): selector-based parameters sent to the cellpy-file loader (hdf5) if
+                    loading from raw is not necessary (or turned off).
+
+                Debugging:
+                    debug (Bool): set to True if you want to run in debug mode (should never be used by non-developers).
+
+        Debug-mode:
+                 - runs only for the first item in your journal
+
+        Examples:
+            >>> # Don't perform recalculation of cycle numbers etc. when merging
+            >>> # All cells:
+            >>> b.update(recalc=False)
+            >>> # For specific cell(s):
+            >>> cell_specs_cell_01 = {"name_of_cell_01": {"recalc": False}}
+            >>> b.update(cell_specs=cell_specs_cell_01)
+
+        """
+        status = "PROD"  # set this to DEV when developing this
+        async_mode = "threading"
+        logging.debug("PARALLEL UPDATE")
+        # TODO: implement experiment.last_cycle
+        if status != "DEV":
+            print("SORRY - MULTIPROCESSING IS NOT IMPLEMENTED PROPERLY YET")
+            return self.update(
+                all_in_memory=all_in_memory,
+                cell_specs=cell_specs,
+                logging_mode=logging_mode,
+                **kwargs,
+            )
+
+        import concurrent.futures
+        import multiprocessing
+
+        max_number_processes = multiprocessing.cpu_count()
+
+        if async_mode == "threading":
+            pool_executor = concurrent.futures.ThreadPoolExecutor
+        else:
+            pool_executor = concurrent.futures.ProcessPoolExecutor
+
+        debugging = kwargs.pop("debug", False)
+
+        # --- cleaning up attributes / arguments etc ---
+        force_cellpy = kwargs.pop("force_cellpy", self.force_cellpy)
+        force_raw = kwargs.pop("force_raw", self.force_raw)
+
+        logging.info("[update experiment]")
+        if all_in_memory is not None:
+            self.all_in_memory = all_in_memory
+
+        logging.info(f"Additional keyword arguments: {kwargs}")
+        selector = kwargs.get("selector", None)
+
+        pages = self.journal.pages
+        if self.nom_cap:
+            warnings.warn(
+                "Setting nominal capacity through attributes will be deprecated soon since it modifies "
+                "the journal pages."
+            )
+            pages[hdr_journal.nom_cap] = self.nom_cap
+
+        if self.instrument:
+            warnings.warn(
+                "Setting instrument through attributes will be deprecated soon since it modifies the journal pages."
+            )
+            pages[hdr_journal.instrument] = self.instrument
+
+        if x := kwargs.pop("instrument", None):
+            warnings.warn(
+                "Setting instrument through params will be deprecated soon since it modifies the journal pages."
+                "Future version will require instrument in the journal pages."
+            )
+            pages[hdr_journal.instrument] = x
+
+        if pages.empty:
+            raise Exception("your journal is empty")
+
+        # --- init ---
+        summary_frames = dict()
+        cell_data_frames = dict()
+        number_of_runs = len(pages)
+        logging.debug(f"You have {number_of_runs} cells in your journal")
+        counter = 0
+        errors = []
+
+        pbar = tqdm(list(pages.iterrows()), file=sys.stdout, leave=False)
+
+        if debugging:
+            pbar = tqdm(list(pages.iterrows())[0:1], file=sys.stdout, leave=False)
+
+        # --- iterating ---
+        # TODO: create a multiprocessing pool and get one statusbar pr cell
+        params = []
+        with pool_executor(max_number_processes) as executor:
+            for index, row in pages.iterrows():
+                counter += 1
+                cell_spec_page = self._get_cell_spec_from_page(index, row)
+
+                if cell_specs is not None:
+                    cell_spec = cell_specs.get(index, dict())
+                else:
+                    cell_spec = dict()
+
+                cell_spec = {**cell_spec_page, **kwargs, **cell_spec}
+
+                # --- UPDATING ARGUMENTS ---
+                filename = None
+                instrument = None
+                cellpy_file = OtherPath(row[hdr_journal.cellpy_file_name])
+                _cellpy_file = None
+                if not force_raw and cellpy_file.is_file():
+                    _cellpy_file = cellpy_file
+                    logging.debug(f"Got cellpy file: {_cellpy_file}")
+                if not force_cellpy:
+                    filename = row[hdr_journal.raw_file_names]
+                    instrument = row[hdr_journal.instrument]
+
+                cycle_mode = row[hdr_journal.cell_type]
+                mass = row[hdr_journal.mass]
+                nom_cap = row[hdr_journal.nom_cap]
+
+                loading = None
+                area = None
+                if hdr_journal.loading in row:
+                    loading = row[hdr_journal.loading]
+                if hdr_journal.area in row:
+                    area = row[hdr_journal.area]
+
+                summary_kwargs = {
+                    "use_cellpy_stat_file": prms.Reader.use_cellpy_stat_file,
+                }
+
+                step_kwargs = {}
+                if not filename and not force_cellpy:
+                    logging.info(
+                        f"Raw file(s) not given in the journal.pages for index={index}"
+                    )
+                    errors.append(index)
+                    continue
+
+                elif not cellpy_file and force_cellpy:
+                    logging.info(
+                        f"Cellpy file not given in the journal.pages for index={index}"
+                    )
+                    errors.append(index)
+                    continue
+
+                else:
+                    logging.info(f"Processing {index}")
+
+                logging.info("loading cell")
+                params.append(
+                    dict(
+                        filename=filename,
+                        instrument=instrument,
+                        cellpy_file=_cellpy_file,
+                        cycle_mode=cycle_mode,
+                        mass=mass,
+                        nom_cap=nom_cap,
+                        loading=loading,
+                        area=area,
+                        step_kwargs=step_kwargs,
+                        summary_kwargs=summary_kwargs,
+                        selector=selector,
+                        logging_mode=logging_mode,
+                        testing=False,
+                        **cell_spec,
+                    )
+                )
+
+            pool = [executor.submit(cellpy.get, **param) for param in params]
+            for i in concurrent.futures.as_completed(pool):
+                cell_data = i.result()
+                if cell_data.empty:
+                    logging.info("...not loaded...")
+                    logging.debug("Data is empty. Could not load cell!")
+                    errors.append("check:" + str(index))
+
+                logging.info("...loaded successfully...")
+                summary_tmp = cell_data.data.summary
+                logging.info("Trying to get summary_data")
+
+                if cell_data.data.steps is None or self.force_recalc:
+                    logging.info("Running make_step_table")
+                    cell_data.make_step_table()
+
+                if summary_tmp is None or self.force_recalc:
+                    logging.info("Running make_summary")
+                    cell_data.make_summary(find_end_voltage=True, find_ir=True)
+
+                # some clean-ups (might not be needed anymore):
+                if not summary_tmp.index.name == hdr_summary.cycle_index:
+                    logging.debug("Setting index to Cycle_Index")
+                    # check if it is a byte-string
+                    if b"Cycle_Index" in summary_tmp.columns:
+                        logging.debug("Seems to be a byte-string in the column-headers")
+                        summary_tmp.rename(
+                            columns={b"Cycle_Index": "Cycle_Index"}, inplace=True
+                        )
+                    try:
+                        summary_tmp.set_index("cycle_index", inplace=True)
+                    except KeyError:
+                        logging.debug("cycle_index already an index")
+
+                summary_frames[index] = summary_tmp
+
+                if self.all_in_memory:
+                    cell_data_frames[index] = cell_data
+                else:
+                    cell_data_frames[index] = cellreader.CellpyCell(initialize=True)
+                    cell_data_frames[index].data.steps = cell_data.data.steps
+
+                if self.save_cellpy:
+                    logging.info("saving to cellpy-format")
+                    n_txt = f"saving {counter}"
+                    pbar.set_description(n_txt, refresh=True)
+                    if self.custom_data_folder is not None:
+                        print("Save to custom data-folder not implemented yet")
+                        print(f"Saving to {row.cellpy_file_name} instead")
+                    if not row.fixed:
+                        logging.info("saving cell to %s" % row.cellpy_file_name)
+                        cell_data.ensure_step_table = True
+                        try:
+                            cell_data.save(row.cellpy_file_name)
+                        except Exception as e:
+                            logging.error("saving file failed")
+                            logging.error(e)
+
+                    else:
+                        logging.debug("saving cell skipped (set to 'fixed' in info_df)")
+                else:
+                    logging.info("You opted to not save to cellpy-format")
+                    logging.info("It is usually recommended to save to cellpy-format:")
+                    logging.info(" >>> b.experiment.save_cellpy = True")
+                    logging.info(
+                        "Without the cellpy-files, you cannot select specific cells"
+                    )
+                    logging.info("if you did not opt to store all in memory")
+
+                if self.export_raw or self.export_cycles:
+                    export_text = "exporting"
+                    if self.export_raw:
+                        export_text += " [raw]"
+                    if self.export_cycles:
+                        export_text += " [cycles]"
+                    logging.info(export_text)
+                    n_txt = f"{export_text} {counter}"
+                    pbar.set_description(n_txt, refresh=True)
+                    cell_data.to_csv(
+                        self.journal.raw_dir,
+                        sep=prms.Reader.sep,
+                        cycles=self.export_cycles,
+                        shifted=self.shifted_cycles,
+                        raw=self.export_raw,
+                        last_cycle=self.last_cycle,
                     )
-                    errors.append("ica:" + str(indx))
+
+                if self.export_ica:
+                    logging.info("exporting [ica]")
+                    try:
+                        helper.export_dqdv(
+                            cell_data,
+                            savedir=self.journal.raw_dir,
+                            sep=prms.Reader.sep,
+                            last_cycle=self.last_cycle,
+                        )
+                    except Exception as e:
+                        logging.error("Could not make/export dq/dv data")
+                        logging.debug(
+                            "Failed to make/export "
+                            "dq/dv data (%s): %s" % (index, str(e))
+                        )
+                        errors.append("ica:" + str(index))
 
         self.errors["update"] = errors
         self.summary_frames = summary_frames
         self.cell_data_frames = cell_data_frames
 
     def export_cellpy_files(self, path=None, **kwargs):
+        """Export all cellpy-files to a given path.
+
+        Remarks:
+            This method can only export to local folders
+            (OtherPath objects are not formally supported, but
+            might still work if the path is local).
+
+        Args:
+            path (str, pathlib.Path): path to export to (default: current working directory)
+        """
         if path is None:
             path = "."
         errors = []
         path = pathlib.Path(path)
         cell_names = self.cell_names
         for cell_name in cell_names:
             cellpy_file_name = self.journal.pages.loc[
@@ -443,14 +798,15 @@
                 self._link_cellpy_file(cell_name)
 
             c.save(cellpy_file_name, **kwargs)
         self.errors["export_cellpy_files"] = errors
 
     @property
     def cell_names(self):
+        """Returns a list of cell-names (strings)"""
         try:
             return [key for key in self.cell_data_frames]
         except TypeError:
             return None
 
     def status(self):
         print("\n")
@@ -466,15 +822,15 @@
                 print(f"{key}: {type(self.memory_dumped[key])}")
         print(80 * "=")
 
     def link(self, **kwargs):
         """Ensure that an appropriate link to the cellpy-files exists for
         each cell.
 
-        The experiment will then contain a CellpyData object for each cell
+        The experiment will then contain a CellpyCell object for each cell
         (in the cell_data_frames attribute) with only the step-table stored.
 
         Remark that running update persists the summary frames instead (or
         everything in case you specify all_in_memory=True).
         This might be considered "a strange and unexpected behaviour". Sorry
         for that (but the authors of this package is also a bit strange...).
 
@@ -573,15 +929,15 @@
                         else:
                             c.make_step_table()
 
                     pbar.set_postfix_str(s="summary", refresh=True)
                     if summary_opts is not None:
                         c.make_summary(**summary_opts)
                     else:
-                        c.make_summary(find_end_voltage=False, find_ir=True)
+                        c.make_summary(find_end_voltage=True, find_ir=True)
 
                 except Exception as e:
                     e_txt = f"recalculating for {indx} failed!"
                     errors.append(e_txt)
                     warnings.warn(e_txt)
                 else:
                     if save:
@@ -602,7 +958,36 @@
     def __init__(self):
         super().__init__()
 
 
 class LifeTimeExperiment(BaseExperiment):
     def __init__(self):
         super().__init__()
+
+
+if __name__ == "__main__":
+    from pathlib import Path
+    import os
+    import pandas as pd
+    import numpy as np
+    import seaborn as sns
+    import plotly.express as px
+
+    import cellpy
+    from cellpy.utils import batch, helpers, plotutils
+
+    project_dir = Path("../../../testdata/batch_project")
+    print(f"{project_dir.resolve()=}")
+    journal = project_dir / "test_project.json"
+    journal = journal.resolve()
+    print(f"{journal=}")
+    assert project_dir.is_dir()
+    assert journal.is_file()
+    os.chdir(project_dir)
+
+    print(f"cellpy version: {cellpy.__version__}")
+    cellpy.log.setup_logging("INFO")
+
+    b = batch.from_journal(journal)
+    b.update()
+
+    print("Ended OK")
```

### Comparing `cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_exporters.py` & `cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_exporters.py`

 * *Files 5% similar despite different names*

```diff
@@ -32,30 +32,30 @@
         self._assign_engine(summary_engine)
         self._assign_engine(cycles_engine)
         self._assign_dumper(csv_dumper)
         if use_screen_dumper:
             self._assign_dumper(screen_dumper)
         self.current_engine = None
 
-    def run_engine(self, engine):
+    def run_engine(self, engine, **kwargs):
         """run engine (once pr. experiment).
 
         Args:
             engine: engine to run (function or method).
 
         The method issues the engine command (with experiments and farms
         as input) that returns an updated farms as well as the barn and
         assigns them both to self.
 
         The farms attribute is a list of farms, i.e. [farm1, farm2, ...], where
         each farm contains pandas DataFrames.
 
         The barns attribute is a pre-defined string used for picking what
         folder(s) the file(s) should be exported to.
-        For example, if barn equals "batch_dir", the the file(s) will be saved
+        For example, if barn equals "batch_dir", the file(s) will be saved
         to the experiments batch directory.
         """
 
         logging.debug("running engine")
         self.current_engine = engine
 
         self.farms, self.barn = engine(experiments=self.experiments, farms=self.farms)
```

### Comparing `cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_helpers.py` & `cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_helpers.py`

 * *Files 2% similar despite different names*

```diff
@@ -215,21 +215,21 @@
     return _remove_date_and_celltype(label)
 
 
 def create_selected_summaries_dict(summaries_list):
     """Creates a dictionary with summary column headers.
 
     Examples:
-        >>> summaries_to_output = ["discharge_capacity", "charge_capacity"]
+        >>> summaries_to_output = ["discharge_capacity_gravimetric", "charge_capacity_gravimetric"]
         >>> summaries_to_output_dict = create_selected_summaries_dict(
         >>>    summaries_to_output
         >>> )
         >>> print(summaries_to_output_dict)
-        {'discharge_capacity': "Discharge_Capacity(mAh/g)",
-               'charge_capacity': "Charge_Capacity(mAh/g)}
+        {'discharge_capacity_gravimetric': "discharge_capacity_gravimetric",
+               'charge_capacity_gravimetric': "discharge_capacity_gravimetric"}
 
     Args:
         summaries_list: list containing cellpy summary column id names
 
     Returns: dictionary of the form {cellpy id name: cellpy summary
         header name,}
 
@@ -325,24 +325,24 @@
 
         out_data.append(v)
         out_data.append(dq)
     return out_data
 
 
 def export_dqdv(cell_data, savedir, sep, last_cycle=None):
-    """Exports dQ/dV data from a CellpyData instance.
+    """Exports dQ/dV data from a CellpyCell instance.
 
     Args:
-        cell_data: CellpyData instance
+        cell_data: CellpyCell instance
         savedir: path to the folder where the files should be saved
         sep: separator for the .csv-files.
         last_cycle: only export up to this cycle (if not None)
     """
     logging.debug("exporting dqdv")
-    filename = cell_data.cell.loaded_from
+    filename = cell_data.data.loaded_from
     no_merged_sets = ""
     firstname, extension = os.path.splitext(filename)
     firstname += no_merged_sets
     if savedir:
         firstname = os.path.join(savedir, os.path.basename(firstname))
         logging.debug(f"savedir is true: {firstname}")
```

### Comparing `cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_journals.py` & `cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_journals.py`

 * *Files 15% similar despite different names*

```diff
@@ -2,81 +2,143 @@
 import logging
 import os
 import pathlib
 import platform
 import shutil
 import tempfile
 import warnings
+from abc import ABC
 
 import pandas as pd
 
 from cellpy.exceptions import UnderDefined
 from cellpy.parameters import prms
 from cellpy.parameters.internal_settings import (
     get_headers_journal,
     keys_journal_session,
 )
-from cellpy.parameters.legacy.internal_settings import (
+from cellpy.parameters.legacy.update_headers import (
     headers_journal_v0 as hdr_journal_old,
 )
 from cellpy.readers import dbreader
 from cellpy.utils.batch_tools.batch_core import BaseJournal
-from cellpy.utils.batch_tools.engines import simple_db_engine
+from cellpy.utils.batch_tools.engines import simple_db_engine, sql_db_engine
 
 hdr_journal = get_headers_journal()
 
 trans_dict = {}
 missing_keys = []
 for key in hdr_journal:
     if key in hdr_journal_old:
         trans_dict[hdr_journal_old[key]] = hdr_journal[key]
     else:
         missing_keys.append(key)
 
 
-class LabJournal(BaseJournal):
-    def __init__(self, db_reader="default", engine=None):
+class LabJournal(BaseJournal, ABC):
+    def __init__(self, db_reader="default", engine=None, batch_col=None, **kwargs):
         """Journal for selected batch.
 
         The journal contains pages (pandas.DataFrame) with prms for
-        each cells (one cell pr row).
+        each cell (one cell pr row).
 
         Args:
             db_reader: either default (a simple excel reader already
                 implemented in cellpy) or other db readers that implement
                 the needed API.
             engine: defaults to simple_db_engine for parsing db using the
                 db_reader
                     self.pages = simple_db_engine(
                         self.db_reader, id_keys, **kwargs
                     )
+            batch_col: the column name for the batch column in the db (used by simple_db_engine).
+            **kwargs: passed to the db_reader
         """
 
         super().__init__()
-        if db_reader == "default":
-            self.db_reader = dbreader.Reader()
-            self.engine = simple_db_engine
+        if db_reader is None:
+            return
+
+        if isinstance(db_reader, str):
+            if db_reader == "off":
+                self.db_reader = None
+                return
+            if db_reader == "default":
+                db_reader = prms.Db.db_type
+            if db_reader == "simple_excel_reader":
+                self.db_reader = dbreader.Reader()
+                self.engine = simple_db_engine
+            elif db_reader == "sql_db_reader":
+                raise NotImplementedError("sql_db_reader is not implemented yet")
+                # self.db_reader = sql_dbreader.SqlReader()
+                # self.engine = sql_db_engine
+            else:
+                raise UnderDefined(f"The db-reader '{db_reader}' is not supported")
         else:
             logging.debug(f"Remark! db_reader: {db_reader}")
             self.db_reader = db_reader
-            if engine is None:
-                self.engine = engine
-        self.batch_col = "b01"
+
+        if engine is None:
+            self.engine = simple_db_engine
+
+        self.batch_col = batch_col or "b01"
+
+    def _repr_html_(self):
+        txt = f"<h2>LabJournal-object</h2> id={hex(id(self))}"
+        txt += "<h3>Main attributes</h3>"
+        txt += f"""
+        <table>
+            <thead>
+                <tr>
+                    <th>Attribute</th>
+                    <th>Value</th>
+                </tr>
+            </thead>
+            <tbody>
+                <tr><td><b>name</b></td><td>{self.name}</td></tr>
+                <tr><td><b>project</b></td><td>{self.project}</td></tr>
+                <tr><td><b>file_name</b></td><td>{self.file_name}</td></tr>
+                <tr><td><b>db_reader</b></td><td>{self.db_reader}</td></tr>
+        """
+        if self.db_reader == "default":
+            txt += f"<tr><td><b>batch_col</b></td><td>{self.batch_col}</td></tr>"
+        txt += f"""
+                <tr><td><b>time_stamp</b></td><td>{self.time_stamp}</td></tr>
+                <tr><td><b>project_dir</b></td><td>{self.project_dir}</td></tr>
+                <tr><td><b>raw_dir</b></td><td>{self.raw_dir}</td></tr>
+                <tr><td><b>batch_dir</b></td><td>{self.batch_dir}</td></tr>
+            </tbody>
+        </table>
+        """
+        txt += "<h3>Session info</h3>"
+        for key in self.session:
+            txt += f"<p><b>{key}</b>: {self.session[key]}</p>"
+
+        txt += "<h3>Pages</h3>"
+        try:
+            txt += self.pages._repr_html_()  # pylint: disable=protected-access
+        except AttributeError:
+            txt += "<p><b>pages</b><br> not found!</p>"
+        except ValueError:
+            txt += "<p><b>pages</b><br> not readable!</p>"
+        return txt
 
     def _check_file_name(self, file_name, to_project_folder=False):
         if file_name is None:
             if not self.file_name:
                 self.generate_file_name()
             file_name = pathlib.Path(self.file_name)
+
         else:
             file_name = pathlib.Path(file_name)
-
         if to_project_folder:
-            file_name = file_name.with_suffix(".json")
-            file_name = pathlib.Path(self.project_dir) / file_name
+            file_name = file_name.with_suffix(".json").name
+            project_dir = pathlib.Path(self.project_dir)
+
+            file_name = project_dir / file_name
         self.file_name = file_name  # updates object (maybe not smart)
         return file_name
 
     def from_db(self, project=None, name=None, batch_col=None, **kwargs):
         """populate journal from db.
 
         Args:
@@ -117,17 +179,25 @@
         if project is not None:
             self.project = project
         if name is None:
             name = self.name
         else:
             self.name = name
         logging.debug(f"batch_name, batch_col: {name}, {batch_col}")
+
         if self.db_reader is not None:
-            id_keys = self.db_reader.select_batch(name, batch_col)
-            self.pages = self.engine(self.db_reader, id_keys, **kwargs)
+            if isinstance(self.db_reader, dbreader.Reader):  # Simple excel-db
+                id_keys = self.db_reader.select_batch(name, batch_col)
+                self.pages = self.engine(self.db_reader, id_keys, **kwargs)
+            else:
+                logging.debug(
+                    "creating journal pages using advanced reader methods (not simple excel-db)"
+                )
+                self.pages = self.engine(self.db_reader, batch_name=name, **kwargs)
+
             if self.pages.empty:
                 logging.critical(
                     f"EMPTY JOURNAL: are you sure you have provided correct input to batch?"
                 )
                 logging.critical(f"name: {name}")
                 logging.critical(f"project: {self.project}")
                 logging.critical(f"batch_col: {batch_col}")
@@ -293,27 +363,27 @@
             logging.critical("no session found in your journal file")
         for item in keys_journal_session:
             session[item] = session.get(item, None)
 
         return session, pages
 
     @classmethod
-    def _clean_pages(cls, pages):
+    def _clean_pages(cls, pages: pd.DataFrame) -> pd.DataFrame:
         import ast
 
         logging.debug("removing empty rows")
         pages = pages.dropna(how="all")
         logging.debug("checking path-names")
         try:
             p = pages[hdr_journal.raw_file_names]
             new_p = []
             for f in p:
                 if isinstance(f, str):
                     try:
-                        new_f = ast.literal_eval(f)
+                        new_f = ast.literal_eval(f"'{f}'")
                         if isinstance(new_f, list):
                             f = new_f
                     except Exception as e:
                         warnings.warn(e)
                         warnings.warn(f"Could not evaluate {f}")
 
                 new_p.append(f)
@@ -357,17 +427,16 @@
                 if column_name != hdr_journal.filename:
                     pages[column_name] = None
 
         return pages
 
     def from_file(self, file_name=None, paginate=True, **kwargs):
         """Loads a DataFrame with all the needed info about the experiment"""
-
         file_name = self._check_file_name(file_name)
-        logging.debug(f"reading {file_name}")
+        logging.info(f"reading {file_name}")
         if pathlib.Path(file_name).suffix.lower() == ".xlsx":
             file_loader = self.read_journal_excel_file
         else:
             file_loader = self.read_journal_jason_file
         try:
             out = file_loader(file_name, **kwargs)
             if out is None:
@@ -393,14 +462,20 @@
             self.name = name
         if project is not None:
             self.project = project
 
         self.pages = (
             frame  # TODO: include a check here to see if the pages are appropriate
         )
+        for hdr in hdr_journal.values():
+            if hdr not in self.pages.columns:
+                self.pages[hdr] = None
+
+        if hdr_journal.filename in self.pages.columns:
+            self.pages = self.pages.set_index(hdr_journal.filename)
 
         if paginate is None:
             if self.name and self.project:
                 paginate = True
 
         if paginate:
             logging.critical(f"paginating {project}/{name} ")
@@ -435,28 +510,57 @@
         logging.debug(f"project: {self.project}")
 
         col_names = list(hdr_journal.values())
         pages = pd.DataFrame(columns=col_names)
         pages.set_index(hdr_journal.filename, inplace=True)
         return pages
 
-    def to_file(self, file_name=None, paginate=True, to_project_folder=True):
+    def duplicate_journal(self, folder=None) -> None:
+        """Copy the journal to folder.
+
+        Args:
+            folder (str or pathlib.Path): folder to copy to (defaults to the
+            current folder).
+        """
+
+        logging.debug(f"duplicating journal to folder {folder}")
+        journal_name = pathlib.Path(self.file_name)
+        if not journal_name.is_file():
+            logging.info("No journal saved")
+            return
+        new_journal_name = journal_name.name
+        if folder is not None:
+            new_journal_name = pathlib.Path(folder) / new_journal_name
+        try:
+            shutil.copy(journal_name, new_journal_name)
+        except shutil.SameFileError:
+            logging.debug("same file exception encountered")
+
+    def to_file(
+        self,
+        file_name=None,
+        paginate=True,
+        to_project_folder=True,
+        duplicate_to_local_folder=True,
+    ):
         """Saves a DataFrame with all the needed info about the experiment.
 
         Args:
-            file_name (str or pathlib.Path): journal file name
+            file_name (str or pathlib.Path): journal file name (.json or .xlsx)
             paginate (bool): make project folders
             to_project_folder (bool): save journal file to the folder containing your cellpy projects
+            duplicate_to_local_folder (bool): save journal file to the folder you are in now also
 
         Returns:
-
+            None
         """
         file_name = self._check_file_name(
             file_name, to_project_folder=to_project_folder
         )
+
         pages = self.pages
         session = self.session
         meta = self._prm_packer()
         top_level_dict = {"info_df": pages, "metadata": meta, "session": session}
 
         is_json = False
         is_xlsx = False
@@ -468,14 +572,15 @@
             is_json = True
 
         if is_xlsx:
             df_session = self._pack_session(session)
             df_meta = self._pack_meta(meta)
 
             try:
+                pages.index.name = "filename"
                 with pd.ExcelWriter(file_name, mode="w", engine="openpyxl") as writer:
                     pages.to_excel(writer, sheet_name="pages", engine="openpyxl")
                     # no index is not supported for multi-index (update to index=False when pandas implement it):
                     df_session.to_excel(writer, sheet_name="session", engine="openpyxl")
                     df_meta.to_excel(
                         writer, sheet_name="meta", engine="openpyxl", index=False
                     )
@@ -495,14 +600,17 @@
 
         self.file_name = file_name
         logging.info(f"Saved file to {file_name}")
 
         if paginate:
             self.paginate()
 
+        if duplicate_to_local_folder:
+            self.duplicate_journal()
+
     @staticmethod
     def _pack_session(session):
         frames = []
         keys = []
         try:
             l_bad_cycle_numbers = []
 
@@ -594,15 +702,14 @@
 
         return project_dir, batch_dir, raw_dir
 
     def generate_file_name(self):
         """generate a suitable file name for the experiment"""
         if not self.project:
             raise UnderDefined("project name not given")
-
         out_data_dir = prms.Paths.outdatadir
         project_dir = os.path.join(out_data_dir, self.project)
         file_name = f"cellpy_batch_{self.name}.json"
         self.file_name = os.path.join(project_dir, file_name)
 
     # v.1.0.0:
     def look_for_file(self):
@@ -634,16 +741,19 @@
         pass
 
 
 def _dev_journal_loading():
     from cellpy import log
 
     log.setup_logging(default_level="DEBUG")
-    journal_file = pathlib.Path("../../../dev_data/db/test_journal.xlsx")
-    print(journal_file.is_file())
+    journal_file = pathlib.Path(
+        "../../../testdata/batch_project/test_project.json"
+    ).resolve()
+    assert journal_file.is_file()
+
     logging.debug(f"reading journal file {journal_file}")
     journal = LabJournal(db_reader=None)
     journal.from_file(journal_file, paginate=False)
     print(80 * "-")
     print(journal.pages)
     print(80 * "-")
     print(journal.session)
```

### Comparing `cellpy-0.4.3a3/cellpy/utils/batch_tools/batch_plotters.py` & `cellpy-1.0.0a0/cellpy/utils/batch_tools/batch_plotters.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import itertools
 import logging
+import sys
 import warnings
 from collections import defaultdict
 
 import numpy as np
 import pandas as pd
 
 from cellpy import prms
@@ -59,15 +60,14 @@
     g, sg = info.loc[c, [hdr_journal["group"], hdr_journal["sub_group"]]]
     return int(g), int(sg)
 
 
 def create_plot_option_dicts(
     info, marker_types=None, colors=None, line_dash=None, size=None, palette=None
 ):
-
     """Create two dictionaries with plot-options.
 
     The first iterates colors (based on group-number), the second iterates
     through marker types.
 
     Returns: group_styles (dict), sub_group_styles (dict)
     """
@@ -155,14 +155,19 @@
     x_range=None,
     y_range=None,
     tools="hover",
 ):
     # TODO: include max cycle (bokeh struggles when there is to much to plot)
     #   could also consider interpolating
     #   or defaulting to datashader for large files.
+
+    warnings.warn(
+        "This utility function will be seriously removed and replaced with a plotly version soon.",
+        category=DeprecationWarning,
+    )
     logging.debug(f"    - creating summary (bokeh) plot for {label}")
     discharge_capacity = None
     if isinstance(data, (list, tuple)):
         charge_capacity = data[0]
         if len(data) == 2:
             discharge_capacity = data[1]
     else:
@@ -181,14 +186,15 @@
 
     sub_cols_charge = None
     sub_cols_discharge = None
     legend_collection = []
     if isinstance(charge_capacity.columns, pd.MultiIndex):
         cols = charge_capacity.columns.get_level_values(1)
         sub_cols_charge = charge_capacity.columns.get_level_values(0).unique()
+
         charge_capacity.columns = [
             f"{col[0]}_{col[1]}" for col in charge_capacity.columns.values
         ]
 
         if discharge_capacity is not None:
             sub_cols_discharge = discharge_capacity.columns.get_level_values(0).unique()
             discharge_capacity.columns = [
@@ -307,62 +313,84 @@
         height_fractions = [0.3, 0.4, 0.3]
     logging.debug(f"   * stacking and plotting")
     logging.debug(f"      backend: {prms.Batch.backend}")
     logging.debug(f"      received kwargs: {kwargs}")
 
     idx = pd.IndexSlice
     all_legend_items = []
-    if add_rate:
 
+    warnings.warn(
+        "This utility function will be removed shortly", category=DeprecationWarning
+    )
+
+    if add_rate:
         try:
             discharge_capacity = summaries.loc[
-                :, idx[["discharge_capacity", "discharge_c_rate"], :]
+                :,
+                idx[
+                    [
+                        hdr_summary["discharge_capacity_gravimetric"],
+                        hdr_summary["discharge_c_rate"],
+                    ],
+                    :,
+                ],
             ]
         except AttributeError:
             warnings.warn(
                 "No discharge rate columns available - consider re-creating summary!"
             )
-            discharge_capacity = summaries.discharge_capacity
+            discharge_capacity = summaries[
+                hdr_summary["discharge_capacity_gravimetric"]
+            ]
 
         try:
             charge_capacity = summaries.loc[
-                :, idx[["charge_capacity", "charge_c_rate"], :]
+                :,
+                idx[
+                    [
+                        hdr_summary["charge_capacity_gravimetric"],
+                        hdr_summary["charge_c_rate"],
+                    ],
+                    :,
+                ],
             ]
         except AttributeError:
             warnings.warn(
                 "No charge rate columns available - consider re-creating summary!"
             )
-            charge_capacity = summaries.charge_capacity
+            charge_capacity = summaries[hdr_summary["charge_capacity_gravimetric"]]
 
         try:
             coulombic_efficiency = summaries.loc[
-                :, idx[["coulombic_efficiency", "charge_c_rate"], :]
+                :, idx[[hdr_summary.coulombic_efficiency, hdr_summary.charge_c_rate], :]
             ]
         except AttributeError:
             warnings.warn(
                 "No charge rate columns available - consider re-creating summary!"
             )
             coulombic_efficiency = summaries.coulombic_efficiency
 
-        if "ir_charge" in summaries.columns:
+        if hdr_summary.ir_charge in summaries.columns:
             try:
-                ir_charge = summaries.loc[:, idx[["ir_charge", "charge_c_rate"], :]]
+                ir_charge = summaries.loc[
+                    :, idx[[hdr_summary.ir_charge, hdr_summary.charge_c_rate], :]
+                ]
 
             except AttributeError:
                 warnings.warn(
                     "No charge rate columns available - consider re-creating summary!"
                 )
                 ir_charge = summaries.ir_charge
         else:
             ir_charge = pd.DataFrame()
     else:
-        discharge_capacity = summaries.discharge_capacity
-        charge_capacity = summaries.charge_capacity
-        coulombic_efficiency = summaries.coulombic_efficiency
-        ir_charge = summaries.ir_charge
+        discharge_capacity = summaries[hdr_summary["discharge_capacity_gravimetric"]]
+        charge_capacity = summaries[hdr_summary["charge_capacity_gravimetric"]]
+        coulombic_efficiency = summaries[hdr_summary["coulombic_efficiency"]]
+        ir_charge = summaries[hdr_summary["ir_charge"]]
 
     h_eff = int(height_fractions[0] * height)
     h_cap = int(height_fractions[1] * height)
     h_ir = int(height_fractions[2] * height)
 
     group_styles, sub_group_styles = create_plot_option_dicts(info)
 
@@ -509,14 +537,18 @@
     summaries,
     width=900,
     height=800,
     height_fractions=None,
     legend_option="all",
     **kwargs,
 ):
+    warnings.warn(
+        "This utility function will be seriously changed soon and possibly removed",
+        category=DeprecationWarning,
+    )
 
     import matplotlib.pyplot as plt
 
     logging.debug(f"   * stacking and plotting")
     logging.debug(f"      backend: {prms.Batch.backend}")
     logging.debug(f"      received kwargs: {kwargs}")
 
@@ -524,16 +556,16 @@
     if height_fractions is None:
         height_fractions = [0.3, 0.4, 0.3]
 
     # print(" running matplotlib plotter ".center(80,"="))
     # convert from bokeh to matplotlib - figsize - inch-ish
     width /= 80
     height /= 120
-    discharge_capacity = summaries.discharge_capacity
-    charge_capacity = summaries.charge_capacity
+    discharge_capacity = summaries[hdr_summary["discharge_capacity_gravimetric"]]
+    charge_capacity = summaries[hdr_summary["charge_capacity_gravimetric"]]
     coulombic_efficiency = summaries.coulombic_efficiency
     try:
         ir_charge = summaries.ir_charge
     except AttributeError:
         logging.debug("the data is missing ir charge")
         ir_charge = None
 
@@ -650,26 +682,29 @@
 
     return canvas
 
 
 def summary_plotting_engine(**kwargs):
     """creates plots of summary data."""
 
+    warnings.warn(
+        "This utility function will be seriously changed soon and possibly removed",
+        category=DeprecationWarning,
+    )
     logging.debug(f"Using {prms.Batch.backend} for plotting")
     experiments = kwargs.pop("experiments")
     farms = kwargs.pop("farms")
     barn = None
     logging.debug("    - summary_plot_engine")
     farms = _preparing_data_and_plotting(experiments=experiments, farms=farms, **kwargs)
     return farms, barn
 
 
 def _plotting_data(pages, summaries, width, height, height_fractions, **kwargs):
     # sub-sub-engine
-
     canvas = None
     if prms.Batch.backend == "bokeh":
         canvas = plot_cycle_life_summary_bokeh(
             pages, summaries, width, height, height_fractions, **kwargs
         )
     elif prms.Batch.backend == "matplotlib":
         logging.info("[obs! experimental]")
@@ -694,28 +729,28 @@
     height_fractions = kwargs.pop(
         "height_fractions", prms.Batch.summary_plot_height_fractions
     )
 
     for experiment in experiments:
         if not isinstance(experiment, CyclingExperiment):
             logging.info(
-                "No! This engine is only really good at" "processing CyclingExperiments"
+                "No! This engine is only really good at processing CyclingExperiments"
             )
             logging.info(experiment)
         else:
             pages = experiment.journal.pages
             try:
                 keys = [df.name for df in experiment.memory_dumped["summary_engine"]]
                 summaries = pd.concat(
                     experiment.memory_dumped["summary_engine"], keys=keys, axis=1
                 )
-
                 canvas = _plotting_data(
                     pages, summaries, width, height, height_fractions, **kwargs
                 )
+
                 farms.append(canvas)
 
             except KeyError:
                 logging.info("could not parse the summaries")
                 logging.info(" - might be new a bug?")
                 logging.info(
                     " - might be a known bug related to dropping cells (b.drop)"
```

### Comparing `cellpy-0.4.3a3/cellpy/utils/batch_tools/dumpers.py` & `cellpy-1.0.0a0/cellpy/utils/batch_tools/dumpers.py`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/cellpy/utils/batch_tools/engines.py` & `cellpy-1.0.0a0/cellpy/utils/batch_tools/engines.py`

 * *Files 19% similar despite different names*

```diff
@@ -2,40 +2,46 @@
 
     Keyword Args: experiments, farms, barn, optionals
     Returns: farms, barn
 """
 
 import logging
 import time
+import warnings
 
 import pandas as pd
 
 from cellpy import dbreader
-from cellpy.parameters.internal_settings import headers_journal
+from cellpy.parameters.internal_settings import get_headers_journal, get_headers_summary
 from cellpy.utils.batch_tools import batch_helpers as helper
 
-# logger = logging.getLogger(__name__)
+hdr_journal = get_headers_journal()
+hdr_summary = get_headers_summary()
 
 SELECTED_SUMMARIES = [
-    "discharge_capacity",
-    "charge_capacity",
-    "coulombic_efficiency",
-    "cumulated_coulombic_efficiency",
-    "ir_discharge",
-    "ir_charge",
-    "end_voltage_discharge",
-    "end_voltage_charge",
-    "charge_c_rate",
-    "discharge_c_rate",
+    hdr_summary["discharge_capacity_gravimetric"],
+    hdr_summary["charge_capacity_gravimetric"],
+    hdr_summary["coulombic_efficiency"],
+    hdr_summary["cumulated_coulombic_efficiency"],
+    hdr_summary["ir_discharge"],
+    hdr_summary["ir_charge"],
+    hdr_summary["end_voltage_discharge"],
+    hdr_summary["end_voltage_charge"],
+    hdr_summary["charge_c_rate"],
+    hdr_summary["discharge_c_rate"],
 ]
 
 
 def cycles_engine(**kwargs):
     """engine to extract cycles"""
     logging.debug("cycles_engine::Not finished yet (sorry).")
+    warnings.warn(
+        "This utility function will be seriously changed soon and possibly removed",
+        category=DeprecationWarning,
+    )
     # raise NotImplementedError
 
     experiments = kwargs["experiments"]
 
     farms = []
     barn = "raw_dir"  # Its a murder in the red barn - murder in the red barn
 
@@ -53,14 +59,18 @@
                 # extract cycles here and send it to the farm
 
     return farms, barn
 
 
 def raw_data_engine(**kwargs):
     """engine to extract raw data"""
+    warnings.warn(
+        "This utility function will be seriously changed soon and possibly removed",
+        category=DeprecationWarning,
+    )
     logging.debug("cycles_engine")
     farms = None
     barn = "raw_dir"
     raise NotImplementedError
 
 
 def summary_engine(**kwargs):
@@ -90,20 +100,24 @@
     return farms, barn
 
 
 def _load_summaries(experiment):
     summary_frames = {}
     for label in experiment.cell_names:
         # TODO: replace this with direct lookup from hdf5?
-        summary_frames[label] = experiment.data[label].cell.summary
+        summary_frames[label] = experiment.data[label].data.summary
     return summary_frames
 
 
 def dq_dv_engine(**kwargs):
     """engine that performs incremental analysis of the cycle-data"""
+    warnings.warn(
+        "This utility function will be seriously changed soon and possibly removed",
+        category=DeprecationWarning,
+    )
     farms = None
     barn = "raw_dir"
     raise NotImplementedError
 
 
 def _query(reader_method, cell_ids, column_name=None):
     if not any(cell_ids):
@@ -118,91 +132,109 @@
     except Exception as e:
         logging.debug(f"Error in querying db.")
         logging.debug(e)
         result = [None for _ in range(len(cell_ids))]
     return result
 
 
+def sql_db_engine(*args, **kwargs) -> pd.DataFrame:
+    print("sql_db_engine")
+    print(f"args: {args}")
+    print(f"kwargs: {kwargs}")
+    return pd.DataFrame()
+
+
+# TODO-246: load area
 def simple_db_engine(
     reader=None,
     cell_ids=None,
     file_list=None,
     pre_path=None,
     include_key=False,
     include_individual_arguments=True,
     additional_column_names=None,
+    batch_name=None,
     **kwargs,
 ):
     """Engine that gets values from the db for given set of cell IDs.
 
     The simple_db_engine looks up values for mass, names, etc. from
     the db using the reader object. In addition, it searches for the
     corresponding raw files / data.
 
     Args:
         reader: a reader object (defaults to dbreader.Reader)
-        cell_ids: keys (cell IDs)
+        cell_ids: keys (cell IDs) (assumes that the db has already been filtered, if not, use batch_name).
         file_list: file list to send to filefinder (instead of searching in folders for files).
         pre_path: prepended path to send to filefinder.
         include_key: include the key col in the pages (the cell IDs).
         include_individual_arguments: include the argument column in the pages.
         additional_column_names: list of additional column names to include in the pages.
+        batch_name: name of the batch (used if cell_ids are not given)
         **kwargs: sent to filefinder
 
     Returns:
         pages (pandas.DataFrame)
     """
 
+    new_version = False
+
     # This is not really a proper Do-er engine. But not sure where to put it.
     if reader is None:
         reader = dbreader.Reader()
         logging.debug("No reader provided. Creating one myself.")
-    pages_dict = dict()
-    pages_dict[headers_journal["filename"]] = _query(reader.get_cell_name, cell_ids)
-    if include_key:
-        pages_dict[headers_journal["id_key"]] = cell_ids
-
-    if include_individual_arguments:
-        pages_dict[headers_journal["argument"]] = _query(reader.get_args, cell_ids)
-
-    pages_dict[headers_journal["mass"]] = _query(reader.get_mass, cell_ids)
-    pages_dict[headers_journal["total_mass"]] = _query(reader.get_total_mass, cell_ids)
-    pages_dict[headers_journal["loading"]] = _query(reader.get_loading, cell_ids)
-    pages_dict[headers_journal["nom_cap"]] = _query(reader.get_nom_cap, cell_ids)
-    pages_dict[headers_journal["experiment"]] = _query(
-        reader.get_experiment_type, cell_ids
-    )
-    pages_dict[headers_journal["fixed"]] = _query(reader.inspect_hd5f_fixed, cell_ids)
-    pages_dict[headers_journal["label"]] = _query(reader.get_label, cell_ids)
-    pages_dict[headers_journal["cell_type"]] = _query(reader.get_cell_type, cell_ids)
-    pages_dict[headers_journal["instrument"]] = _query(reader.get_instrument, cell_ids)
-    pages_dict[headers_journal["raw_file_names"]] = []
-    pages_dict[headers_journal["cellpy_file_name"]] = []
-    pages_dict[headers_journal["comment"]] = _query(reader.get_comment, cell_ids)
-
-    if additional_column_names is not None:
-        for k in additional_column_names:
-            try:
-                pages_dict[k] = _query(reader.get_by_column_label, cell_ids, k)
-            except Exception as e:
-                logging.info(f"Could not retrieve from column {k} ({e})")
 
-    # get id_key (not implemented yet
+    if cell_ids is None:
+        pages_dict = reader.from_batch(
+            batch_name=batch_name,
+            include_key=include_key,
+            include_individual_arguments=include_individual_arguments,
+        )
+
+    else:
+        pages_dict = dict()
+        pages_dict[hdr_journal["filename"]] = _query(reader.get_cell_name, cell_ids)
+        if include_key:
+            pages_dict[hdr_journal["id_key"]] = cell_ids
+
+        if include_individual_arguments:
+            pages_dict[hdr_journal["argument"]] = _query(reader.get_args, cell_ids)
+
+        pages_dict[hdr_journal["mass"]] = _query(reader.get_mass, cell_ids)
+        pages_dict[hdr_journal["total_mass"]] = _query(reader.get_total_mass, cell_ids)
+        pages_dict[hdr_journal["loading"]] = _query(reader.get_loading, cell_ids)
+        pages_dict[hdr_journal["nom_cap"]] = _query(reader.get_nom_cap, cell_ids)
+        pages_dict[hdr_journal["area"]] = _query(reader.get_area, cell_ids)
+        pages_dict[hdr_journal["experiment"]] = _query(
+            reader.get_experiment_type, cell_ids
+        )
+        pages_dict[hdr_journal["fixed"]] = _query(reader.inspect_hd5f_fixed, cell_ids)
+        pages_dict[hdr_journal["label"]] = _query(reader.get_label, cell_ids)
+        pages_dict[hdr_journal["cell_type"]] = _query(reader.get_cell_type, cell_ids)
+        pages_dict[hdr_journal["instrument"]] = _query(reader.get_instrument, cell_ids)
+        pages_dict[hdr_journal["raw_file_names"]] = []
+        pages_dict[hdr_journal["cellpy_file_name"]] = []
+        pages_dict[hdr_journal["comment"]] = _query(reader.get_comment, cell_ids)
+        pages_dict[hdr_journal["group"]] = _query(reader.get_group, cell_ids)
+
+        if additional_column_names is not None:
+            for k in additional_column_names:
+                try:
+                    pages_dict[k] = _query(reader.get_by_column_label, cell_ids, k)
+                except Exception as e:
+                    logging.info(f"Could not retrieve from column {k} ({e})")
 
-    logging.debug(f"created info-dict from {reader.db_file}:")
-    # logging.debug(info_dict)
+        logging.debug(f"created info-dict from {reader.db_file}:")
 
     for key in list(pages_dict.keys()):
         logging.debug("%s: %s" % (key, str(pages_dict[key])))
 
-    _groups = _query(reader.get_group, cell_ids)
-
-    logging.debug(">\ngroups: %s" % str(_groups))
+    _groups = pages_dict[hdr_journal["group"]]
     groups = helper.fix_groups(_groups)
-    pages_dict[headers_journal["group"]] = groups
+    pages_dict[hdr_journal["group"]] = groups
 
     my_timer_start = time.time()
     pages_dict = helper.find_files(
         pages_dict, file_list=file_list, pre_path=pre_path, **kwargs
     )
     my_timer_end = time.time()
     if (my_timer_end - my_timer_start) > 5.0:
@@ -210,38 +242,36 @@
             "The function _find_files was very slow. "
             "Save your journal so you don't have to run it again! "
             "You can load it again using the from_journal(journal_name) method."
         )
 
     pages = pd.DataFrame(pages_dict)
     try:
-        pages = pages.sort_values([headers_journal.group, headers_journal.filename])
+        pages = pages.sort_values([hdr_journal.group, hdr_journal.filename])
     except TypeError as e:
         _report_suspected_duplicate_id(
             e,
             "sort the values",
-            pages[[headers_journal.group, headers_journal.filename]],
+            pages[[hdr_journal.group, hdr_journal.filename]],
         )
 
     pages = helper.make_unique_groups(pages)
 
     try:
-        pages[headers_journal.label] = pages[headers_journal.filename].apply(
+        pages[hdr_journal.label] = pages[hdr_journal.filename].apply(
             helper.create_labels
         )
     except AttributeError as e:
         _report_suspected_duplicate_id(
-            e, "make labels", pages[[headers_journal.label, headers_journal.filename]]
+            e, "make labels", pages[[hdr_journal.label, hdr_journal.filename]]
         )
 
     else:
         # TODO: check if drop=False works [#index]
-        pages.set_index(
-            headers_journal["filename"], inplace=True
-        )  # edit this to allow for
+        pages.set_index(hdr_journal["filename"], inplace=True)  # edit this to allow for
         # non-numeric index-names (for tab completion and python-box)
     return pages
 
 
 def _report_suspected_duplicate_id(e, what="do it", on=None):
     logging.warning(f"could not {what}")
     logging.warning(f"{on}")
```

### Comparing `cellpy-0.4.3a3/cellpy/utils/data/raw/20160805_test001_45_cc_01.res` & `cellpy-1.0.0a0/cellpy/utils/data/raw/20160805_test001_45_cc_01.res`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/cellpy/utils/easyplot.py` & `cellpy-1.0.0a0/cellpy/utils/easyplot.py`

 * *Files 1% similar despite different names*

```diff
@@ -245,15 +245,14 @@
 
             # If using arbin sql
             if self.use_arbin_sql:
                 cpobj = cellpy.get(
                     filename=file, instrument="arbin_sql"
                 )  # Initiate cellpy object
             else:  # Not Arbin SQL? Then its probably a local file
-
                 # Check that file(s) exist
                 if linked_files:
                     file_name = "_".join(file)
                     for _f in file:
                         if not os.path.isfile(_f):
                             logging.error("File not found: " + str(_f))
                             raise FileNotFoundError
@@ -457,15 +456,16 @@
         self,
         server="localhost",
         uid="sa",
         pwd="Changeme123",
         driver="ODBC Driver 17 for SQL Server",
     ):
         """Sets cellpy.prms.Instruments.Arbin details to fit what is inserted.
-        Parameters: Server = 'IP of server', uid = 'username', pwd = 'password', driver = 'ODBC Driver 17 for SQL Server'"""
+        Parameters: Server = 'IP of server', uid = 'username', pwd = 'password', driver = 'ODBC Driver 17 for SQL Server'
+        """
         cellpy.prms.Instruments.Arbin["SQL_server"] = server
         cellpy.prms.Instruments.Arbin["SQL_UID"] = uid
         cellpy.prms.Instruments.Arbin["SQL_PWD"] = pwd
         cellpy.prms.Instruments.Arbin["SQL_Driver"] = driver
         self.use_arbin_sql = True
 
     def give_color(self):
@@ -627,15 +627,15 @@
             elif self.kwargs["only_dischg"]:
                 ax.scatter(dchgs[0], dchgs[1], c=color, label=label)
             elif self.kwargs["only_chg"]:
                 ax.scatter(chgs[0], chgs[1], c=color, alpha=0.2)
 
             if self.kwargs["cyclelife_coulombic_efficiency"]:
                 # Get CE for cyc_nums
-                coulombic_efficiency = cpobj.cell.summary[
+                coulombic_efficiency = cpobj.data.summary[
                     "coulombic_efficiency_u_percentage"
                 ]
                 cycs = []
                 CEs = []
                 for cyc in keys:
                     if cyc in cyc_nums:
                         cycs.append(cyc)
@@ -645,17 +645,17 @@
                 ax_ce.scatter(cycs, CEs, c=color, marker="+")
                 # print(filename + " Dchg 1-3: " + str(dchgs[1][0:3])  + ", CE 1-3: " + str(coulombic_efficiency[0:3]))
 
             if (
                 self.kwargs["cyclelife_charge_c_rate"]
                 or self.kwargs["cyclelife_discharge_c_rate"]
             ):
-                # charge_c_rate = cpobj.cell.summary["charge_c_rate"] #This gives incorrect c-rates.
+                # charge_c_rate = cpobj.data.summary["charge_c_rate"] #This gives incorrect c-rates.
 
-                stepstable = cpobj.cell.steps
+                stepstable = cpobj.data.steps
                 chg_c_rates, dchg_c_rates = get_effective_C_rates(stepstable)
 
                 selected_chg_c_rates = []
                 selected_dchg_c_rates = []
                 selected_cycs = []
 
                 for cyc in keys:
@@ -744,27 +744,25 @@
                 self.fix_cyclelife(fig, ax)
 
                 # Save fig
                 savepath = outpath.strip("_") + "_Cyclelife"
                 self.save_fig(fig, savepath)
 
         if not self.kwargs["cyclelife_separate_data"]:
-
             # Set all plot settings from Plot object
             self.fix_cyclelife(fig, ax)
 
             # Save fig
             savepath = outpath.strip("_") + "_Cyclelife"
             self.save_fig(fig, savepath)
 
     def plot_gc(self):
         """Takes all the parameters inserted in the object creation and plots Voltage-Capacity curves"""
 
         if self.kwargs["all_in_one"]:  # Everything goes in the same figure.
-
             fig, ax = self.give_fig()
             colors = [
                 "tab:blue",
                 "tab:orange",
                 "tab:green",
                 "tab:red",
                 "tab:purple",
@@ -958,15 +956,14 @@
                 self.save_fig(fig, savepath)
 
     def plot_dQdV(self):
         """Takes all the parameters inserted in the object creation and plots dQdV"""
         from cellpy.utils import ica
 
         if self.kwargs["all_in_one"]:  # Everything goes in the same figure.
-
             fig, ax = self.give_fig()
             colors = [
                 "tab:blue",
                 "tab:orange",
                 "tab:green",
                 "tab:red",
                 "tab:purple",
@@ -1137,15 +1134,14 @@
                 "tab:olive",
                 "tab:cyan",
             ] * 5
             savepath = self.outpath
 
             colorbar_incrementor = -1
             for cpobj, cyc_nums, color, filename in self.file_data:
-
                 # Get Pandas DataFrame of pot vs cap from cellpy object
                 df = cpobj.get_cap(
                     method="forth-and-forth",
                     label_cycle_number=True,
                     categorical_column=True,
                 )
 
@@ -1477,15 +1473,15 @@
             # df = cpobj.get_cap(method="forth-and-forth", label_cycle_number=True, categorical_column=True)
             outpath += os.path.basename(filename).split(".")[0] + "_"
 
             handles.append(
                 Line2D([0], [0], marker="o", color=color, label=filename, linestyle="")
             )
 
-            stepstable = cpobj.cell.steps
+            stepstable = cpobj.data.steps
             chglist, dchglist = get_effective_C_rates_and_caps(stepstable)
 
             # Remove all cycles which are not in cyc_nums by looking at the 0th element (cyc num) of every sublist in chglist
             new_chglist = [x for x in chglist if x[0] in cyc_nums]
             new_dchglist = [x for x in dchglist if x[0] in cyc_nums]
 
             linregress_xlist = []
@@ -1870,15 +1866,14 @@
                 savepath += self.kwargs["outtype"]
 
             print("Saving to: " + savepath)
             fig.savefig(savepath, bbox_inches="tight", dpi=self.kwargs["figres"])
 
 
 def get_effective_C_rates(steptable):
-
     newdf = steptable[["step_time_avr", "cycle", "type"]]
     chg_c_rates = []
     dchg_c_rates = []
     for i, elem in enumerate(newdf.iterrows()):
         if elem[1]["type"] == "charge":
             chg_c_rates.append(1 / (elem[1]["step_time_avr"] / 3600))
         elif elem[1]["type"] == "discharge":
```

### Comparing `cellpy-0.4.3a3/cellpy/utils/example_data.py` & `cellpy-1.0.0a0/cellpy/utils/example_data.py`

 * *Files 6% similar despite different names*

```diff
@@ -7,23 +7,23 @@
 
 logging.info("Ready to help you to get some data to play with.")
 CURRENT_PATH = Path(os.path.dirname(os.path.realpath(__file__)))
 RAW_PATH = CURRENT_PATH / "data" / "raw"
 H5_PATH = CURRENT_PATH / "data"
 
 
-def arbin_file(auto_summary=True, testing=False):
+def raw_file(auto_summary=True, testing=False):
     """load an example data file (arbin).
 
     Args:
         auto_summary (bool): run make_summary automatically (defaults to True)
         testing (bool): run in test mode
 
     Returns:
-        cellpy.CellpyData object with the arbin data loaded
+        cellpy.CellpyCell object with the arbin data loaded
 
     """
     file_path = RAW_PATH / "20160805_test001_45_cc_01.res"
     mass = 0.704
     return cellpy.get(
         filename=file_path, mass=mass, auto_summary=auto_summary, testing=testing
     )
@@ -32,15 +32,15 @@
 def cellpy_file(testing=False):
     """load an example cellpy file.
 
     Args:
         testing (bool): run in test mode
 
     Returns:
-        cellpy.CellpyData object with the arbin data loaded
+        cellpy.CellpyCell object with the arbin data loaded
     """
 
     file_path = H5_PATH / "20160805_test001_45_cc.h5"
     return cellpy.get(filename=file_path, testing=testing)
 
 
 def cellpy_file_path():
@@ -49,7 +49,13 @@
     return H5_PATH / "20160805_test001_45_cc.h5"
 
 
 def arbin_file_path():
     """Get the path to an example arbin res file"""
 
     return RAW_PATH / "20160805_test001_45_cc_01.res"
+
+
+if __name__ == "__main__":
+    a = raw_file()
+    print("Saving new version of the cellpy file!")
+    a.save(cellpy_file_path())
```

### Comparing `cellpy-0.4.3a3/cellpy/utils/helpers.py` & `cellpy-1.0.0a0/cellpy/utils/helpers.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,47 +1,49 @@
-import collections
 import logging
 import os
 import pathlib
 import warnings
 from copy import deepcopy
 
 import numpy as np
 import pandas as pd
 from scipy import stats
 
 import cellpy
-from cellpy import prmreader, prms
+from cellpy import prms
 from cellpy.parameters.internal_settings import (
-    ATTRS_CELLPYDATA,
-    ATTRS_DATASET,
-    headers_journal,
-    headers_normal,
-    headers_step_table,
-    headers_summary,
+    get_headers_journal,
+    get_headers_summary,
+    get_headers_step_table,
+    get_headers_normal,
 )
-from cellpy.readers.cellreader import CellpyData
-from cellpy.utils import batch, ica
+from cellpy.readers.cellreader import CellpyCell
+from cellpy.utils.batch import Batch
 
-hdr_summary = headers_summary
-hdr_steps = headers_step_table
-hdr_normal = headers_normal
-hdr_journal = headers_journal
+hdr_summary = get_headers_summary()
+hdr_steps = get_headers_step_table()
+hdr_normal = get_headers_normal()
+hdr_journal = get_headers_journal()
 
 
-def _make_average(
-    frames, keys=None, columns=None, normalize_cycles=False, key_index_bounds=None
+def _make_average_legacy(
+    frames,
+    keys=None,
+    columns=None,
+    skip_st_dev_for_equivalent_cycle_index=True,
+    key_index_bounds=None,
 ):
     if key_index_bounds is None:
         key_index_bounds = [1, -2]
     hdr_norm_cycle = hdr_summary["normalized_cycle_index"]
     hdr_cum_charge = hdr_summary["cumulated_charge_capacity"]
     cell_id = ""
     not_a_number = np.NaN
     new_frames = []
+
     if columns is None:
         columns = frames[0].columns
 
     if keys is not None:
         if isinstance(keys, (list, tuple)):
             cell_id = list(
                 set(
@@ -52,51 +54,132 @@
                         for k in keys
                     ]
                 )
             )[0]
         elif isinstance(keys, str):
             cell_id = keys
     new_frame = pd.concat(frames, axis=1)
-
     for col in columns:
         number_of_cols = len(new_frame.columns)
-        if col in [hdr_norm_cycle, hdr_cum_charge] and normalize_cycles:
+        if (
+            col in [hdr_norm_cycle, hdr_cum_charge]
+            and skip_st_dev_for_equivalent_cycle_index
+        ):
             if number_of_cols > 1:
                 avg_frame = (
                     new_frame[col].agg(["mean"], axis=1).rename(columns={"mean": col})
                 )
             else:
                 avg_frame = new_frame[col].copy()
 
         else:
-
             new_col_name_mean = col + "_mean"
             new_col_name_std = col + "_std"
 
             if number_of_cols > 1:
-                # very slow:
                 avg_frame = (
                     new_frame[col]
                     .agg(["mean", "std"], axis=1)
                     .rename(
                         columns={"mean": new_col_name_mean, "std": new_col_name_std}
                     )
                 )
             else:
                 avg_frame = pd.DataFrame(
                     data=new_frame[col].values, columns=[new_col_name_mean]
                 )
                 avg_frame[new_col_name_std] = not_a_number
-
         new_frames.append(avg_frame)
     final_frame = pd.concat(new_frames, axis=1)
 
     return final_frame, cell_id
 
 
+def _make_average(
+    frames,
+    keys=None,
+    columns=None,
+    skip_st_dev_for_equivalent_cycle_index=True,
+    key_index_bounds=None,
+):
+    if key_index_bounds is None:
+        key_index_bounds = [1, -2]
+    hdr_norm_cycle = hdr_summary["normalized_cycle_index"]
+    hdr_cum_charge = hdr_summary["cumulated_charge_capacity"]
+    cell_id = ""
+    not_a_number = np.NaN
+    new_frames = []
+
+    if columns is None:
+        columns = frames[0].columns
+
+    if keys is not None:
+        if isinstance(keys, (list, tuple)):
+            cell_id = list(
+                set(
+                    [
+                        "_".join(
+                            k.split("_")[key_index_bounds[0] : key_index_bounds[1]]
+                        )
+                        for k in keys
+                    ]
+                )
+            )[0]
+        elif isinstance(keys, str):
+            cell_id = keys
+    new_frame = pd.concat(frames, axis=1)
+    normalized_cycle_index_frame = pd.DataFrame(index=new_frame.index)
+    for col in columns:
+        number_of_cols = len(new_frame.columns)
+        if col == hdr_norm_cycle and skip_st_dev_for_equivalent_cycle_index:
+            if number_of_cols > 1:
+                normalized_cycle_index_frame = (
+                    new_frame[col]
+                    .agg(["mean"], axis=1)
+                    .rename(columns={"mean": "equivalent_cycle"})
+                )
+            else:
+                normalized_cycle_index_frame = new_frame[col].copy()
+
+        else:
+            new_col_name_mean = "mean"
+            new_col_name_std = "std"
+
+            if number_of_cols > 1:
+                avg_frame = (
+                    new_frame[col].agg(["mean", "std"], axis=1)
+                    # .rename(
+                    #     columns={"mean": "value"}
+                    # )
+                )
+            else:
+                avg_frame = pd.DataFrame(
+                    data=new_frame[col].values, columns=[new_col_name_mean]
+                )
+                avg_frame[new_col_name_std] = not_a_number
+
+            avg_frame = avg_frame.assign(variable=col)
+            new_frames.append(avg_frame)
+
+    if not normalized_cycle_index_frame.empty:
+        new_frames = [
+            pd.concat([normalized_cycle_index_frame, x], axis=1) for x in new_frames
+        ]
+    final_frame = pd.concat(new_frames, axis=0)
+    cols = final_frame.columns.to_list()
+    new_cols = []
+    for n in ["variable", "mean", "std"]:
+        if n in cols:
+            new_cols.append(n)
+            cols.remove(n)
+    cols.extend(new_cols)
+    final_frame = final_frame.reindex(columns=cols)
+    return final_frame, cell_id
+
+
 def update_journal_cellpy_data_dir(
     pages, new_path=None, from_path="PureWindowsPath", to_path="Path"
 ):
     """Update the path in the pages (batch) from one type of OS to another.
 
     I use this function when I switch from my work PC (windows) to my home
     computer (mac).
@@ -123,175 +206,96 @@
     pages.cellpy_file_names = pages.cellpy_file_names.apply(
         lambda x: to_path(new_path) / x.name
     )
     return pages
 
 
 def make_new_cell():
-    """create an empty CellpyData object."""
+    """create an empty CellpyCell object."""
     warnings.warn(
-        "make_new_cell is deprecated, use CellpyData.vacant instead", DeprecationWarning
+        "make_new_cell is deprecated, use CellpyCell.vacant instead", DeprecationWarning
     )
-    new_cell = cellpy.cellreader.CellpyData(initialize=True)
+    new_cell = cellpy.cellreader.CellpyCell(initialize=True)
     return new_cell
 
 
-def split_experiment(cell, base_cycles=None):
-    """Split experiment (CellpyData object) into several sub-experiments.
-
-    Args:
-        cell (CellpyData): original cell
-        base_cycles (int or list of ints): cycle(s) to do the split on.
-
-    Returns:
-        List of CellpyData objects
-    """
-    warnings.warn(
-        "split_experiment is deprecated, use CellpyData.split_many instead",
-        DeprecationWarning,
-    )
-
-    if base_cycles is None:
-        all_cycles = cell.get_cycle_numbers()
-        base_cycles = int(np.median(all_cycles))
-
-    cells = list()
-    if not isinstance(base_cycles, (list, tuple)):
-        base_cycles = [base_cycles]
-
-    dataset = cell.cell
-    steptable = dataset.steps
-    data = dataset.raw
-    summary = dataset.summary
-
-    for b_cycle in base_cycles:
-        steptable0, steptable = [
-            steptable[steptable.cycle < b_cycle],
-            steptable[steptable.cycle >= b_cycle],
-        ]
-        data0, data = [
-            data[data.cycle_index < b_cycle],
-            data[data.cycle_index >= b_cycle],
-        ]
-        summary0, summary = [
-            summary[summary.index < b_cycle],
-            summary[summary.index >= b_cycle],
-        ]
-
-        new_cell = CellpyData.vacant()
-
-        new_cell.cell.steps = steptable0
-
-        new_cell.cell.raw = data0
-        new_cell.cell.summary = summary0
-
-        old_cell = CellpyData.vacant()
-        old_cell.cell.steps = steptable
-
-        old_cell.cell.raw = data
-        old_cell.cell.summary = summary
-
-        for attr in ATTRS_DATASET:
-            value = getattr(cell.cell, attr)
-            setattr(new_cell.cell, attr, value)
-            setattr(old_cell.cell, attr, value)
-
-        for attr in ATTRS_CELLPYDATA:
-            value = getattr(cell, attr)
-            setattr(new_cell, attr, value)
-            setattr(old_cell, attr, value)
-
-        cells.append(new_cell)
-
-    cells.append(old_cell)
-
-    return cells
-
-
-def add_normalized_cycle_index(summary, nom_cap=None, column_name=None):
+def add_normalized_cycle_index(summary, nom_cap, column_name=None):
     """Adds normalized cycles to the summary data frame.
 
     This functionality is now also implemented as default when creating
-    the summary (make_summary). However it is kept here if you would like to
+    the summary (make_summary). However, it is kept here if you would like to
     redo the normalization, for example if you want to use another nominal
     capacity or if you would like to have more than one normalized cycle index.
 
     Args:
-        summary (pandas.DataFrame): cell summary
-        cell (CellpyData): cell object
-        nom_cap (float): nominal capacity to use when normalizing. Defaults to
-            the nominal capacity defined in the cell object (this is typically
-            set during creation of the CellpyData object based on the value
-            given in the parameter file).
+        summary (pandas.DataFrame): data summary
+        nom_cap (float): nominal capacity to use when normalizing.
         column_name (str): name of the new column. Uses the name defined in
             cellpy.parameters.internal_settings as default.
 
     Returns:
-        cell object now with normalized cycle index in its summary.
+        data object now with normalized cycle index in its summary.
     """
     hdr_norm_cycle = hdr_summary["normalized_cycle_index"]
-    hdr_cum_charge = hdr_summary["cumulated_charge_capacity"]
+    hdr_cum_charge = hdr_summary["cumulated_charge_capacity_gravimetric"]
 
     if column_name is None:
         column_name = hdr_norm_cycle
-    hdr_cum_charge = hdr_cum_charge
 
-    # if nom_cap is None:
-    #   nom_cap = cell.cell.nom_cap
     summary[column_name] = summary[hdr_cum_charge] / nom_cap
     return summary
 
 
 def add_c_rate(cell, nom_cap=None, column_name=None):
     """Adds C-rates to the step table data frame.
 
     This functionality is now also implemented as default when creating
     the step_table (make_step_table). However it is kept here if you would
     like to recalculate the C-rates, for example if you want to use another
     nominal capacity or if you would like to have more than one column with
     C-rates.
 
     Args:
-        cell (CellpyData): cell object
+        cell (CellpyCell): cell object
         nom_cap (float): nominal capacity to use for estimating C-rates.
             Defaults to the nominal capacity defined in the cell object
             (this is typically set during creation of the CellpyData object
             based on the value given in the parameter file).
         column_name (str): name of the new column. Uses the name defined in
             cellpy.parameters.internal_settings as default.
 
     Returns:
-        cell object.
+        data object.
     """
 
     # now also included in step_table
     # TODO: remove this function
     if column_name is None:
         column_name = hdr_steps["rate_avr"]
     if nom_cap is None:
-        nom_cap = cell.cell.nom_cap
+        nom_cap = cell.data.nom_cap
 
     spec_conv_factor = cell.get_converter_to_specific()
-    cell.cell.steps[column_name] = abs(
-        round(cell.cell.steps.current_avr / (nom_cap / spec_conv_factor), 2)
+    cell.data.steps[column_name] = abs(
+        round(cell.data.steps.current_avr / (nom_cap / spec_conv_factor), 2)
     )
 
     return cell
 
 
 def add_areal_capacity(cell, cell_id, journal):
     """Adds areal capacity to the summary."""
 
     loading = journal.pages.loc[cell_id, hdr_journal["loading"]]
 
-    cell.cell.summary[hdr_summary["areal_charge_capacity"]] = (
-        cell.cell.summary[hdr_summary["charge_capacity"]] * loading / 1000
+    cell.data.summary[hdr_summary["areal_charge_capacity"]] = (
+        cell.data.summary[hdr_summary["charge_capacity"]] * loading / 1000
     )
-    cell.cell.summary[hdr_summary["areal_discharge_capacity"]] = (
-        cell.cell.summary[hdr_summary["discharge_capacity"]] * loading / 1000
+    cell.data.summary[hdr_summary["areal_discharge_capacity"]] = (
+        cell.data.summary[hdr_summary["discharge_capacity"]] * loading / 1000
     )
     return cell
 
 
 def _remove_outliers_from_summary(s, filter_vals, freeze_indexes=None):
     if freeze_indexes is not None:
         try:
@@ -496,21 +500,21 @@
         b = deepcopy(b)
 
     if last is None:
         return b
 
     for cell_number, cell_label in enumerate(b.experiment.cell_names):
         c = b.experiment.data[cell_label]
-        s = c.cell.summary
+        s = c.data.summary
         if isinstance(last, dict):
             last_this_cell = last.get(cell_label, None)
         else:
             last_this_cell = last
         s = remove_last_cycles_from_summary(s, last_this_cell)
-        c.cell.summary = s
+        c.data.summary = s
     if keep_old:
         return b
 
 
 def yank_before(b, first=None, keep_old=False):
     """Cut all cycles before a given cycle index number.
 
@@ -528,27 +532,27 @@
         b = deepcopy(b)
 
     if first is None:
         return b
 
     for cell_number, cell_label in enumerate(b.experiment.cell_names):
         c = b.experiment.data[cell_label]
-        s = c.cell.summary
+        s = c.data.summary
         if isinstance(first, dict):
             first_this_cell = first.get(cell_label, None)
         else:
             first_this_cell = first
         s = remove_first_cycles_from_summary(s, first_this_cell)
-        c.cell.summary = s
+        c.data.summary = s
     if keep_old:
         return b
 
 
 def yank_outliers(
-    b,
+    b: Batch,
     zscore_limit=None,
     low=0.0,
     high=7_000.0,
     filter_cols=None,
     freeze_indexes=None,
     remove_indexes=None,
     remove_last=False,
@@ -590,15 +594,15 @@
 
     removed_cycles = dict()
 
     # remove based on indexes and values
     for cell_number, cell_label in enumerate(b.experiment.cell_names):
         logging.debug(f"yanking {cell_label} ")
         c = b.experiment.data[cell_label]
-        s = c.cell.summary
+        s = c.data.summary
         before = set(s.index)
         if remove_indexes is not None:
             logging.debug("removing indexes")
             if isinstance(remove_indexes, dict):
                 remove_indexes_this_cell = remove_indexes.get(cell_label, None)
             else:
                 remove_indexes_this_cell = remove_indexes
@@ -623,196 +627,203 @@
         if distance is not None:
             s = remove_outliers_from_summary_on_nn_distance(
                 s,
                 distance=distance,
                 filter_cols=filter_cols,
                 freeze_indexes=freeze_indexes,
             )
-            c.cell.summary = s
+            c.data.summary = s
 
         if window_size is not None:
             s = remove_outliers_from_summary_on_window(
                 s,
                 window_size=window_size,
                 cut=window_cut,
                 iterations=iterations,
                 freeze_indexes=freeze_indexes,
             )
 
         removed = before - set(s.index)
-        c.cell.summary = s
+        c.data.summary = s
         if removed:
             removed_cycles[cell_label] = list(removed)
 
     if zscore_limit is not None:
         logging.info("using the zscore - removed cycles not kept track on")
         for j in range(iterations):
             tot_rows_removed = 0
             for cell_number, cell_label in enumerate(b.experiment.cell_names):
                 c = b.experiment.data[cell_label]
-                n1 = len(c.cell.summary)
+                n1 = len(c.data.summary)
                 s = remove_outliers_from_summary_on_zscore(
-                    c.cell.summary,
+                    c.data.summary,
                     filter_cols=filter_cols,
                     zscore_limit=zscore_limit,
                     freeze_indexes=freeze_indexes,
                 )
                 # TODO: populate removed_cycles
                 rows_removed = n1 - len(s)
                 tot_rows_removed += rows_removed
-                c.cell.summary = s
+                c.data.summary = s
             if tot_rows_removed == 0:
                 break
             zscore_limit *= zscore_multiplyer
 
     if keep_old:
         return b
     else:
         return removed_cycles
 
 
-# from helpers - updated
 def concatenate_summaries(
-    b,
+    b: Batch,
     max_cycle=None,
     rate=None,
     on="charge",
     columns=None,
     column_names=None,
     normalize_capacity_on=None,
     scale_by=None,
     nom_cap=None,
     normalize_cycles=False,
-    add_areal=False,
     group_it=False,
     rate_std=None,
     rate_column=None,
     inverse=False,
     inverted=False,
-    key_index_bounds=[1, -2],
+    key_index_bounds=None,
     melt=False,
     cell_type_split_position="auto",
-):
-
+    mode="collector",
+) -> pd.DataFrame:
     """Merge all summaries in a batch into a gigantic summary data frame.
 
-    TODO: Allow also dictionaries of cell objects.
-    TODO: Allow iterating through batch-objects (for id, name in b.iteritems() or similar)
-
     Args:
         b (cellpy.batch object): the batch with the cells.
         max_cycle (int): drop all cycles above this value.
         rate (float): filter on rate (C-rate)
         on (str or list of str): only select cycles if based on the rate of this step-type (e.g. on="charge").
-        columns (list): selected column(s) (using cellpy name) [defaults to "charge_capacity"]
+        columns (list): selected column(s) (using cellpy attribute name) [defaults to "charge_capacity_gravimetric"]
         column_names (list): selected column(s) (using exact column name)
-        normalize_capacity_on (list): list of cycle numbers that will be used for setting the basis of the normalization (typically the first few cycles after formation)
-        scale_by (float or str): scale the normalized data with nominal capacity if "nom_cap", or given value (defaults to one).
+        normalize_capacity_on (list): list of cycle numbers that will be used for setting the basis of the
+            normalization (typically the first few cycles after formation)
+        scale_by (float or str): scale the normalized data with nominal capacity if "nom_cap",
+            or given value (defaults to one).
         nom_cap (float): nominal capacity of the cell
-        normalize_cycles (bool): perform a normalisation of the cycle numbers (also called equivalent cycle index)
-        add_areal (bool):  add areal capacity to the summary
+        normalize_cycles (bool): perform a normalization of the cycle numbers (also called equivalent cycle index)
         group_it (bool): if True, average pr group.
         rate_std (float): allow for this inaccuracy when selecting cycles based on rate
         rate_column (str): name of the column containing the C-rates.
-        inverse (bool): select steps that does not have the given C-rate.
-        inverted (bool): select cycles that does not have the steps filtered by given C-rate.
+        inverse (bool): select steps that do not have the given C-rate.
+        inverted (bool): select cycles that do not have the steps filtered by given C-rate.
         key_index_bounds (list): used when creating a common label for the cells by splitting and combining from
             key_index_bound[0] to key_index_bound[1].
         melt (bool): return frame as melted (long format).
         cell_type_split_position (int | None | "auto"): list item number for creating a cell type identifier
             after performing a split("_") on the cell names (only valid if melt==True). Set to None to not
             include a cell_type col.
+        mode (str): set to something else than "collector" to get the "old" behaviour of this function.
 
     Returns:
         Multi-index ``pandas.DataFrame``
 
     Notes:
         The returned ``DataFrame`` has the following structure:
 
         - top-level columns or second col for melted: cell-names (cell_name)
         - second-level columns or first col for melted: summary headers (summary_headers)
         - row-index or third col for melted: cycle number (cycle_index)
         - cell_type on forth col for melted if cell_type_split_position is given
 
     """
 
-    # TODO: refactor me
-    # TODO: check if selecting normalize_cycles and group_it performs the operation in logical order
-    if normalize_capacity_on is not None:
-        default_columns = ["normalized_charge_capacity"]
-    else:
-        default_columns = ["charge_capacity"]
-
-    import logging
+    if mode != "collector":
+        warnings.warn(
+            "This helper function will be removed shortly", category=DeprecationWarning
+        )
 
+    default_columns = [hdr_summary["charge_capacity_gravimetric"]]
     reserved_cell_label_names = ["FC"]
-
     hdr_norm_cycle = hdr_summary["normalized_cycle_index"]
-    hdr_cum_charge = hdr_summary["cumulated_charge_capacity"]
 
-    cell_names_nest = []
-    frames = []
-    keys = []
+    if key_index_bounds is None:
+        key_index_bounds = [1, -2]
 
-    if columns is not None:
-        if any(s.startswith("areal_") for s in columns):
-            add_areal = True
-        if normalize_capacity_on is not None:
-            _columns = []
-            for name in columns:
-                if name.startswith("normalized_"):
-                    _columns.append(hdr_summary[name])
-                else:
-                    _columns.append(hdr_summary["normalized_" + name])
-            columns = _columns
-        else:
-            columns = [hdr_summary[name] for name in columns]
-    else:
+    if columns is None:
         columns = []
 
-    if column_names is not None:
-        columns += column_names
+    if column_names is None:
+        column_names = []
+
+    if isinstance(columns, str):
+        columns = [columns]
+
+    if isinstance(column_names, str):
+        column_names = [column_names]
 
-    if len(columns) > 1:
-        columns = [hdr_summary[name] for name in default_columns]
+    columns = [hdr_summary[name] for name in columns]
+    columns += column_names
 
-    logging.info(f"concatenating the following columns: {columns}")
-    normalize_cycles_headers = []
+    if not columns:
+        columns = default_columns
+
+    cell_names_nest = []
+    output_columns = columns.copy()
+    frames = []
+    keys = []
 
     if normalize_cycles:
         if hdr_norm_cycle not in columns:
-            normalize_cycles_headers.append(hdr_norm_cycle)
-        if hdr_cum_charge not in columns:
-            normalize_cycles_headers.append(hdr_cum_charge)
+            output_columns.insert(0, hdr_norm_cycle)
+
+    if normalize_capacity_on is not None:
+        normalize_capacity_headers = [
+            hdr_summary["normalized_charge_capacity"],
+            hdr_summary["normalized_discharge_capacity"],
+        ]
+        output_columns = [
+            col
+            for col in output_columns
+            if col
+            not in [
+                hdr_summary["charge_capacity"],
+                hdr_summary["discharge_capacity"],
+            ]
+        ]
+        output_columns.extend(normalize_capacity_headers)
 
     if group_it:
         g = b.pages.groupby("group")
         for gno, b_sub in g:
             cell_names_nest.append(list(b_sub.index))
     else:
         cell_names_nest.append(list(b.experiment.cell_names))
 
     for cell_names in cell_names_nest:
         frames_sub = []
         keys_sub = []
-
         for cell_id in cell_names:
             logging.debug(f"Processing [{cell_id}]")
-            c = b.experiment.data[cell_id]
+            group = b.pages.loc[cell_id, "group"]
+            sub_group = b.pages.loc[cell_id, "sub_group"]
+            try:
+                c = b.experiment.data[cell_id]
+                # print(c.data.summary.columns.sort_values())
+            except KeyError as e:
+                logging.debug(f"Could not load data for {cell_id}")
+                logging.debug(f"{e}")
+                raise e
 
             if not c.empty:
                 if max_cycle is not None:
                     c = c.drop_from(max_cycle + 1)
-                if add_areal:
-                    c = add_areal_capacity(c, cell_id, b.experiment.journal)
-
                 if normalize_capacity_on is not None:
                     if scale_by == "nom_cap":
                         if nom_cap is None:
-                            scale_by = c.cell.nom_cap
+                            scale_by = c.data.nom_cap
                         else:
                             scale_by = nom_cap
                     elif scale_by is None:
                         scale_by = 1.0
 
                     c = add_normalized_capacity(
                         c, norm_cycles=normalize_capacity_on, scale=scale_by
@@ -826,61 +837,59 @@
                         rate_std=rate_std,
                         rate_column=rate_column,
                         inverse=inverse,
                         inverted=inverted,
                     )
 
                 else:
-                    s = c.cell.summary
+                    s = c.data.summary
 
                 if columns is not None:
-                    if normalize_cycles:
-                        s = s.loc[:, normalize_cycles_headers + columns].copy()
-                    else:
-                        s = s.loc[:, columns].copy()
-
-                if normalize_cycles:
-                    if nom_cap is None:
-                        _nom_cap = c.cell.nom_cap
-                    else:
-                        _nom_cap = nom_cap
-
-                    if not group_it:
-                        s = add_normalized_cycle_index(s, nom_cap=_nom_cap)
-                        if hdr_cum_charge not in columns:
-                            s = s.drop(columns=hdr_cum_charge)
+                    s = s.loc[:, output_columns].copy()
 
-                    s = s.reset_index(drop=True)
+                # somehow using normalized cycles (i.e. equivalent cycles) messes up the order of the index sometimes:
+                if normalize_cycles and mode != "collector":
+                    s = s.reset_index()
+
+                # add group and subgroup
+                if not group_it:
+                    s = s.assign(group=group, sub_group=sub_group)
 
                 frames_sub.append(s)
                 keys_sub.append(cell_id)
 
         if group_it:
-            try:
-                if normalize_cycles:
+            if mode == "collector":
+                try:
                     s, cell_id = _make_average(
                         frames_sub,
                         keys_sub,
-                        normalize_cycles_headers + columns,
-                        True,
-                        key_index_bounds,
+                        output_columns,
+                        key_index_bounds=key_index_bounds,
                     )
-                    s = add_normalized_cycle_index(s, nom_cap=_nom_cap)
-                    if hdr_cum_charge not in columns:
-                        s = s.drop(columns=hdr_cum_charge)
+                except ValueError as e:
+                    print("could not make average!")
+                    print(e)
                 else:
-                    s, cell_id = _make_average(
-                        frames_sub, keys_sub, columns, key_index_bounds=key_index_bounds
-                    )
-            except ValueError as e:
-                print("could not make average!")
-                print(e)
+                    frames.append(s)
+                    keys.append(cell_id)
             else:
-                frames.append(s)
-                keys.append(cell_id)
+                try:
+                    s, cell_id = _make_average_legacy(
+                        frames_sub,
+                        keys_sub,
+                        output_columns,
+                        key_index_bounds=key_index_bounds,
+                    )
+                except ValueError as e:
+                    print("could not make average!")
+                    print(e)
+                else:
+                    frames.append(s)
+                    keys.append(cell_id)
         else:
             frames.extend(frames_sub)
             keys.extend(keys_sub)
 
     if frames:
         if len(set(keys)) != len(keys):
             logging.info("Got several columns with same test-name")
@@ -892,16 +901,39 @@
                     if name in used_names:
                         name += "x"
                     else:
                         break
                 new_keys.append(name)
                 used_names.append(name)
             keys = new_keys
-        cdf = pd.concat(frames, keys=keys, axis=1)
-        cdf = cdf.rename_axis(columns=["cell_name", "summary_header"])
+
+        if mode == "collector":
+            old_normalized_cycle_header = hdr_norm_cycle
+            cycle_header = "cycle"
+            normalized_cycle_header = "equivalent_cycle"
+            group_header = "group"
+            sub_group_header = "sub_group"
+            cell_header = "cell"
+            average_header_end = "_mean"
+            std_header_end = "_std"
+
+            cdf = pd.concat(
+                frames, keys=keys, axis=0, names=[cell_header, cycle_header]
+            )
+            cdf = cdf.reset_index(drop=False)
+            id_vars = [cell_header, cycle_header]
+            if not group_it:
+                id_vars.extend([group_header, sub_group_header])
+            if normalize_cycles:
+                cdf = cdf.rename(
+                    columns={old_normalized_cycle_header: normalized_cycle_header}
+                )
+            return cdf
+
+        # if not using through collectors (i.e. using the old methodology instead):
         if melt:
             cdf = cdf.reset_index(drop=False).melt(
                 id_vars=hdr_summary.cycle_index, value_name="value"
             )
             melted_column_order = [
                 "summary_header",
                 "cell_name",
@@ -914,23 +946,21 @@
                 _pp = cdf.cell_name.str.split("_", expand=True).values[0]
                 for _p in _pp[1:]:
                     if _p not in reserved_cell_label_names:
                         break
                     cell_type_split_position += 1
 
             if cell_type_split_position is not None:
-
                 cdf = cdf.assign(
                     cell_type=cdf.cell_name.str.split("_", expand=True)[
                         cell_type_split_position
                     ]
                 )
                 melted_column_order.insert(-2, "cell_type")
             cdf = cdf.reindex(columns=melted_column_order)
-
         return cdf
     else:
         logging.info("Empty - nothing to concatenate!")
         return pd.DataFrame()
 
 
 def create_rate_column(df, nom_cap, spec_conv_factor, column="current_avr"):
@@ -949,30 +979,29 @@
     inverse=False,
     inverted=False,
     fix_index=True,
 ):
     """Select only cycles charged or discharged with a given rate.
 
     Parameters:
-        cell (cellpy.CellpyData)
+        cell (cellpy.CellpyCell)
         rate (float): the rate to filter on. Remark that it should be given
             as a float, i.e. you will have to convert from C-rate to
             the actual numeric value. For example, use rate=0.05 if you want
             to filter on cycles that has a C/20 rate.
         on (str): only select cycles if based on the rate of this step-type (e.g. on="charge").
         rate_std (float): allow for this inaccuracy in C-rate when selecting cycles
         rate_column (str): column header name of the rate column,
         inverse (bool): select steps that do not have the given C-rate.
         inverted (bool): select cycles that do not have the steps filtered by given C-rate.
         fix_index (bool): automatically set cycle indexes as the index for the summary dataframe if not already set.
 
     Returns:
         filtered summary (Pandas.DataFrame).
     """
-
     if on is None:
         on = ["charge"]
     else:
         if not isinstance(on, (list, tuple)):
             on = [on]
 
     if rate_column is None:
@@ -985,16 +1014,16 @@
         rate = 0.05
 
     if rate_std is None:
         rate_std = 0.1 * rate
 
     cycle_number_header = hdr_summary["cycle_index"]
 
-    step_table = cell.cell.steps
-    summary = cell.cell.summary
+    step_table = cell.data.steps
+    summary = cell.data.summary
 
     if summary.index.name != cycle_number_header:
         warnings.warn(
             f"{cycle_number_header} not set as index\n"
             f"Current index :: {summary.index}\n"
         )
 
@@ -1035,15 +1064,15 @@
 
 def add_normalized_capacity(
     cell, norm_cycles=None, individual_normalization=False, scale=1.0
 ):
     """Add normalized capacity to the summary.
 
     Args:
-        cell (CellpyData): cell to add normalized capacity to.
+        cell (CellpyCell): cell to add normalized capacity to.
         norm_cycles (list of ints): the cycles that will be used to find
             the normalization factor from (averaging their capacity)
         individual_normalization (bool): find normalization factor for both
             the charge and the discharge if true, else use normalization factor
             from charge on both charge and discharge.
         scale (float): scale of normalization (default is 1.0).
 
@@ -1057,34 +1086,34 @@
 
     col_name_charge = hdr_summary["charge_capacity"]
     col_name_discharge = hdr_summary["discharge_capacity"]
     col_name_norm_charge = hdr_summary["normalized_charge_capacity"]
     col_name_norm_discharge = hdr_summary["normalized_discharge_capacity"]
 
     try:
-        norm_val_charge = cell.cell.summary.loc[norm_cycles, col_name_charge].mean()
+        norm_val_charge = cell.data.summary.loc[norm_cycles, col_name_charge].mean()
     except KeyError as e:
         print(f"Oh no! Are you sure these cycle indexes exist?")
         print(f"  norm_cycles: {norm_cycles}")
-        print(f"  cycle indexes: {list(cell.cell.summary.index)}")
+        print(f"  cycle indexes: {list(cell.data.summary.index)}")
         raise KeyError from e
     if individual_normalization:
-        norm_val_discharge = cell.cell.summary.loc[
+        norm_val_discharge = cell.data.summary.loc[
             norm_cycles, col_name_discharge
         ].mean()
     else:
         norm_val_discharge = norm_val_charge
 
     for col_name, norm_col_name, norm_value in zip(
         [col_name_charge, col_name_discharge],
         [col_name_norm_charge, col_name_norm_discharge],
         [norm_val_charge, norm_val_discharge],
     ):
-        cell.cell.summary[norm_col_name] = (
-            scale * cell.cell.summary[col_name] / norm_value
+        cell.data.summary[norm_col_name] = (
+            scale * cell.data.summary[col_name] / norm_value
         )
 
     return cell
 
 
 def load_and_save_resfile(filename, outfile=None, outdir=None, mass=1.00):
     """Load a raw data file and save it as cellpy-file.
@@ -1095,15 +1124,15 @@
         outfile (str): optional, name of hdf5-file.
         filename (str): name of the resfile.
 
     Returns:
         out_file_name (str): name of saved file.
     """
     warnings.warn(DeprecationWarning("This option will be removed in v.0.4.0"))
-    d = CellpyData()
+    d = CellpyCell()
 
     if not outdir:
         outdir = prms.Paths.cellpydatadir
 
     if not outfile:
         outfile = os.path.basename(filename).split(".")[0] + ".h5"
         outfile = os.path.join(outdir, outfile)
```

### Comparing `cellpy-0.4.3a3/cellpy/utils/ica.py` & `cellpy-1.0.0a0/cellpy/utils/ica.py`

 * *Files 2% similar despite different names*

```diff
@@ -63,15 +63,14 @@
         savgol_filter_window_order=3,
         voltage_fwhm=0.01,
         gaussian_order=0,
         gaussian_mode="reflect",
         gaussian_cval=0.0,
         gaussian_truncate=4.0,
     ):
-
         self.capacity = capacity
         self.voltage = voltage
 
         self.capacity_preprocessed = None
         self.voltage_preprocessed = None
         self.capacity_inverted = None
         self.voltage_inverted = None
@@ -176,15 +175,14 @@
             d_capacity = np.diff(capacity)
             d_voltage = np.diff(voltage)
 
             self.d_capacity_mean = np.mean(d_capacity)
             self.d_voltage_mean = np.mean(d_voltage)
 
         if err_est:
-
             splits = int(self.number_of_points / self.points_pr_split)
             rest = self.number_of_points % self.points_pr_split
 
             if splits < self.minimum_splits:
                 txt = "no point in splitting, too little data"
                 logging.debug(txt)
                 self.errors.append("splitting: to few points")
@@ -327,14 +325,17 @@
             self._voltage_processed = self.voltage_inverted[1:]
             self.voltage_processed = (
                 self.voltage_inverted[1:] - 0.5 * self.voltage_inverted_step
             )
 
         elif self.increment_method == "hist":
             logging.debug(" - diff using HIST")
+            logging.warning(
+                "Using the 'hist' method has not been thoroughly tested yet"
+            )
             # raise NotImplementedError
 
             df = pd.DataFrame(
                 {"Capacity": self.capacity_inverted, "Voltage": self.voltage_inverted}
             )
             df["dQ"] = df.Capacity.diff()
             df["Voltage"] = df.Voltage.round(decimals=4)
@@ -415,24 +416,25 @@
     """Returns tuple with first and last item."""
     if isinstance(x, (pd.DataFrame, pd.Series)):
         return x.iloc[0], x.iloc[-1]
     else:
         return x[0], x[-1]
 
 
-def dqdv_cycle(cycle, splitter=True, **kwargs):
+def dqdv_cycle(cycle, splitter=True, label_direction=False, **kwargs):
     """Convenience functions for creating dq-dv data from given capacity and
     voltage cycle.
 
     Returns the DataFrame with a 'voltage' and a 'incremental_capacity'
     column.
 
     Args:
         cycle (pandas.DataFrame): the cycle data ('voltage', 'capacity', 'direction' (1 or -1)).
         splitter (bool): insert a np.NaN row between charge and discharge.
+        label_direction (bool):
 
     Returns:
         List of step numbers corresponding to the selected steptype.
         Returns a ``pandas.DataFrame`` instead of a list if ``pdtype`` is set to ``True``.
 
     Additional key-word arguments are sent to Converter:
 
@@ -472,57 +474,67 @@
     if cycle.empty:
         raise NullData(f"The cycle (type={type(cycle)}) is empty.")
 
     c_first = cycle.loc[cycle["direction"] == -1]
     c_last = cycle.loc[cycle["direction"] == 1]
 
     converter = Converter(**kwargs)
+
     converter.set_data(c_first["capacity"], c_first["voltage"])
     converter.inspect_data()
     converter.pre_process_data()
     converter.increment_data()
     converter.post_process_data()
     voltage_first = converter.voltage_processed
     incremental_capacity_first = converter.incremental_capacity
 
     if splitter:
         voltage_first = np.append(voltage_first, np.NaN)
         incremental_capacity_first = np.append(incremental_capacity_first, np.NaN)
 
     converter = Converter(**kwargs)
+
     converter.set_data(c_last["capacity"], c_last["voltage"])
     converter.inspect_data()
     converter.pre_process_data()
     converter.increment_data()
     converter.post_process_data()
     voltage_last = converter.voltage_processed[::-1]
     incremental_capacity_last = converter.incremental_capacity[::-1]
+
     voltage = np.concatenate((voltage_first, voltage_last))
     incremental_capacity = np.concatenate(
         (incremental_capacity_first, incremental_capacity_last)
     )
 
+    if label_direction:
+        direction_first = -np.ones(len(voltage_first))
+        direction_last = np.ones(len(voltage_last))
+        direction = np.concatenate((direction_first, direction_last))
+        return voltage, incremental_capacity, direction
+
     return voltage, incremental_capacity
 
 
-def dqdv_cycles(cycles, not_merged=False, **kwargs):
+def dqdv_cycles(cycles, not_merged=False, label_direction=False, **kwargs):
     """Convenience functions for creating dq-dv data from given capacity and
     voltage cycles.
 
     Returns a DataFrame with a 'voltage' and a 'incremental_capacity'
     column.
 
     Args:
         cycles (pandas.DataFrame): the cycle data ('cycle', 'voltage',
              'capacity', 'direction' (1 or -1)).
         not_merged (bool): return list of frames instead of concatenating (
             defaults to False).
+        label_direction (bool): include 'direction' (1 or -1).
 
     Returns:
-        ``pandas.DataFrame`` with columns 'cycle', 'voltage', 'dq'.
+        ``pandas.DataFrame`` with columns 'cycle', 'voltage', 'dq' (and 'direction' if label_direction is True).
 
     Additional key-word arguments are sent to Converter:
 
     Keyword Args:
         points_pr_split (int): only used when investigating data using
             splits, defaults to 10.
         max_points: None
@@ -562,27 +574,42 @@
 
     # TODO: should add option for normalising based on first cycle capacity
     # this is e.g. done by first finding the first cycle capacity (nom_cap)
     # (or use nominal capacity given as input) and then propagating this to
     # Converter using the key-word arguments
     #   normalize=True, normalization_factor=1.0, normalization_roof=nom_cap
 
+    if len(cycles) < 1:
+        logging.debug("The food was without nutrition")
+        return pd.DataFrame()
+
     ica_dfs = list()
     cycle_group = cycles.groupby("cycle")
     keys = list()
     for cycle_number, cycle in cycle_group:
         cycle = cycle.dropna()
-        v, dq = dqdv_cycle(cycle, splitter=True, **kwargs)
-        _ica_df = pd.DataFrame({"voltage": v, "dq": dq})
+        if label_direction:
+            v, dq, direction = dqdv_cycle(
+                cycle, splitter=True, label_direction=True, **kwargs
+            )
+            _d = {"voltage": v, "dq": dq, "direction": direction}
+            _cols = ["voltage", "dq", "direction"]
+        else:
+            v, dq = dqdv_cycle(cycle, splitter=True, label_direction=False, **kwargs)
+            _d = {"voltage": v, "dq": dq}
+            _cols = ["voltage", "dq"]
+
+        _ica_df = pd.DataFrame(_d)
         if not not_merged:
+            _cols.insert(0, "cycle")
             _ica_df["cycle"] = cycle_number
-            _ica_df = _ica_df[["cycle", "voltage", "dq"]]
+            _ica_df = _ica_df[_cols]
         else:
             keys.append(cycle_number)
-            _ica_df = _ica_df[["voltage", "dq"]]
+            _ica_df = _ica_df[_cols]
         ica_dfs.append(_ica_df)
 
     if not_merged:
         return keys, ica_dfs
 
     ica_df = pd.concat(ica_dfs)
     return ica_df
@@ -712,19 +739,19 @@
     converter.pre_process_data()
     converter.increment_data()
     converter.post_process_data()
 
     return converter.voltage_processed, converter.incremental_capacity
 
 
-def dqdv_frames(cell, split=False, tidy=True, **kwargs):
+def dqdv_frames(cell, split=False, tidy=True, label_direction=False, **kwargs):
     """Returns dqdv data as pandas.DataFrame(s) for all cycles.
 
     Args:
-        cell (CellpyData-object).
+        cell (CellpyCell-object).
         split (bool): return one frame for charge and one for
             discharge if True (defaults to False).
         tidy (bool): returns the split frames in wide format (defaults
             to True. Remark that this option is currently not available
             for non-split frames).
 
     Returns:
@@ -777,15 +804,17 @@
     # (or use nominal capacity given as input) and then propagating this to
     # Converter using the key-word arguments
     #   normalize=True, normalization_factor=1.0, normalization_roof=nom_cap
 
     if split:
         return _dqdv_split_frames(cell, tidy=tidy, **kwargs)
     else:
-        return _dqdv_combinded_frame(cell, tidy=tidy, **kwargs)
+        return _dqdv_combinded_frame(
+            cell, tidy=tidy, label_direction=label_direction, **kwargs
+        )
 
 
 def _constrained_dq_dv_using_dataframes(capacity, minimum_v, maximum_v, **kwargs):
     converter = Converter(**kwargs)
     converter.set_data(capacity)
     converter.inspect_data()
     converter.pre_process_data()
@@ -821,35 +850,39 @@
             # d.name = f"{cycle}"
             d.name = n
             incremental_charge_list.append(d)
 
     return incremental_charge_list
 
 
-def _dqdv_combinded_frame(cell, tidy=True, **kwargs):
+def _dqdv_combinded_frame(cell, tidy=True, label_direction=False, **kwargs):
     """Returns full cycle dqdv data for all cycles as one pd.DataFrame.
 
     Args:
-        cell: CellpyData-object
+        cell: CellpyCell-object
 
     Returns:
         pandas.DataFrame with the following columns:
             cycle: cycle number
             voltage: voltage
             dq: the incremental capacity
     """
     cycle = kwargs.pop("cycle", None)
+    number_of_points = kwargs.pop("number_of_points", None)
     cycles = cell.get_cap(
         cycle=cycle,
         method="forth-and-forth",
         categorical_column=True,
         label_cycle_number=True,
         insert_nan=False,
+        number_of_points=number_of_points,
+    )
+    ica_df = dqdv_cycles(
+        cycles, not_merged=not tidy, label_direction=label_direction, **kwargs
     )
-    ica_df = dqdv_cycles(cycles, not_merged=not tidy, **kwargs)
 
     if not tidy:
         # dqdv_cycles returns a list of cycle numbers and a list of DataFrames
         # if not_merged is set to True (or not False)
         keys, ica_df = ica_df
         ica_df = pd.concat(ica_df, axis=1, keys=keys)
         return ica_df
@@ -866,15 +899,15 @@
     steptable=None,
     max_cycle_number=None,
     **kwargs,
 ):
     """Returns dqdv data as pandas.DataFrames for all cycles.
 
     Args:
-        cell (CellpyData-object).
+        cell (CellpyCell-object).
         tidy (bool): return in wide format if False (default),
             long (tidy) format if True.
 
     Returns:
         (charge_ica_frame, discharge_ica_frame) where the frames are
         pandas.DataFrames where the first column is voltage ('v') and
         the following columns are the incremental capcaity for each
@@ -1021,15 +1054,15 @@
     )
     # test_data_dir_out = os.path.join(test_data_dir, "out")
     test_cellpy_file = "20160805_test001_45_cc.h5"
     test_cellpy_file_full = os.path.join(test_data_dir, test_cellpy_file)
     # mass = 0.078609164
 
     # ---------- loading test-data ----------------------
-    cell = cellreader.CellpyData()
+    cell = cellreader.CellpyCell()
     cell.load(test_cellpy_file_full)
     list_of_cycles = cell.get_cycle_numbers()
     number_of_cycles = len(list_of_cycles)
     print("you have %i cycles" % number_of_cycles)
     # cell.save(test_cellpy_file_full)
     return cell
```

### Comparing `cellpy-0.4.3a3/cellpy/utils/plotutils.py` & `cellpy-1.0.0a0/cellpy/utils/plotutils.py`

 * *Files 4% similar despite different names*

```diff
@@ -149,14 +149,17 @@
 hdr_summary = get_headers_summary()
 hdr_raw = get_headers_normal()
 hdr_steps = get_headers_step_table()
 hdr_journal = get_headers_journal()
 
 
 def _hv_bokeh_available():
+    warnings.warn(
+        "This utility function will be removed shortly", category=DeprecationWarning
+    )
     if not hv_available:
         print("You need holoviews. But I cannot load it. Aborting...")
         return False
     if not bokeh_available:
         print("You need Bokeh. But I cannot find it. Aborting...")
         return False
     return True
@@ -169,14 +172,19 @@
         columns: pandas columns
         label: if not provided, generate, if provided, return as is
         end: the string to use for searching
 
     Returns:
         column header, label
     """
+
+    warnings.warn(
+        "This utility function will be removed shortly", category=DeprecationWarning
+    )
+    # TODO @jepe: refactor and use col names directly from HeadersNormal instead
     hdr = None
     lab = None
     for col in columns:
         if col.endswith(end):
             hdr = col
             if label is None:
                 lab = col.replace("_", " ")
@@ -184,14 +192,15 @@
                 lab = label
             break
     return hdr, lab
 
 
 def plot_concatenated(
     dataframe,
+    title="",
     x=None,
     y=None,
     err=None,
     xlabel=None,
     ylabel=None,
     points=True,
     line=True,
@@ -218,37 +227,38 @@
 ):
     """Create a holoviews plot of the concatenated summary.
 
     This function is still under development. Feel free to contribute.
 
     Args:
         dataframe: the concatenated summary
+        title (str): title of the plot (defaults to empty)
         x: colum-name for the x variable (not implemented yet)
         y: colum-name for the y variable (not implemented yet)
         err: colum-name for the std variable (not implemented yet)
         xlabel: label for x-axis
         ylabel: label for y-axis
         points (bool): plot points if True
         line (bool): plot line if True
         errors (bool): plot errors if True
         hover (bool): add hover tool if True
         width: width of plot
         height: height of plot
-        journal: batch.journal object
+        journal: `batch.journal` object
         file_id_level: the level (multiindex-level) where the cell-names are.
         hdr_level:  the level (multiindex-level) where the parameter names are.
         axis: what axis to use when looking in the data-frame (row-based or col-based).
         mean_end: used for searching for y-column names
         std_end: used for searching for e-column names
         cycle_end: used for searching for x-column name
-        legend_title: title to put over the legends
+        legend_title: title to put over the legend
         marker_size: size of the markers used
         cmap: color-map to use
         spread (bool): plot error-bands instead of error-bars if True
-        extension (str): "matplotlib" or "bokeh". Note, this uses hv.extension) and will affect the
+        extension (str): "matplotlib" or "bokeh". Note, this uses `hv.extension`) and will affect the
             state of your notebook
         edges (bool): show all axes
         keys (dict): columns to plot (not working yet)
         simple (bool): making a simple hv.Overlay instead of an hv.NdOverlay if True
         **kwargs: key-word arguments sent to hv.NdOverlay
 
     Example:
@@ -273,14 +283,19 @@
         >>> hv.extension("matplotlib")
         >>> my_plot.opts(aspect="auto", fig_inches=(12,7), fig_size=90, legend_position="top_right",
         >>>              legend_cols = 2,
         >>>              show_frame=True)
 
     """
     # TODO: add option for using labels from journal in the legend
+    # TODO @jepe: refactor and use col names directly from HeadersNormal instead
+
+    warnings.warn(
+        "This utility function will be removed shortly", category=DeprecationWarning
+    )
 
     if keys is None:
         keys = dict()
 
     if not hv_available:
         print(
             "This function uses holoviews. But could not import it."
@@ -372,15 +387,14 @@
             )
 
         else:
             curve = hv.Curve(group, (hdr_x, lab_x), (hdr_y, lab_y), label=label)
 
         if points:
             if not averaged and journal is not None:
-
                 scatter = hv.Scatter(curve).opts(color=color, marker=marker)
 
                 if edges and extension == "matplotlib":
                     scatter = scatter.opts(edgecolor="k")
 
                 if edges and extension == "bokeh":
                     scatter = scatter.opts(line_color="k", line_width=1)
@@ -424,14 +438,17 @@
                 new_curve_dict[k] = curve_dict[keys[k]]
             curve_dict = new_curve_dict
 
         final_plot = hv.Overlay(
             [*curve_dict.values()], vdims=[*curve_dict.keys()]
         ).opts(**overlay_opts, **kwargs)
     else:
+        overlay_opts["title"] = title
+        logging.info(f"overlay_opts: {overlay_opts}")
+        logging.info(f"additional_kwargs_overlay_opts: {kwargs}")
         final_plot = hv.NdOverlay(curve_dict, kdims=legend_title).opts(
             **overlay_opts, **kwargs
         )
 
     if hover and not extension == "plotly":
         if points:
             final_plot.opts(opts.Scatter(tools=["hover"]))
@@ -486,15 +503,14 @@
         _color_list.append(next(color_cycler))
     for i in sub_groups:
         _symbol_list.append(next(symbol_cycler))
     return _color_list, _symbol_list
 
 
 def _raw_plot(raw_curve, title="Voltage versus time", **kwargs):
-
     tgt = raw_curve.relabel(title).opts(
         width=800,
         height=300,
         labelled=["y"],
         # tools=["pan","box_zoom", "reset"],
         active_tools=["pan"],
     )
@@ -506,27 +522,31 @@
     layout.opts(opts.Layout(shared_axes=False, merge_tools=False))
     return layout
 
 
 def raw_plot(cell, y=("voltage", "Voltage (V vs Li/Li+)"), title=None, **kwargs):
     # TODO: missing doc-string
 
+    warnings.warn(
+        "This utility function will be replaced shortly", category=DeprecationWarning
+    )
+
     if title is None:
         if isinstance(y, (list, tuple)):
             pre_title = str(y[0])
         else:
             pre_title = str(y)
         title = " ".join([pre_title, "versus", "time"])
 
     if not _hv_bokeh_available():
         return
 
     hv.extension("bokeh", logo=False)
 
-    raw = cell.cell.raw
+    raw = cell.data.raw
     raw["test_time_hrs"] = raw[hdr_raw["test_time_txt"]] / 3600
     x = ("test_time_hrs", "Time (hours)")
     raw_curve = hv.Curve(raw, x, y)
     layout = _raw_plot(raw_curve, title=title, **kwargs)
     return layout
 
 
@@ -572,14 +592,19 @@
         use_bokeh (bool): use bokeh to plot (defaults to True). If not, use matplotlib.
         **kwargs: parameters specific to either matplotlib or bokeh.
 
     Returns:
         ``matplotlib.axes`` or None
     """
     # TODO: missing doc-string
+
+    warnings.warn(
+        "This utility function will be replaced shortly", category=DeprecationWarning
+    )
+
     if use_bokeh and not bokeh_available:
         print("OBS! bokeh is not available - using matplotlib instead")
         use_bokeh = False
 
     if use_bokeh:
         axes = _cycle_info_plot_bokeh(
             cell,
@@ -718,15 +743,15 @@
 
     if title is None:
         title = f"{t_y} vs. {t_x}"
 
     cols = [x, y]
     cols.extend([h_cycle, h_step])
 
-    df = cell.cell.raw.loc[:, cols]
+    df = cell.data.raw.loc[:, cols]
 
     if cycle is not None:
         if not isinstance(cycle, (list, tuple)):
             cycle = [cycle]
 
         _df = df.loc[df[h_cycle].isin(cycle), :]
         if len(cycle) < 5:
@@ -756,15 +781,15 @@
         else:
             df = _df
 
     x_min, x_max = df[x].min(), df[x].max()
     y_min, y_max = df[y].min(), df[y].max()
 
     if info_level > 0:
-        table = cell.cell.steps
+        table = cell.data.steps
         df = _add_step_info_cols(df, table, cycle, step)
 
     source = ColumnDataSource(df)
 
     plot = figure(
         title=title,
         tools="pan,reset,save,wheel_zoom,box_zoom,undo,redo",
@@ -913,25 +938,24 @@
     if show_it:
         show(plot)
 
     return plot
 
 
 def _cycle_info_plot_matplotlib(cell, cycle, get_axes=False):
-
     # obs! hard-coded col-names. Please fix me.
     if not plt_available:
         print(
             "This function uses matplotlib. But I could not import it. "
             "So I decided to abort..."
         )
         return
 
-    data = cell.cell.raw
-    table = cell.cell.steps
+    data = cell.data.raw
+    table = cell.data.steps
 
     span_colors = ["#4682B4", "#FFA07A"]
 
     voltage_color = "#008B8B"
     current_color = "#CD5C5C"
 
     m_cycle_data = data.cycle_index == cycle
@@ -1010,14 +1034,18 @@
         file_name (str): the file name
         wide (bool): release the equal aspect lock (default on for holoviews)
         size (int or tuple of ints): figure size in inches
         dpi (int): resolution
         **kwargs: sent to cellpy.utils.plotutils.hv_bokeh_to_mpl
     """
 
+    warnings.warn(
+        "This utility function will be removed shortly", category=DeprecationWarning
+    )
+
     out_path = Path("out/")
     extension = "png"
 
     if size is None:
         size = (6, 6)
 
     # create file name:
@@ -1062,14 +1090,17 @@
         return
     print(f"saved to {file_name}")
 
 
 def hv_bokeh_to_mpl(figure, wide=False, size=(6, 4), **kwargs):
     # I think this needs to be tackled differently. For example by setting hv.extension("matplotlib") and
     # re-making the figure. Or making a custom renderer.
+    warnings.warn(
+        "This utility function will be removed shortly", category=DeprecationWarning
+    )
     figure = hv.render(figure, backend="matplotlib")
     axes = figure.axes
     number_of_axes = len(axes)
     if number_of_axes > 1:
         for j, ax in enumerate(axes):
             if j < number_of_axes - 1:
                 ax.set_xlabel("")
@@ -1096,45 +1127,57 @@
     cap_ylim=None,
     ce_ylim=None,
     ir_ylim=None,
     simple=False,
     group_it=False,
     spread=True,
     capacity_unit="gravimetric",
+    capacity_unit_label=None,
+    internal_resistance_unit="Ohm",
     **kwargs,
 ):
     """create a holoviews-plot containing Coulombic Efficiency, Capacity, and IR.
 
     Args:
         b (cellpy.batch object): the batch with the cells.
         cap_ylim (tuple of two floats): scaling of y-axis for capacity plots.
         ce_ylim (tuple of two floats): scaling of y-axis for c.e. plots.
         ir_ylim (tuple of two floats): scaling of y-axis for i.r. plots.
         simple (bool): if True, use hv.Overlay instead of hv.NdOverlay.
         group_it (bool): if True, average pr group.
-        spread (bool): if True, show spread instead of error-bars
-        capacity_unit (str): select "gravimetric", or "areal"
+        spread (bool): if True, show spread instead of error-bars.
+        capacity_unit (str): select "gravimetric", or "areal".
+        capacity_unit_label (str): shown in the plot title for the capacity plot
+            (defaults to mAh/g(a.m.) for gravimetric and mAh/cm2 for areal).
+        internal_resistance_unit (str): shown in the plot title for the ir plots (defaults to Ohm).
+
+    **kwargs:
+        Sent to plotutils.bplot.
 
     Returns:
         ``hv.Overlay`` or ``hv.NdOverlay``
     """
 
+    warnings.warn(
+        "This utility function will be removed shortly", category=DeprecationWarning
+    )
+
     extension = kwargs.pop("extension", "bokeh")
 
     cap_colum_dict = {
         "gravimetric": {
-            "discharge": "discharge_capacity",
-            "charge": "charge_capacity",
-            "unit": "mAh/g(a.m.)",
+            "discharge": "discharge_capacity_gravimetric",
+            "charge": "charge_capacity_gravimetric",
+            "unit": capacity_unit_label or "mAh/g(a.m.)",
             "ylim": (0, 5000),
         },
         "areal": {
-            "discharge": "areal_discharge_capacity",
-            "charge": "areal_charge_capacity",
-            "unit": "mAh/cm2",
+            "discharge": "discharge_capacity_areal",
+            "charge": "charge_capacity_areal",
+            "unit": capacity_unit_label or "mAh/cm2",
             "ylim": (0, 3),
         },
     }
 
     if cap_ylim is not None:
         cap_colum_dict[capacity_unit]["ylim"] = cap_ylim
 
@@ -1244,15 +1287,18 @@
         overlay_opts(
             title="",
             show_legend=False,
             xlabel="",
             ylabel="discharge",
             **overlay_sensitive_opts["ird"],
         ),
-        layout_opts(title="Internal Resistance (Ohm)", **layout_sensitive_opts["ird"]),
+        layout_opts(
+            title=f"Internal Resistance ({internal_resistance_unit})",
+            **layout_sensitive_opts["ird"],
+        ),
     )
 
     oplot_irc = bplot(b, columns=["ir_charge"], **bplot_shared_opts, **kwargs).opts(
         hv.opts.Curve(ylim=ir_ylim),
         overlay_opts(
             title="",
             show_legend=False,
@@ -1266,32 +1312,32 @@
 
 
 def bplot(b, individual=False, cols=1, **kwargs):
     """plot batch summaries.
 
     This is wrapper around the two functions concatenate_summaries and plot_concatenated.
 
-    >>> p1 = bplot(b, columns=["charge_capacity"], journal=b.experiment.journal, group_it=True)
+    >>> p1 = bplot(b, columns=["charge_capacity_gravimetric"], journal=b.experiment.journal, group_it=True)
 
     is equivalent to:
 
-    >>> cs = helpers.concatenate_summaries(b, columns=["charge_capacity"], group_it=True)
+    >>> cs = helpers.concatenate_summaries(b, columns=["charge_capacity_gravimetric"], group_it=True)
     >>> p1 = plot_concatenated(cs, journal=journal)
 
     Args:
         b (cellpy.batch object): the batch with the cells.
         individual (bool): in case of multiple columns, return a list of plots instaed of a hv.Layout
         cols (int): number of columns.
 
     Key-word arguments sent further to the concatenator:
 
     Keyword Args:
         rate (float): filter on rate (C-rate)
         on (str or list of str): only select cycles if based on the rate of this step-type (e.g. on="charge").
-        columns (list): selected column(s) (using cellpy name) [defaults to "charge_capacity"]
+        columns (list): selected column(s) (using cellpy attribute name) [defaults to "charge_capacity_gravimetric"]
         column_names (list): selected column(s) (using exact column name)
         normalize_capacity_on (list): list of cycle numbers that will be used for setting the basis of the normalization
             (typically the first few cycles after formation)
         scale_by (float or str): scale the normalized data with nominal capacity if "nom_cap", or given value (defaults to one).
         nom_cap (float): nominal capacity of the cell
         normalize_cycles (bool): perform a normalisation of the cycle numbers (also called equivalent cycle index)
         add_areal (bool):  add areal capacity to the summary
@@ -1304,24 +1350,33 @@
 
     Key-word arguments sent further to the plotter:
 
     Keyword Args:
         width (int): width of plot.
         spread (bool): use error-spread instead of error-bars.
         simple (bool): use ``hv.Overlay`` instead of ``hv.NdOverlay``.
+        extension (str): plotting backend.
 
     Returns:
         ``holoviews`` plot
     """
+    warnings.warn(
+        "This utility function will be removed shortly", category=DeprecationWarning
+    )
+
     width = kwargs.pop("width", 800)
     journal = kwargs.pop("journal", b.experiment.journal)
     spread = kwargs.pop("spread", True)
     simple = kwargs.pop("simple", False)
-    columns = kwargs.pop("columns", ["charge_capacity"])
+    columns = kwargs.pop("columns", ["charge_capacity_gravimetric"])
     extension = kwargs.pop("extension", "bokeh")
+    if extension != "bokeh":
+        logging.critical(
+            f"Setting extension to {extension}. Remark that this will globally change the hv settings."
+        )
     p = collections.OrderedDict()
     i_width = width // cols
     for i, col in enumerate(columns):
         try:
             cs = helpers.concatenate_summaries(b, columns=[col], **kwargs)
             _p = plot_concatenated(
                 cs,
```

### Comparing `cellpy-0.4.3a3/cellpy.egg-info/PKG-INFO` & `cellpy-1.0.0a0/cellpy.egg-info/PKG-INFO`

 * *Files 19% similar despite different names*

```diff
@@ -1,32 +1,36 @@
 Metadata-Version: 2.1
 Name: cellpy
-Version: 0.4.3a3
-Summary: Extract and manipulate data from battery cell testers.
+Version: 1.0.0a0
+Summary: Extract and manipulate data from battery data testers.
 Home-page: https://github.com/jepegit/cellpy
 Author: Jan Petter Maehlen
 Author-email: jepe@ife.no
 License: MIT license
 Keywords: cellpy
-Classifier: Development Status :: 3 - Alpha
+Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Natural Language :: English
-Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
 Provides-Extra: batch
 Provides-Extra: fit
 Provides-Extra: all
 License-File: LICENSE
 License-File: AUTHORS.rst
 
-===============================
-cellpy
-===============================
+.. image:: https://github.com/jepegit/cellpy/blob/master/docs/_static/cellpy-icon-long.svg
+  :height: 100
+  :alt: cellpy
+
+===================================================================
+cellpy - *a library for assisting in analysing batteries and cells*
+===================================================================
 
 
 .. image:: https://img.shields.io/pypi/v/cellpy.svg
         :target: https://pypi.python.org/pypi/cellpy
 
 .. image:: https://img.shields.io/travis/jepegit/cellpy.svg
         :target: https://travis-ci.org/jepegit/cellpy
@@ -40,177 +44,198 @@
 
 
 This Python Package was developed to help the
 researchers at IFE, Norway, in their cumbersome task of
 interpreting and handling data from cycling tests of
 batteries and cells.
 
+
 Documentation
--------------
+=============
 
 The documentation for ``cellpy`` is hosted on `Read the docs
 <https://cellpy.readthedocs.io>`_.
 
 
 Installation and dependencies
------------------------------
+=============================
 
 The easiest way to install ``cellpy`` is to install with conda or pip.
 
 With conda::
 
-   conda install cellpy --channel conda-forge
+   conda install -c conda-forge cellpy
 
 Or if you prefer installing using pip::
 
-   pip install cellpy
+   python -m pip install cellpy
 
 Have a look at the documentation for more detailed installation procedures, especially
 with respect to "difficult" dependencies when installing with pip.
 
 Licence
--------
+=======
 
 ``cellpy`` is free software made available under the MIT License.
 
 Features
---------
+========
 
-* Load test-data and store in hdf5 format.
+* Load test-data and store in a common format.
+* Summarize and compare data.
 * Filter out the steps of interest.
 * Process and plot the data.
 * And more...
 
 
 
 
 =======
 History
 =======
 
+
+1.0.0 (2023) [under development]
+================================
+
+* Unit handling: renaming summary headers
+* Unit handling: new cellpy-file-format version
+* Unit handling: tool for converting old to new format
+* Templates: using one repository with sub-folders
+* Templates: adding more documentation
+* File handling: allow for external raw files (ssh)
+* Readers: neware.txt (one version/model)
+* Readers: arbin_sql7 (experimental, @jtgibson91)
+* Batch plotting: collectors for both data collection, plotting and saving
+* Internals: rename main classes (CellpyData -> CellpyCell, Cell -> Data)
+* Internals: rename .cell property to .data
+* Internals: allow for only one Data-object pr CellpyCell object
+
+
 0.4.3 (2023)
-------------
+============
 
 * Neware txt loader (supports one specific format only, other formats will have to wait for v.1.0)
 
 
 0.4.2 (2022)
-------------
+============
 
 * Changed definition of Coulombic Difference (negative of previous)
 * Updated loaders with hooks and additional base class TxtLoader with configuration mechanism
 * Support for Maccor txt files
 * Supports only python 3.8 and up
 * Optional parameters through batch and pages
 * Several bug fixes and minor improvements / adjustments
 * Restrict use of instrument label to only one option
+* Fix bug in example file (@kevinsmia1939)
 
 
 0.4.1 (2021)
-------------
+============
 
 * Updated documentations
 * CLI improvements
 * New argument for get_cap: max_cycle
 * Reverting from using Documents to user home for location of prm file in windows.
 * Easyplot by Amund
 * Arbin sql reader by Muhammad
 
 
 0.4.0 (2020)
-------------
+============
 
 * Reading arbin .res files with auxiliary data should now work.
 * Many bugs have been removed - many new introduced.
 * Now on conda-forge (can be installed using conda).
 
 
 0.4.0 a2 (2020)
----------------
+===============
 
 * Reading PEC files now updated and should work
 
 
 0.4.0 a1 (2020)
----------------
+===============
 
 * New column names (lowercase and underscore)
 * New batch concatenating and plotting routines
 
 
 0.3.3 (2020)
-------------
+============
 
 * Switching from git-flow to github-flow
 * New cli options for running batches
 * cli option for creating template notebooks
 * Using ruamel.yaml instead of pyyaml
 * Using python-box > 4
 * Several bug-fixes
 
 
 0.3.2 (2019)
-------------
+============
 
 * Starting fixing documentation
 * TODO: create conda package
 * TODO: extensive tests
 
 
 0.3.1 (2019)
-------------
+============
 
 * Refactoring - renaming from dfsummary to summary
 * Refactoring - renaming from step_table to steps
 * Refactoring - renaming from dfdata to raw
-* Refactoring - renaming cellpy.cell to cellpy.get
+* Refactoring - renaming cellpy.data to cellpy.get
 * Updated save and load cellpy files allowing for new naming
 * Implemented cellpy new and cellpy serve cli functionality
 
 
 0.3.0 (2019)
-------------
+============
 
 * New batch-feature
 * Improved make-steps and make-summary functionality
 * Improved cmd-line interface for setup
 * More helper functions and tools
 * Experimental support for other instruments
 * invoke tasks for developers
 
 0.2.1 (2018)
-------------
+============
 
 * Allow for using mdbtools also on win
 * Slightly faster find_files using cache and fnmatch
 * Bug fix: error in sorting files when using pathlib fixed
 
 
 0.2.0 (2018-10-17)
-------------------
+==================
 
 * Improved creation of step tables (much faster)
 * Default compression on cellpy (hdf5) files
 * Bug fixes
 
 
 0.1.22 (2018-07-17)
--------------------
+===================
 
 * Parameters can be set by dot-notation (python-box).
 * The parameter Instruments.cell_configuration is removed.
 * Options for getting voltage curves in different formats.
 * Fixed python 3.6 issues with Read the Docs.
 * Can now also be used on posix (the user must install mdb_tools first).
 * Improved logging allowing for custom log-directory.
 
 
 0.1.21 (2018-06-09)
--------------------
+===================
 
 * No legacy python.
 
 
 0.1.0 (2016-09-26)
-------------------
+==================
 
 * First release on PyPI.
```

### Comparing `cellpy-0.4.3a3/docs/Makefile` & `cellpy-1.0.0a0/docs/Makefile`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/docs/_build/.doctrees/nbsphinx/notebooks_tutorial_get_cap_6_0.png` & `cellpy-1.0.0a0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_6_0.png`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/docs/_build/.doctrees/nbsphinx/notebooks_tutorial_get_cap_7_0.png` & `cellpy-1.0.0a0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_7_0.png`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/docs/_build/.doctrees/nbsphinx/notebooks_tutorial_get_cap_8_0.png` & `cellpy-1.0.0a0/docs/_build/.doctrees/nbsphinx/examples_and_tutorials_notebooks_tutorial_get_cap_8_0.png`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/docs/_build/_images/notebooks_tutorial_get_cap_6_0.png` & `cellpy-1.0.0a0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_6_0.png`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/docs/_build/_images/notebooks_tutorial_get_cap_7_0.png` & `cellpy-1.0.0a0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_7_0.png`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/docs/_build/_images/notebooks_tutorial_get_cap_8_0.png` & `cellpy-1.0.0a0/docs/_build/_images/examples_and_tutorials_notebooks_tutorial_get_cap_8_0.png`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/docs/_build/_images/templates_jupyterlab_001.png` & `cellpy-1.0.0a0/docs/_build/_images/templates_jupyterlab_001.png`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/docs/_build/_images/tutorials_utils_plotting_fig1.png` & `cellpy-1.0.0a0/docs/_build/_images/tutorials_utils_plotting_fig1.png`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/docs/_build/_images/tutorials_utils_plotting_fig2.png` & `cellpy-1.0.0a0/docs/_build/_images/tutorials_utils_plotting_fig2.png`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/docs/conf.py` & `cellpy-1.0.0a0/docs/conf.py`

 * *Files 2% similar despite different names*

```diff
@@ -38,14 +38,15 @@
 # Get the project root dir, which is the parent dir of this
 cwd = os.getcwd()
 project_root = os.path.dirname(cwd)
 project_prmsdir = os.path.join(project_root, r"cellpy\parameters")
 project_utils = os.path.join(project_root, r"cellpy\utils")
 project_scripts = os.path.join(project_root, r"cellpy\scripts")
 project_readers = os.path.join(project_root, r"cellpy\readers")
+project_internals = os.path.join(project_root, r"cellpy\internals")
 
 # print project_root
 # print project_prmsdir
 # print project_utils
 # print project_scripts
 # print project_readers
 
@@ -60,14 +61,15 @@
 # version is used.
 
 sys.path.insert(0, project_root)
 sys.path.insert(0, project_prmsdir)
 sys.path.insert(0, project_utils)
 sys.path.insert(0, project_scripts)
 sys.path.insert(0, project_readers)
+sys.path.insert(0, project_internals)
 
 
 # -- General configuration ---------------------------------------------
 
 # If your documentation needs a minimal Sphinx version, state it here.
 # needs_sphinx = '1.0'
 
@@ -75,14 +77,15 @@
 # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
 extensions = [
     "sphinx.ext.autodoc",
     "sphinx.ext.viewcode",
     "sphinx.ext.napoleon",
     "sphinx.ext.intersphinx",
     "nbsphinx",
+    "sphinx.ext.graphviz",
 ]
 
 # Napoleon settings
 napoleon_google_docstring = True
 napoleon_numpy_docstring = True
 napoleon_include_init_with_doc = False
 napoleon_include_private_with_doc = False
@@ -105,25 +108,30 @@
 # source_encoding = 'utf-8-sig'
 
 # The master toctree document.
 master_doc = "index"
 
 # General information about the project.
 project = "cellpy"
-copyright = "2022, Jan Petter Maehlen"
+copyright = "2023, Jan Petter Maehlen"
 
 # The version info for the project you're documenting, acts as replacement
 # for |version| and |release|, also used in various other places throughout
 # the built documents.
 #
 # The short X.Y version.
 version = version_ns["__version__"]
 # The full version, including alpha/beta/rc tags.
 release = version_ns["__version__"]
 
+# Exposing the version number to the docs (use |ProjectVersion| in the docs)
+rst_epilog = f"""
+.. |ProjectVersion| replace:: Version: {version}
+"""
+
 # The language for content autogenerated by Sphinx. Refer to documentation
 # for a list of supported languages.
 # language = None
 
 # There are two options for replacing |today|: either, you set today to
 # some non-false value, then it is used:
 # today = ''
@@ -139,25 +147,25 @@
 # default_role = None
 
 # If true, '()' will be appended to :func: etc. cross-reference text.
 # add_function_parentheses = True
 
 # If true, the current module name will be prepended to all description
 # unit titles (such as .. function::).
-# add_module_names = True
+add_module_names = True
 
 # If true, sectionauthor and moduleauthor directives will be shown in the
 # output. They are ignored by default.
 # show_authors = False
 
 # The name of the Pygments (syntax highlighting) style to use.
 pygments_style = "sphinx"
 
 # A list of ignored prefixes for module index sorting.
-# modindex_common_prefix = []
+modindex_common_prefix = ["cellpy."]
 
 # If true, keep warnings as "system message" paragraphs in the built
 # documents.
 # keep_warnings = False
 
 
 # -- Options for HTML output -------------------------------------------
@@ -192,15 +200,15 @@
 # 16x16 or 32x32 pixels large.
 # html_favicon = None
 
 # Add any paths that contain custom static files (such as style sheets)
 # here, relative to this directory. They are copied after the builtin
 # static files, so a file named "default.css" will overwrite the builtin
 # "default.css".
-html_static_path = ["_build/html/_static"]
+html_static_path = ["_build/_static"]
 
 # If not '', a 'Last updated on:' timestamp is inserted at every page
 # bottom, using the given strftime format.
 # html_last_updated_fmt = '%b %d, %Y'
 
 # If true, SmartyPants will be used to convert quotes and dashes to
 # typographically correct entities.
@@ -319,9 +327,9 @@
 
 # How to display URL addresses: 'footnote', 'no', or 'inline'.
 # texinfo_show_urls = 'footnote'
 
 # If true, do not generate a @detailmenu in the "Top" node's menu.
 # texinfo_no_detailmenu = False
 
-modindex_common_prefix = ["cellpy."]
+
 nbsphinx_kernel_name = "python3"
```

### Comparing `cellpy-0.4.3a3/docs/developers/dev_various.rst` & `cellpy-1.0.0a0/docs/developers_guide/dev_various.rst`

 * *Files 23% similar despite different names*

```diff
@@ -1,13 +1,11 @@
-=======
 Various
 =======
 
 
-
 Using ``pytest`` fixtures
 -------------------------
 
 Retrieve constants during tests
 ...............................
 
 There is a fixture in ``conftest.py`` aptly named ``parameters`` making all the variables defined in ``fdv.py`` accessible for the
@@ -44,7 +42,18 @@
 .. code-block:: batch
 
     cellpy/
         parameters/
             .cellpy_prms_default.conf
             prms.py
             internal_settings.py
+
+
+Installing `pyodbc` on Mac (no `conda`)
+---------------------------------------
+
+If you do not want to use `conda`, you might miss a couple of libraries.
+
+The easiest fix is to install `uniuxodbc` using `brew` as explained in
+`Stack Overflow #54302793 <https://stackoverflow.com/questions/54302793/
+dyld-library-not-loaded-usr-local-opt-unixodbc-lib-libodbc-2-dylib>`_.
+
```

### Comparing `cellpy-0.4.3a3/docs/formats.rst` & `cellpy-1.0.0a0/docs/main_description/formats.rst`

 * *Files 12% similar despite different names*

```diff
@@ -1,58 +1,65 @@
-.. highlight:: shell
-
 ================================
 File Formats and Data Structures
 ================================
 
 .. warning::
    This part of the documentation is currently being updated.
-   It is 99.9% trustable, but in need of better logical structure and formatting.
 
 
 The most important file formats and data structures for cellpy is
 summarized here.
 It is also possible to look into the source-code at the
 repository https://github.com/jepegit/cellpy.
 
 Data Structures
----------------
+===============
+
+CellpyCell - main structure
+---------------------------
+
+.. graphviz::
+
+   digraph {
+      "CellpyCell" -> "Data";
+      "CellpyCell" -> "session metadata";
+      "CellpyCell" -> "cellpy metadata";
+      "CellpyCell" -> "methods";
+   }
 
-CellpyData - main structure
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 This class is the main work-horse for cellpy where all the functions
 for reading, selecting, and tweaking your data is located.
 It also contains the header definitions, both for the cellpy hdf5
 format, and for the various cell-tester file-formats that can be read.
 The class can contain several tests and each test is stored in a list.
 
 The class contains several attributes that can be assigned directly:
 
 .. code-block:: python
 
-    CellpyData.tester = "arbin_res"
-    CellpyData.auto_dirs = True
-    print(CellpyData.cellpy_datadir)
+    CellpyCell.tester = "arbin_res"  # TODO - update this
+    CellpyCell.auto_dirs = True
+    print(CellpyCell.cellpy_datadir)
 
 
 The data for the experiment(s)/runs(s) are stored in the class attribute
-``CellpyData.cells``
+``CellpyCell.cells``
 This attribute is just a list of runs (each run is a
-``cellpy.cellreader.Cell`` instance).
-This implies that you can store many runs in one ``CellpyData`` instance.
+``cellpy.cellreader.Data`` instance).
+This implies that you can store many runs in one ``CellpyCell`` instance.
 Sometimes this can be necessary, but it is recommended to only store one
 run in one instance. Most of the functions (the class methods) automatically
-selects the 0-th item in ``CellpyData.cells`` if the ``test_number`` is not
+selects the 0-th item in ``CellpyCell.cells`` if the ``test_number`` is not
 explicitly given.
 
 You may already have figured it out: in cellpy, data for a given cell
-is usually named a run. And each run is a ``cellpy.cellreader.Cell`` instance.
+is usually named a run. And each run is a ``cellpy.cellreader.Data`` instance.
 
-Here is a list of other important class attributes in ``CellpyData``:
+Here is a list of other important class attributes in ``CellpyCell``:
 
 Column headings
 ...............
 ``cellpy`` uses ``pandas.DataFrame`` objects internally. The column headers
 of the dataframes are defined in corresponding dataclass objects that can be
 accessed using both dot-notation and through normal dictionary look-up.
 
@@ -202,15 +209,15 @@
         raw_file_names: str = "raw_file_names"
         cellpy_file_name: str = "cellpy_file_name"
         group: str = "group"
         sub_group: str = "sub_group"
         comment: str = "comment"
 
 
-    CellpyData.keys_journal_session = ["starred", "bad_cells", "bad_cycles", "notes"]
+    CellpyCell.keys_journal_session = ["starred", "bad_cells", "bad_cycles", "notes"]
 
 step types
 ..........
 
 Identifiers for the different steps have pre-defined names given in the
 class attribute list `list_of_step_types` and is written to the "step" column.
 
@@ -223,15 +230,15 @@
                           'rest', 'not_known']
 
 
 For each type of testers that are supported by ``cellpy``,
 a set of column headings and
 other different settings/attributes must be provided. These definitions stored in the
 ``cellpy.parameters.internal_settings`` module and are also injected into
-the CellpyData class upon initiation.
+the CellpyCell class upon initiation.
 
 Supported testers are:
 
 * arbin (.res type files)
 
 Testers that are partly supported (but not tested very well) are:
 
@@ -245,15 +252,15 @@
 * maccor
 
 In addition, ``cellpy`` can load custom csv-ish files by providing a file description (using the
 ``nstruments.Custom`` object).
 
 
 Tester dependent attributes
-~~~~~~~~~~~~~~~~~~~~~~~~~~~
+---------------------------
 
 arbin .res
 ..........
 
 Three tables are read from the .res file:
 
 * normal table: contains measurement data.
@@ -339,20 +346,20 @@
 
 Maccor .txt
 ...........
 
 TODO...
 
 
-CellpyData - methods
-~~~~~~~~~~~~~~~~~~~~
+CellpyCell - methods
+--------------------
 
-The ``CellpyData`` object contains lots of methods for manipulating, extracting
+The ``CellpyCell`` object contains lots of methods for manipulating, extracting
 and summarising the data from the run(s). Two methods are typically automatically run when
-you create your ``CellpyData`` object when running ``cellpy.get(filename)``:
+you create your ``CellpyCell`` object when running ``cellpy.get(filename)``:
 
     - ``make_step_table``: creates a statistical summary of all the steps in the run(s) and categorizes
       the step type from that. It is also possible to give the step types directly (step_specifications).
 
     - ``make_summary``: create a summary based on cycle number.
 
 Other methods worth mentioning are (based on what I typically use):
@@ -364,33 +371,46 @@
     - ``get_cap``: get the capacity-voltage graph from one or more cycles in three different formats as well
       as optionally interpolated, normalized and/or scaled.
 
     - ``get_cycle_numbers``: get the cycle numbers for your run.
 
     - ``get_ocv``: get the rest steps after each charge and discharge step.
 
-Take a look at API section (Module index, ``cellpy.readers.cellreader.CellpyData``) for more info.
+Take a look at API section (Module index, ``cellpy.readers.cellreader.CellpyCell``) for more info.
+
+Data
+----
+
+.. graphviz::
+
+   digraph {
+    "CellpyCell" -> "Data";
+        "Data" -> "cell metadata (cell)";
+        "Data" -> "cell metadata (test)";
+        "Data" -> "methods";
+        "Data" -> "raw";
+        "Data" -> "steps";
+        "Data" -> "summary";
+   }
 
-Cells
-~~~~~
 
-Each run is a ``cellpy.cellreader.Cell`` instance.
+Each run is a ``cellpy.cellreader.Data`` instance.
 The instance contain general information about
 the run-settings (such as mass etc.).
 The measurement data, information, and summary is stored
 in three ``pandas.DataFrames``:
 
     - ``raw``: raw data from the run.
     - ``steps``: stats from each step (and step type), created using the
-      ``CellpyData.make_step_table`` method.
+      ``CellpyCell.make_step_table`` method.
     - ``summary``: summary data vs. cycle number (e.g. coulombic coulombic efficiency), created using
-      the ``CellpyData.make_summary`` method.
+      the ``CellpyCell.make_summary`` method.
 
 The headers (columns) for the different DataFrames were given earlier in this chapter.
-As mentioned above, the ``Cell`` object also contains metadata for the run.
+As mentioned above, the ``Data`` object also contains metadata for the run.
 
 metadata
 ........
 
 .. code-block:: python
 
     cell_no = None
@@ -427,15 +447,15 @@
     cell_type = None
     separator_type = None
     active_electrode_current_collector = None
     reference_electrode_current_collector = None
     comment = None
 
 
-The ``Cell`` object can also take custom metadata if provieded as keyword arguments (for developers).
+The ``Data`` object can also take custom metadata if provieded as keyword arguments (for developers).
 
 FileID
-~~~~~~
+------
 
 The ``FileID`` object contains information about the raw file(s) and is used when comparing the cellpy-file
 with the raw file(s) (for example to check if it has been updated compared to the cellpy-file).
 Notice that ``FileID`` will contain a list of file identification parameters if the run is from several raw files.
```

### Comparing `cellpy-0.4.3a3/docs/installation.rst` & `cellpy-1.0.0a0/docs/main_description/installation.rst`

 * *Files 8% similar despite different names*

```diff
@@ -1,17 +1,19 @@
+.. highlight:: shell
+
 ============
 Installation
 ============
 
 If you are (relatively) new to installing python packages, please jump to the
-getting started tutorial (:doc:`tutorials/getting_started_tutorial`)
+getting started tutorial (:ref:`getting-started`)
 for an opinionated step-by-step procedure.
 
 Stable release
---------------
+==============
 
 The preferred way to install ``cellpy`` is by using conda:
 
 .. code-block:: console
 
     $ conda install cellpy --channel conda-forge
 
@@ -19,15 +21,15 @@
 This will also install all of the critical dependencies, as well as ``jupyter``
 that comes in handy when working with ``cellpy``.
 
 If you would like to install only ``cellpy``, you should install using pip.
 You also need to take into account that ``cellpy`` uses several packages
 that are a bit cumbersome to install on
 windows. It is therefore recommended to install one of the ``anaconda``
-python packages (python 3.8 or above) before installing ``cellpy``.
+python packages (python 3.9 or above) before installing ``cellpy``.
 If you chose ``miniconda``, you should install
 ``scipy``, ``numpy`` and ``pytables`` using ``conda``:
 
 .. code-block:: console
 
     $ conda install scipy numpy pytables
 
@@ -56,17 +58,17 @@
     $ conda install -c conda-forge pyodbc
 
 .. _pyodbc: https://github.com/mkleehammer/pyodbc/
 
 Some of the utilities in ``cellpy`` have additional dependencies:
 
 - Using the ``ocv_rlx`` utilities requires ``lmfit`` and ``matplotlib``.
-- For using the ``batch`` utilities efficiently, ``holoviews`` is needed, as
-  well as ``bokeh`` and ``matplotlib`` for plotting.
-
+- For using the ``batch`` utilities efficiently, you should install
+  ``bokeh``, ``plotly``, and ``matplotlib`` for plotting. Also, ``holoviews``
+  is a good tool to have.
 
 If this is the first time you install ``cellpy``, it is recommended
 that you run the setup script:
 
 .. code-block:: console
 
     $ cellpy setup -i
@@ -97,15 +99,15 @@
     in the getting-started tutorial. For Posix-type systems, you will need to download
     and install ``mdbtools``. If you are on Windows and you cannot get your
     ``pyodbc`` to work, you can try the same there also (search for Windows
     binaries and set the appropriate settings in your ``cellpy`` config file).
 
 
 From sources
-------------
+============
 
 The sources for ``cellpy`` can be downloaded from the `Github repo`_.
 
 You can clone the public repository by:
 
 .. code-block:: console
 
@@ -115,20 +117,20 @@
 Once you have a copy of the source, you can install in development
 mode using pip:
 
 .. code-block:: console
 
     $ pip install -e .
 
-(assuming that you are in the project folder, *i. e.* the folder that
+(assuming that you are in the project folder, *i.e.* the folder that
 contains the setup.py file)
 
 Further reading
----------------
+===============
 
 You can find more information in the Tutorials, particularly
-in :doc:`tutorials/getting_started_tutorial`.
+in ':ref:`getting-started`'.
 
 .. _Github repo: https://github.com/jepegit/cellpy
```

### Comparing `cellpy-0.4.3a3/docs/make.bat` & `cellpy-1.0.0a0/docs/make.bat`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/docs/source/cellpy.readers.instruments.rst` & `cellpy-1.0.0a0/docs/source/cellpy.readers.instruments.rst`

 * *Files 18% similar despite different names*

```diff
@@ -1,90 +1,135 @@
 cellpy.readers.instruments package
 ==================================
 
+Subpackages
+-----------
+
+.. toctree::
+   :maxdepth: 4
+
+   cellpy.readers.instruments.configurations
+   cellpy.readers.instruments.loader_specific_modules
+   cellpy.readers.instruments.processors
+
 Submodules
 ----------
 
-cellpy.readers.instruments.arbin module
----------------------------------------
+cellpy.readers.instruments.arbin\_res module
+--------------------------------------------
 
 .. automodule:: cellpy.readers.instruments.arbin_res
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
-.. automodule:: cellpy.readers.instruments.arbin_sql
-    :members:
-    :undoc-members:
-    :show-inheritance:
+cellpy.readers.instruments.arbin\_sql module
+--------------------------------------------
 
-.. automodule:: cellpy.readers.instruments.arbin_sql_csv
-    :members:
-    :undoc-members:
-    :show-inheritance:
+.. automodule:: cellpy.readers.instruments.arbin_sql
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+cellpy.readers.instruments.arbin\_sql\_7 module
+-----------------------------------------------
+
+.. automodule:: cellpy.readers.instruments.arbin_sql_7
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
-cellpy.readers.instruments.custom module
----------------------------------------
+cellpy.readers.instruments.arbin\_sql\_csv module
+-------------------------------------------------
 
-.. automodule:: cellpy.readers.instruments.custom
-    :members:
-    :undoc-members:
-    :show-inheritance:
+.. automodule:: cellpy.readers.instruments.arbin_sql_csv
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
-cellpy.readers.instruments.biologics\_mpr module
+cellpy.readers.instruments.arbin\_sql\_h5 module
 ------------------------------------------------
 
-.. automodule:: cellpy.readers.instruments.biologics_mpr
-    :members:
-    :undoc-members:
-    :show-inheritance:
+.. automodule:: cellpy.readers.instruments.arbin_sql_h5
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+cellpy.readers.instruments.arbin\_sql\_xlsx module
+--------------------------------------------------
+
+.. automodule:: cellpy.readers.instruments.arbin_sql_xlsx
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.readers.instruments.base module
----------------------------------------
+--------------------------------------
 
 .. automodule:: cellpy.readers.instruments.base
-    :members:
-    :undoc-members:
-    :show-inheritance:
-
-cellpy.readers.instruments.pec module
--------------------------------------
-
-.. automodule:: cellpy.readers.instruments.pec
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
-cellpy.readers.instruments.maccor module
-----------------------------------------
+cellpy.readers.instruments.biologics\_mpr module
+------------------------------------------------
 
-.. automodule:: cellpy.readers.instruments.maccor_txt
-    :members:
-    :undoc-members:
-    :show-inheritance:
+.. automodule:: cellpy.readers.instruments.biologics_mpr
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
-cellpy.readers.instruments.processors module
---------------------------------------------
+cellpy.readers.instruments.custom module
+----------------------------------------
 
-pre-processors
-..............
+.. automodule:: cellpy.readers.instruments.custom
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+cellpy.readers.instruments.ext\_nda\_reader module
+--------------------------------------------------
+
+.. automodule:: cellpy.readers.instruments.ext_nda_reader
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+cellpy.readers.instruments.local\_instrument module
+---------------------------------------------------
+
+.. automodule:: cellpy.readers.instruments.local_instrument
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+cellpy.readers.instruments.maccor\_txt module
+---------------------------------------------
 
-.. automodule:: cellpy.readers.instruments.processors.pre_processors
-    :members:
-    :undoc-members:
-    :show-inheritance:
-
-post-processors
-...............
-
-.. automodule:: cellpy.readers.instruments.processors.post_processors
-    :members:
-    :undoc-members:
-    :show-inheritance:
+.. automodule:: cellpy.readers.instruments.maccor_txt
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+cellpy.readers.instruments.neware\_txt module
+---------------------------------------------
+
+.. automodule:: cellpy.readers.instruments.neware_txt
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+cellpy.readers.instruments.pec\_csv module
+------------------------------------------
+
+.. automodule:: cellpy.readers.instruments.pec_csv
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 Module contents
 ---------------
 
 .. automodule:: cellpy.readers.instruments
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
```

### Comparing `cellpy-0.4.3a3/docs/source/cellpy.readers.rst` & `cellpy-1.0.0a0/docs/source/cellpy.readers.rst`

 * *Files 13% similar despite different names*

```diff
@@ -1,53 +1,61 @@
 cellpy.readers package
 ======================
 
 Subpackages
 -----------
 
 .. toctree::
+   :maxdepth: 4
 
-    cellpy.readers.instruments
+   cellpy.readers.instruments
 
 Submodules
 ----------
 
 cellpy.readers.cellreader module
 --------------------------------
 
 .. automodule:: cellpy.readers.cellreader
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.readers.core module
 --------------------------
 
 .. automodule:: cellpy.readers.core
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.readers.dbreader module
 ------------------------------
 
 .. automodule:: cellpy.readers.dbreader
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.readers.filefinder module
 --------------------------------
 
 .. automodule:: cellpy.readers.filefinder
-    :members:
-    :undoc-members:
-    :show-inheritance:
-
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+cellpy.readers.sql\_dbreader module
+-----------------------------------
+
+.. automodule:: cellpy.readers.sql_dbreader
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 Module contents
 ---------------
 
 .. automodule:: cellpy.readers
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
```

### Comparing `cellpy-0.4.3a3/docs/source/cellpy.rst` & `cellpy-1.0.0a0/docs/source/cellpy.rst`

 * *Files 18% similar despite different names*

```diff
@@ -1,46 +1,48 @@
 cellpy package
 ==============
 
 Subpackages
 -----------
 
 .. toctree::
+   :maxdepth: 4
 
-    cellpy.parameters
-    cellpy.readers
-    cellpy.utils
+   cellpy.internals
+   cellpy.parameters
+   cellpy.readers
+   cellpy.utils
 
 Submodules
 ----------
 
 cellpy.cli module
 -----------------
 
 .. automodule:: cellpy.cli
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.exceptions module
 ------------------------
 
 .. automodule:: cellpy.exceptions
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.log module
 -----------------
 
 .. automodule:: cellpy.log
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 Module contents
 ---------------
 
 .. automodule:: cellpy
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
```

### Comparing `cellpy-0.4.3a3/docs/source/cellpy.utils.batch_tools.rst` & `cellpy-1.0.0a0/docs/source/cellpy.utils.batch_tools.rst`

 * *Files 10% similar despite different names*

```diff
@@ -4,91 +4,98 @@
 Submodules
 ----------
 
 cellpy.utils.batch\_tools.batch\_analyzers module
 -------------------------------------------------
 
 .. automodule:: cellpy.utils.batch_tools.batch_analyzers
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.utils.batch\_tools.batch\_core module
 --------------------------------------------
 
 .. automodule:: cellpy.utils.batch_tools.batch_core
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.utils.batch\_tools.batch\_experiments module
 ---------------------------------------------------
 
 .. automodule:: cellpy.utils.batch_tools.batch_experiments
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.utils.batch\_tools.batch\_exporters module
 -------------------------------------------------
 
 .. automodule:: cellpy.utils.batch_tools.batch_exporters
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.utils.batch\_tools.batch\_helpers module
 -----------------------------------------------
 
 .. automodule:: cellpy.utils.batch_tools.batch_helpers
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.utils.batch\_tools.batch\_journals module
 ------------------------------------------------
 
 .. automodule:: cellpy.utils.batch_tools.batch_journals
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.utils.batch\_tools.batch\_plotters module
 ------------------------------------------------
 
 .. automodule:: cellpy.utils.batch_tools.batch_plotters
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.utils.batch\_tools.batch\_reporters module
 -------------------------------------------------
 
 .. automodule:: cellpy.utils.batch_tools.batch_reporters
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.utils.batch\_tools.dumpers module
 ----------------------------------------
 
 .. automodule:: cellpy.utils.batch_tools.dumpers
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.utils.batch\_tools.engines module
 ----------------------------------------
 
 .. automodule:: cellpy.utils.batch_tools.engines
-    :members:
-    :undoc-members:
-    :show-inheritance:
-
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+cellpy.utils.batch\_tools.sqlite\_from\_excel\_db module
+--------------------------------------------------------
+
+.. automodule:: cellpy.utils.batch_tools.sqlite_from_excel_db
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 Module contents
 ---------------
 
 .. automodule:: cellpy.utils.batch_tools
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
```

### Comparing `cellpy-0.4.3a3/docs/source/cellpy.utils.rst` & `cellpy-1.0.0a0/docs/source/cellpy.utils.rst`

 * *Files 19% similar despite different names*

```diff
@@ -1,94 +1,117 @@
 cellpy.utils package
 ====================
 
 Subpackages
 -----------
 
 .. toctree::
+   :maxdepth: 4
 
-    cellpy.utils.batch_tools
+   cellpy.utils.batch_tools
 
 Submodules
 ----------
 
 cellpy.utils.batch module
 -------------------------
 
 .. automodule:: cellpy.utils.batch
-    :members:
-    :undoc-members:
-    :show-inheritance:
-    :exclude-members: check_iterate, check_new, check_standard
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+cellpy.utils.collectors module
+------------------------------
+
+.. automodule:: cellpy.utils.collectors
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+cellpy.utils.collectors\_old module
+-----------------------------------
+
+.. automodule:: cellpy.utils.collectors_old
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.utils.diagnostics module
 -------------------------------
 
 .. automodule:: cellpy.utils.diagnostics
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
-cellpy.utils.helpers module
----------------------------
+cellpy.utils.easyplot module
+----------------------------
 
-.. automodule:: cellpy.utils.helpers
-    :members:
-    :undoc-members:
-    :show-inheritance:
+.. automodule:: cellpy.utils.easyplot
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
-cellpy.utils.example\_data
---------------------------
+cellpy.utils.example\_data module
+---------------------------------
 
 .. automodule:: cellpy.utils.example_data
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+cellpy.utils.helpers module
+---------------------------
+
+.. automodule:: cellpy.utils.helpers
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.utils.ica module
 -----------------------
 
 .. automodule:: cellpy.utils.ica
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
-cellpy.utils.live module (not implemented yet)
-----------------------------------------------
+cellpy.utils.live module
+------------------------
 
 .. automodule:: cellpy.utils.live
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.utils.ocv\_rlx module
 ----------------------------
 
 .. automodule:: cellpy.utils.ocv_rlx
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 cellpy.utils.plotutils module
 -----------------------------
 
 .. automodule:: cellpy.utils.plotutils
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
-cellpy.utils.easyplot module
+cellpy.utils.processor module
 -----------------------------
 
-.. automodule:: cellpy.utils.easyplot
-    :members:
-    :undoc-members:
-    :show-inheritance:
-
+.. automodule:: cellpy.utils.processor
+   :members:
+   :undoc-members:
+   :show-inheritance:
 
 Module contents
 ---------------
 
 .. automodule:: cellpy.utils
-    :members:
-    :undoc-members:
-    :show-inheritance:
+   :members:
+   :undoc-members:
+   :show-inheritance:
```

### Comparing `cellpy-0.4.3a3/docs/tips_and_tricks.rst` & `cellpy-1.0.0a0/docs/examples_and_tutorials/tips_and_tricks.rst`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/docs/tutorials/configuring.rst` & `cellpy-1.0.0a0/docs/examples_and_tutorials/basic_interactions/05_configuring.rst`

 * *Files 6% similar despite different names*

```diff
@@ -1,25 +1,24 @@
 Configuring cellpy
 ==================
 
 How the configuration parameters are set and read
 -------------------------------------------------
 
-When ``cellpy`` is imported, it sets a default set of parameters.
+When ``cellpy`` is imported, a default set of parameters is set.
 Then it tries to read the parameters
-from your .conf-file (located in your user directory). If it is successful,
-the parameters set in your .conf-file
-will over-ride the default ones.
+from your .conf-file (located in your user directory). If successful,
+the parameters set in your .conf-file will over-ride the default.
 
 The parameters are stored in the module ``cellpy.parameters.prms``.
 
-If you during your script (or in your ``jupyter notebook``) would like to
-change some of the settings (*e.g.* if you
+If you would like to change some of the settings during your script
+(or in your ``jupyter notebook``), *e.g.* if you
 want to use the ``cycle_mode`` option "cathode" instead of the
-default "anode"), then import the prms class and set new
+default "anode", then import the prms class and set new
 values:
 
 .. code-block:: python
 
     from cellpy import parameters.prms
 
     # Changing cycle_mode to cathode
@@ -32,16 +31,15 @@
     prms.Paths.outdatadir = 'experiment01/processed_data'
 
 
 The configuration file
 ----------------------
 
 ``cellpy`` tries to read your .conf-file when imported the first time,
-and looks in your user directory on posix or in the documents folder on
-windows (*e.g.* C:\\Users\\USERNAME\\Documents on not-too-old versions of windows) after
+and looks in your user directory after
 files named ``.cellpy_prms_SOMENAME.conf``.
 
 If you have run ``cellpy setup`` in the cmd window or in the shell, the
 configuration file will be placed in the appropriate place.
 It will have the name ``.cellpy_prms_USERNAME.conf`` (where USERNAME is your username).
 
 The configuration file is a YAML-file and it is reasonably easy to read and edit (but
@@ -50,140 +48,140 @@
 As an example, here are the first lines
 from one of the authors' configuration file:
 
 .. code-block:: yaml
 
     ---
     Paths:
-      outdatadir: C:\scripts\processing_cellpy\out
-      rawdatadir: I:\Org\MPT-BAT-LAB\Arbin-data
-      cellpydatadir: C:\scripts\processing_cellpy\cellpyfiles
-      db_path: C:\scripts\processing_cellpy\db
-      filelogdir: C:\scripts\processing_cellpy\logs
-      examplesdir: C:\scripts\processing_cellpy\examples
-      notebookdir: C:\scripts\processing_cellpy\notebooks
-      templatedir: C:\scripting\processing_cellpy\templates
-      batchfiledir: C:\scripts\processing_cellpy\batchfiles
-      db_filename: 2020_Cell_Analysis_db_001.xlsx
+        outdatadir: C:\scripts\processing_cellpy\out
+        rawdatadir: I:\Org\MPT-BAT-LAB\Arbin-data
+        cellpydatadir: C:\scripts\processing_cellpy\cellpyfiles
+        db_path: C:\scripts\processing_cellpy\db
+        filelogdir: C:\scripts\processing_cellpy\logs
+        examplesdir: C:\scripts\processing_cellpy\examples
+        notebookdir: C:\scripts\processing_cellpy\notebooks
+        templatedir: C:\scripting\processing_cellpy\templates
+        batchfiledir: C:\scripts\processing_cellpy\batchfiles
+        db_filename: 2023_Cell_Analysis_db_001.xlsx
+        env_file: .env_cellpy
+
 
     FileNames:
-      file_name_format: YYYYMMDD_[NAME]EEE_CC_TT_RR
+        file_name_format: YYYYMMDD_[NAME]EEE_CC_TT_RR
 
 
 The first part contains definitions of the different paths, files and file-patterns
-that ``cellpy`` will use. This is probably the place
-where you most likely will have to do some edits sometime.
+that ``cellpy`` will use. This is the place where you most likely will have to do
+some edits sometime.
 
-Next comes definitions needed when using a db.
+The next part contains definitions required when using a database:
 
 .. code-block:: yaml
 
     # settings related to the db used in the batch routine
     Db:
-      db_type: simple_excel_reader
-      db_table_name: db_table
-      db_header_row: 0
-      db_unit_row: 1
-      db_data_start_row: 2
-      db_search_start_row: 2
-      db_search_end_row: -1
+        db_type: simple_excel_reader
+        db_table_name: db_table
+        db_header_row: 0
+        db_unit_row: 1
+        db_data_start_row: 2
+        db_search_start_row: 2
+        db_search_end_row: -1
 
     # definitions of headers for the simple_excel_reader
     DbCols:
-      id:
-      - id
-      - int
-      exists:
-      - exists
-      - bol
-      batch:
-      - batch
-      - str
-      sub_batch_01:
-      - b01
-      - str
-      .
-      .
-
-
-Its rather long (since it needs to define the column names used in the db excel sheet).
-After this, the settings the datasets and the ``cellreader`` comes, as well as for the different instruments.
-You will also find the settings for the ``batch`` utility at the bottom.
+        id:
+        - id
+        - int
+        exists:
+        - exists
+        - bol
+        batch:
+        - batch
+        - str
+        sub_batch_01:
+        - b01
+        - str
+        .
+        .
+
+
+This part is rather long (since it needs to define the column names used in the db excel sheet).
+
+The next part contains settings regarding your dataset and the ``cellreader``, as well as for
+the different ``instruments``. At the bottom you will find the settings for the ``batch`` utility.
 
 .. code-block:: yaml
 
     # settings related to your data
     DataSet:
-      nom_cap: 3579
+        nom_cap: 3579
 
     # settings related to the reader
     Reader:
-      Reader:
-        diagnostics: false
-        filestatuschecker: size
-        force_step_table_creation: true
-        force_all: false
-        sep: ;
-        cycle_mode: anode
-        sorted_data: true
-        load_only_summary: false
-        select_minimal: false
-        limit_loaded_cycles:
-        ensure_step_table: false
-        daniel_number: 5
-        voltage_interpolation_step: 0.01
-        time_interpolation_step: 10.0
-        capacity_interpolation_step: 2.0
-        use_cellpy_stat_file: false
-        auto_dirs: true
+        Reader:
+            diagnostics: false
+            filestatuschecker: size
+            force_step_table_creation: true
+            force_all: false
+            sep: ;
+            cycle_mode: anode
+            sorted_data: true
+            select_minimal: false
+            limit_loaded_cycles:
+            ensure_step_table: false
+            voltage_interpolation_step: 0.01
+            time_interpolation_step: 10.0
+            capacity_interpolation_step: 2.0
+            use_cellpy_stat_file: false
+            auto_dirs: true
 
     # settings related to the instrument loader
     # (each instrument can have its own set of settings)
     Instruments:
-      tester: arbin
-      custom_instrument_definitions_file:
+        tester: arbin
+        custom_instrument_definitions_file:
 
-      Arbin:
+    Arbin:
         max_res_filesize: 1000000000
         chunk_size:
         max_chunks:
         use_subprocess: false
         detect_subprocess_need: false
         sub_process_path:
         office_version: 64bit
         SQL_server: localhost
         SQL_UID:
         SQL_PWD:
         SQL_Driver: ODBC Driver 17 for SQL Server
         odbc_driver:
-      Maccor:
+    Maccor:
         default_model: one
 
     # settings related to running the batch procedure
     Batch:
-      fig_extension: png
-      backend: bokeh
-      notebook: true
-      dpi: 300
-      markersize: 4
-      symbol_label: simple
-      color_style_label: seaborn-deep
-      figure_type: unlimited
-      summary_plot_width: 900
-      summary_plot_height: 800
-      summary_plot_height_fractions:
-      - 0.2
-      - 0.5
-      - 0.3
+        fig_extension: png
+        backend: bokeh
+        notebook: true
+        dpi: 300
+        markersize: 4
+        symbol_label: simple
+        color_style_label: seaborn-deep
+        figure_type: unlimited
+        summary_plot_width: 900
+        summary_plot_height: 800
+        summary_plot_height_fractions:
+        - 0.2
+        - 0.5
+        - 0.3
     ...
 
 
 As you can see, the author of this particular file most likely works with
 silicon as anode material for lithium ion
 batteries (the ``nom_cap`` is set to 3579 mAh/g, *i.e.* the theoretical
-gravimetric lithium capacity for silicon at
-normal temperatures). And, he or she is using windows.
+gravimetric lithium capacity for silicon at normal temperatures) and is using windows.
 
 By the way, if you are wondering what
 the '.' means... it means nothing - it was just something I added in this
-tutorial text to indicate that there are
+tutorial text to indicate that there is
 more stuff in the actual file than what is shown here.
```

### Comparing `cellpy-0.4.3a3/docs/tutorials/data_mining.rst` & `cellpy-1.0.0a0/docs/examples_and_tutorials/basic_interactions/07_data_mining.rst`

 * *Files 26% similar despite different names*

```diff
@@ -1,32 +1,35 @@
 Data mining / using a database
 ==============================
 
+.. note:: This chapter would benefit from some more love and care. Any help
+    on that would be highly appreciated.
+
 One important motivation for developing the ``cellpy`` project is to facilitate
 handling many cell testing experiments within a reasonable time and with a
 "tunable" degree of automation. It is therefore convenient to be able to
 couple both the meta-data (look-up) to some kind of data-base, as well as
 saving the extracted key parameters to either the same or another database
 (where I recommend the latter). The database(s) will be a valuable asset for
 further data analyses (either using statistical methods, e.g. Bayesian
 modelling, or as input to machine learning algorithms, for example deep
 learning using cnn).
 
 Meta-data database
-..................
+------------------
 
 TODO.
 
 Parameters and feature extraction
-.................................
+---------------------------------
 
 TODO.
 
 Bayesian modelling
-..................
+------------------
 
 TODO.
 
 Example: reinforcement deep learning (resnet)
 .............................................
 
 TODO.
```

### Comparing `cellpy-0.4.3a3/docs/tutorials/getting_started_tutorial.rst` & `cellpy-1.0.0a0/docs/examples_and_tutorials/basic_interactions/01_getting_started_tutorial.rst`

 * *Files 24% similar despite different names*

```diff
@@ -1,208 +1,347 @@
+.. _getting-started:
+
 The getting started with ``cellpy`` tutorial (opinionated version)
 ==================================================================
 
 This tutorial will help you getting started with ``cellpy`` and
-tries to give you a step-by-step recipe. The information in this tutorial
-can also (most likely) be found elsewhere. For the novice users,
-jump directly to chapter 1.2.
+tries to give you a step-by-step recipe.
 
 How to install ``cellpy`` - the minimalistic explanation
 --------------------------------------------------------
 
 If you know what you are doing, and only need the most basic features
 of ``cellpy``, you should be able to get things up and running by
 issuing a simple
 
-.. code:: bash
+.. code-block:: console
 
-   pip install cellpy
+    pip install cellpy
 
 It is recommended that you use a Python environment (or conda
-environment) and give it a easy to remember name *e.g.* ``cellpy``.
-
-You also need the typical scientific python pack, including ``numpy``,
-``scipy``, and ``pandas``. It is recommended that you at least install
-``scipy`` before you install ``cellpy`` (the main benefit being that you
-can use ``conda`` so that you dont have to hassle with missing
-C-compilers if you are on an Windows machine).
-
-
-Install a couple of other dependencies
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-You should also install some additional dependencies:
-
-``pytables`` is needed for working with the hdf5 files (the cellpy-files):
-
-.. code:: bash
-
-   conda install -c conda-forge pytables
-
-If you would like to use some of the fitting routines in ``cellpy``, you
-will need to install ``lmfit``:
-
-.. code:: bash
-
-   conda install -c conda-forge lmfit
-
-
-Another tool that is really handy is Jupyter. And the plotting library
-bundle holoviz. You might already have them installed. If not, I recommend
-that you look at their documentation (google it) and install them. You can most
-likely use the same method as for pytables etc.
-
-Note! In addition to the requirements set in the ``setup.py`` file, you
-will also need a Python ODBC bridge for loading .res-files from Arbin
-testers. And possible also other *too-be-implemented* file formats. I
-recommend `pyodbc <https://github.com/mkleehammer/pyodbc/wiki>`__ that
-can be installed from conda forge or using pip.
+environment) and give it an easy-to-remember name *e.g.* ``cellpy``.
 
-.. code:: bash
+To make sure your environment contains the correct packages and
+dependencies, you can create the environment based on the available
+`environment.yml <https://github.com/jepegit/cellpy/blob/master/environment.yml>`_
+file. For further information on dependencies and requirements for setting up
+``cellpy`` to read .res (Arbin) files, have a look at the *Install dependencies*
+part of the next section.
 
-   conda install -c conda-forge pyodbc
+For the installation of specific versions and pre-releases, see
+:ref:`check-cellpy`.
 
-For reading .res-files
-(which actually are in a Microsoft Access format) you also need a driver
-or similar to help your ODBC bridge accessing it. A small hint for
-Windows users: if you dont have one of the most recent Office version,
-you might not be allowed to install a driver of different bit than your
-office version is using (the installers can be found
-`here <https://www.microsoft.com/en-US/download/details.aspx?id=13255>`__).
-Also remark that the driver needs to be of the same bit as your Python
-(so, if you are using 32 bit Python, you will need the 32 bit driver).
-
-For POSIX systems, I have not found any suitable drivers. Instead,
-``cellpy`` will try to use ``mdbtools``\ to first export the data to
-temporary csv-files, and then import from those csv-file (using the
-``pandas`` library). You can install ``mdbtools`` using your systems
-preferred package manager (*e.g.* ``apt-get install mdbtools``).
-
-The tea spoon explanation
--------------------------
+How to install and run ``cellpy`` - the tea spoon explanation
+-------------------------------------------------------------
 
 If you are used to installing stuff from the command line (or shell),
 then things might very well run smoothly. However, a considerable
-percentage of us dont feel exceedingly comfortable installing things by
-writing commands inside a small black window. Lets face it; we belong
-to the *point-and-click* (or *double-click*) generation, not the
+percentage of potential users dont feel exceedingly comfortable installing
+things by writing commands inside a small black window. Lets face it; most of us
+belong to the *point-and-click* (or *double-click*) generation, not the
 *write-cryptic-commands* generation. So, hopefully without insulting the
-savvy, here is a tea-spoon explanation
+savvy, here is a tea-spoon explanation:
 
-Install a scientific stack of python 3.x
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+1. Install a scientific stack of python 3.x
+...........................................
 
-If the words virtual environment or miniconda dont ring any bells,
+If the words virtual environment or miniconda do not ring any bells,
 you should install the Anaconda scientific Python distribution. Go to
 `www.anaconda.com <https://www.anaconda.com/>`__ and select the
-Anaconda distribution (press the ``Download Now`` button). And no, dont
-select python 2.7. Use at least python 3.6. And select the 64 bit version
+Anaconda distribution (press the ``Download Now`` button).
+Use at least python 3.9, and select the 64 bit version
 (if you fail at installing the 64 bit version, then you can try the
 weaker 32 bit version). Download it and let it install.
 
-Create a virtual environment
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+.. caution:: The bin version matters sometimes, so try to make a mental note
+    of what you selected. E.g., if you plan to use the Microsoft Access odbc
+    driver (see below), and it is 32-bit, you probably should chose to install
+    a 32-bit python version).
 
-This step can be omitted (but its not necessary very smart to do so).
-Create a virtual conda environment called ``my_cellpy`` (the name is not
-important, but it should be a name you are able to remember).
+Python should now be available on your computer, as well as
+a huge amount of python packages. And Anaconda is kind enough
+to also install an alternative command window called "Anaconda Prompt"
+that has the correct settings ensuring that the conda command works
+as it should.
 
+2. Create a virtual environment
+...............................
 
-Open up a command window (you can find a command window on Windows by
-*e.g* pressing the Windows button + r and typing ``cmd.exe``), or even better,
-open up "anaconda prompt". Then type
+This step can be omitted (but its not necessarily smart to do so).
+Create a virtual conda environment called ``cellpy`` (the name is not
+important, but it should be a name you are able to remember) by following
+the steps below:
 
-.. code:: bash
+Open up the "Anaconda Prompt" (or use the command window) and type
 
-   conda create -n my_cellpy
+.. code-block:: console
 
-Then activate your environment:
+    conda create -n cellpy
 
-.. code:: bash
+This creates your virtual environment (here called *cellpy*) in which ``cellpy``
+will be installed and used.
 
-   conda activate my_cellpy
+You then have to activate the environment:
 
-If you get an error message, then it could be that your Python version is
-not available for you (maybe you installed as root?). If you were using
-the command window on windows, try to locate the anaconda prompt program and run that
-instead.
+.. code-block:: console
 
-Install ``cellpy``
-~~~~~~~~~~~~~~~~~~
+    conda activate cellpy
 
-.. code:: bash
 
-   conda install -c conda-forge cellpy
+3. Install ``cellpy``
+.....................
 
-Note that the bin version matters some times, so try
-to make a mental note of what you selected (for
-example, if you plan to use the Microsoft Access odbc driver, and it is
-32-bit, you probably should chose to install an 32-bit python version
-(see next sub-chapter)).
+In your activated ``cellpy`` environment in the Anaconda Prompt if you
+chose to make one, or in the base environment if you chose not to, run:
 
-If you don't have the newest office suit, you might need to install
-the Microsoft Access odbc driver which can be downloaded from `this
-page <https://www.microsoft.com/en-US/download/details.aspx?id=13255>`__
+.. code-block:: console
 
+    conda install -c conda-forge cellpy
 
-Check your installation
-~~~~~~~~~~~~~~~~~~~~~~~
+Congratulations, you have (hopefully) successfully installed cellpy.
+
+If you run into problems, doublecheck that all your dependencies are
+installed and check your Microsoft Access odbc drivers.
+
+.. _check-cellpy:
+
+4. Check your cellpy installation
+.................................
 
 The easiest way to check if ``cellpy`` has been installed, is to issue
 the command for printing the version number to the screen
 
-.. code:: bash
+.. code-block:: console
 
-   cellpy info --version
+    cellpy info --version
 
 If the program prints the expected version number, you probably
 succeeded. If it crashes, then you will have to retrace your steps, redo
 stuff and hope for the best. If it prints an older (lower) version
-number than you expect, it is a big chance that you have installed it
+number than you expect, there is a big chance that you have installed it
 earlier, and what you would like to do is to do an ``upgrade`` instead
 of an ``install``
 
-.. code:: bash
+.. code-block:: console
 
    pip install --upgrade cellpy
 
-It could also be that you want to install a pre-release (a version that
-is so bleeding edge that it ends with a alpha or beta release
-identification, *e.g.* ends with .b2). Then you will need to add the
-pre modifier
+If you want to install a pre-release (a version that is so bleeding edge
+that it ends with a alpha or beta release identification, *e.g.* ends
+with .b2). Then you will need to add the pre modifier
 
-.. code:: bash
+.. code-block:: console
 
    pip install --pre cellpy
 
 To run a more complete check of your installation, there exist a
 ``cellpy`` sub-command than can be helpful
 
-.. code:: bash
+.. code-block:: console
 
    cellpy info --check
 
 
-The ``cellpy`` command to your rescue
--------------------------------------
+5. Set up ``cellpy``
+....................
+
+After you have installed ``cellpy`` it is highly recommended that you
+create an appropriate configuration file and folders for raw data,
+cellpy-files, logs, databases and output data (and inform ``cellpy`` about it).
+
+To do this, run the setup command:
+
+.. code-block:: console
+
+       cellpy setup
+
+To run the setup in interactive mode, use -i:
+
+.. code-block:: console
+
+       cellpy setup -i
+
+This creates the cellpy configuration file ``_cellpy_prms_USERNAME.conf``
+in your home directory (USERNAME = your user name) and creates the standard
+cellpy_data folders (if they do not exist).
+The ``-i`` option makes sure that the setup is done interactively:
+The program will ask you about where specific folders are, *e.g.* where
+you would like to put your outputs and where your cell data files are
+located. If the folders do not exist, ``cellpy`` will try to create them.
+
+If you want to specify a root folder different from the default (your HOME
+folder), you can use the ``-d`` option *e.g.*
+``cellpy setup -i -d /Users/kingkong/cellpydir``
+
+.. hint:: You can always edit your configurations directly in the cellpy configuration
+   file ``_cellpy_prms_USER.conf``. This file should be located inside your
+   home directory, /. in posix and c:\users\USERNAME in not-too-old windows.
+
+
+6. Create a notebook and run ``cellpy``
+.......................................
+
+Inside your Anaconda Prompt window, write:
+
+.. code-block:: console
+
+       jupyter notebook  # or jupyter lab
+
+Your browser should then open and you are ready to write your first cellpy script.
+
+There are many good tutorials on how to work with jupyter.
+This one by Real Python is good for beginners:
+`Jupyter Notebook: An Introduction <https://realpython.com/jupyter-notebook-introduction/>`_
+
+
+More about installing and setting up ``cellpy``
+-----------------------------------------------
+
+Fixing dependencies
+...................
+
+To make sure your environment contains the correct packages and dependencies
+required for running cellpy, you can create an environment based on the available
+``environment.yml`` file. Download the
+`environment.yml <https://github.com/jepegit/cellpy/blob/master/environment.yml>`_
+file and place it in the directory shown in your Anaconda Prompt. If you want to
+change the name of the environment, you can do this by changing the first line of
+the file. Then type (in the Anaconda Prompt):
+
+.. code-block:: console
+
+    conda env create -f environment.yml
+
+Then activate your environment:
+
+.. code-block:: console
+
+    conda activate cellpy
+
+
+``cellpy`` relies on a number of other python package and these need
+to be installed. Most of these packages are included when creating the environment
+based on the ``environment.yml`` file as outlined above.
+
+Basic dependencies
+~~~~~~~~~~~~~~~~~~
+
+In general, you need the typical scientific python pack, including
+
+- ``numpy``
+- ``scipy``
+- ``pandas``
+
+It is recommended that you at least install ``scipy`` before you install
+``cellpy`` (the main benefit being that you can use ``conda`` so that you
+do not have to hassle with missing C-compilers if you are on an Windows
+machine).
+Additional dependencies are:
+
+- ``pytables`` is needed for working with the hdf5 files (the cellpy-files):
+
+.. code-block:: console
+
+    conda install -c conda-forge pytables
+
+- ``lmfit`` is required to use some of the fitting routines in ``cellpy``:
+
+.. code-block:: console
+
+    conda install -c conda-forge lmfit
+
+- ``holoviz`` and ``plotly``: plotting library used in several of our example notebooks.
+
+- ``jupyter``: used for tutorial notebooks and in general very useful tool
+   for working with and sharing your ``cellpy`` results.
+
+For more details, I recommend that you look at the documentation of these
+packages (google it) and install them. You can most
+likely use the same method as for pytables *etc*.
+
+Additional requirements for .res files
+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+.res files from Arbin testers are  actually in a Microsoft Access format.
+For loading .res-files (possible also for other *to-be-implemented* file
+formats) you will thus also need a *Python ODBC bridge* (in addition to the
+requirements set in the ``setup.py`` file).
+I recommend `pyodbc <https://github.com/mkleehammer/pyodbc/wiki>`__ that
+can be installed from conda forge or using pip.
+
+.. code-block:: console
+
+    conda install -c conda-forge pyodbc
+
+Additionally, you need a driver or similar to help your ODBC bridge
+accessing it.
+
+**For Windows users:** if you do not have one of the
+most recent Office versions, you might not be allowed to install a driver
+of different bit than your office version is using (the installers can be found
+`here <https://www.microsoft.com/en-US/download/details.aspx?id=13255>`__).
+Also remark that the driver needs to be of the same bit as your Python
+(so, if you are using 32 bit Python, you will need the 32 bit driver).
+
+**For POSIX systems:** I have not found any suitable drivers. Instead,
+``cellpy`` will try to use ``mdbtools``\ to first export the data to
+temporary csv-files, and then import from those csv-file (using the
+``pandas`` library). You can install ``mdbtools`` using your systems
+preferred package manager (*e.g.* ``apt-get install mdbtools``).
+
+
+The cellpy configuration file
+.............................
+The paths to raw data, the cellpy data base file, file locations etc. are set in
+the ``.cellpy_prms_USER.conf`` file that is located in your home directory.
+
+To get the filepath to your config file (and other cellpy info), run:
+
+.. code-block:: console
+
+    cellpy info -l
+
+The config file is written in YAML format and it should be relatively easy to
+edit it in a text editor.
+
+Within the config file, the paths are the most important parts that need to
+be set up correctly. This tells ``cellpy`` where to find (and save) different files,
+such as the database file and raw data.
+
+Furthermore, the config file contains details about the database-file to be
+used for cell info and metadata (i.e. type and structure of the database file such
+as column headers etc.). For more details, see chapter on Configuring cellpy.
+
+
+The 'database' file
+...................
+The database file should contain information (cell name, type, mass loading etc.)
+on your cells, so that cellpy can find and link the test data to the provided
+metadata.
+
+The database file is also useful when working with the ``cellpy`` batch routine.
+
+
+Useful ``cellpy`` commands
+--------------------------
 
 To help installing and controlling your ``cellpy`` installation, a CLI
-is provided with four main commands, including ``info`` for getting
-information about your installation, and ``setup`` for helping you to
-set up your installation and writing a configuration file.
+(command-line-interface) is provided with several commands (including the already
+mentioned ``info`` for getting information about your installation, and
+``setup`` for helping you to set up your installation and writing a configuration file).
 
-To get more information, you can issue
+To get a list of these commands including some basic information, you can issue
 
-.. code:: bash
+.. code-block:: console
 
    cellpy --help
 
-This will out-put some (hopefully) helpful text
+This will output some (hopefully) helpful text
 
-.. code:: bash
+.. code-block:: console
 
     Usage: cellpy [OPTIONS] COMMAND [ARGS]...
 
     Options:
       --help  Show this message and exit.
 
     Commands:
@@ -213,109 +352,77 @@
       run    Run a cellpy process.
       serve  Start a Jupyter server
       setup  This will help you to setup cellpy.
 
 You can get information about the sub-commands by issuing -help after
 them also. For example, issuing
 
-.. code:: bash
+.. code-block:: console
 
    cellpy info --help
 
 gives
 
-.. code:: bash
+.. code-block:: console
 
-   Usage: cellpy info [OPTIONS]
+    Usage: cellpy info [OPTIONS]
 
-   Options:
+    Options:
      -v, --version    Print version information.
      -l, --configloc  Print full path to the config file.
      -p, --params     Dump all parameters to screen.
      -c, --check      Do a sanity check to see if things works as they should.
      --help           Show this message and exit.
 
-Using the ``cellpy`` command for your first time setup
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-
-After you have installed ``cellpy`` it is highly recommended that you
-create an appropriate configuration file and create folders for raw
-data, cellpy-files, logs, databases and output data (and inform
-``cellpy`` about it)
-
-.. code:: bash
-
-   cellpy setup -i
-
-The ``-i`` option makes sure that the setup is done interactively.
-The program will ask you about where specific folders are, *e.g.* where
-you would like to put your outputs and where your cell data files are
-located. If the folders dont exist, ``cellpy`` will try to create them.
-
-If you want to specify a root folder different from the default (your HOME
-folder), you can use the ``-d`` option *e.g.*
-``cellpy setup -i -d /Users/kingkong/cellpydir``
-
-.. hint::
-
-    If you don't choose the ``-i`` option and goes for accepting all the defaults,
-    you can always edit your configurations
-    directly in the cellpy configuration file (that should be located inside your
-    home directory, /~ in posix and c:\users\NAME in not-too-old windows).
-
-When you have answered all your questions, a configuration file will be
-made and saved to your home directory. You can always issue
-``cellpy info -l`` to find out where your configuration file is located
-(its written in YAML format and it should be relatively easy to edit it
-in a text editor)
 
 Running your first script
 -------------------------
 
 As with most software, you are encouraged to play a little with it. I
 hope there are some useful stuff in the code repository (for example in
 the `examples
 folder <https://github.com/jepegit/cellpy/tree/master/examples>`__).
 
-.. hint::
-    The ``cellpy pull`` command can assist in downloading
+.. hint:: The ``cellpy pull`` command can assist in downloading
     both examples and tests.
 
-Let's start by a trying to import ``cellpy`` in an interactive Python session.
+Start by trying to import ``cellpy`` in an interactive Python session.
 If you have an icon to press to start up the Python in interactive mode,
-do that (it could also be for example an ipython console or a
-Jupyter Notebook).
-You can also start an interactive Python session
-if you are in your terminal window of command window by just writing ``python``
-and pressing enter.
+do that (it could also be for example an ipython console or a Jupyter
+Notebook).
+You can also start an interactive Python session if you are in your
+terminal window of command window by just writing ``python`` and pressing
+enter.
+*Hint:* Remember to activate your cellpy (or whatever name you
+chose) environment.
 
 Once inside Python, try issuing ``import cellpy``. Hopefully you should not see
 any error-messages.
 
 .. code-block:: python
 
     Python 3.9.9 | packaged by conda-forge | (main, Dec 20 2021, 02:36:06)
     [MSC v.1929 64 bit (AMD64)] on win32
     Type "help", "copyright", "credits" or "license" for more information.
     >>> import cellpy
     >>>
 
 Nothing bad happened this time. If you got an error message, try to interpret
 it and check if you have skipped any steps in this tutorial. Maybe you are
-missing the ``box`` package? if so, go out of the Python interpreter if you
+missing the ``box`` package? If so, go out of the Python interpreter if you
 started it in your command window, or open another command window and write
 
-.. code:: bash
+.. code-block:: console
 
     pip install python-box
 
 and try again.
 
-Now let's try to be a bit more ambitious. Start up python again if you not
-still running it and try this:
+Now let's try to be a bit more ambitious. Start up python again if you are
+not still running it and try this:
 
 .. code-block:: python
 
     >>> from cellpy import prmreader
     >>> prmreader.info()
 
 The ``prmreader.info()`` command should print out information about your
@@ -324,26 +431,7 @@
 
 Try scrolling to find your own ``prms.Paths.rawdatadir``. Does it look
 right? These settings can be changed by either re-running the
 ``cellpy setup -i`` command (not in Python, but in the command window /
 terminal window). You probably need to use the ``--reset`` flag this time
 since it is not your first time running it).
 
-
-What next?
-----------
-
-For example: If you want to use the highly popular (?) ``cellpy.utils.batch``
-utility, you
-need to make (or copy from a friend) the "database" (an excel-file with
-appropriate headers in the first row) and make sure that all the paths
-are set up correctly in you cellpy configuration file.
-
-Or, for example: If you would like to do some interactive plotting of your
-data, try to install holoviz and use Jupyter Lab to make some fancy plots
-and dash-boards.
-
-And why not: make a script that goes through all your thousands of measured
-cells, extracts the life-time (e.g. number of cycles until the capacity
-has dropped below 80% of the average of the three first cycles), and plot
-this versus time the cell was put. And maybe color the data-points based
-on who was doing the experiment?
```

### Comparing `cellpy-0.4.3a3/docs/usage.rst` & `cellpy-1.0.0a0/docs/main_description/usage.rst`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 =====
 Usage
 =====
 
 1. Simple usage as a python library
------------------------------------
+===================================
 
 To use ``cellpy``, start with importing it::
 
     >>> import cellpy
 
 Let us define some variables::
 
@@ -56,15 +56,15 @@
 or tuning the data (*e.g.* ``split`` and ``merge``).
 
 Take a look at the index page (:doc:`modules <source/modules>`), some of
 the tutorials (:doc:`tutorials <basics>`) or notebook examples (:doc:`Example notebooks <notebooks>`).
 
 
 2. Convenience methods and tools
---------------------------------
+================================
 
 The easiest way to load a file is to use the ``cellpy.get`` method. It
 interprets the file-type from the file extension and automatically creates
 the step table as well as the summary table::
 
     >>> import cellpy
     >>> c = cellpy.get(r"C:\data\20141030_CELL_6_cc_01.res", mass=0.982)
@@ -72,15 +72,15 @@
     >>> # cellpy.get("cellpyfiles/20141030_CELL_6_cc_0.h5")
 
 
 There also exists a method that takes the raw-file name and the cellpy-file name
 as input and only loads the raw-file if the cellpy-file is older than the
 raw-file::
 
-    >>> c = cellreader.CellpyData()
+    >>> c = cellreader.CellpyCell()
     >>> raw_files = [rawfile_01, rawfile_02]
     >>> c.loadcell(raw_files, cellpy_file)
 
 ``cellpy`` contains a logger (the logs are saved in the cellpy logging
 directory as defined in the config file). You can set the log level
 (to the screen) by::
```

### Comparing `cellpy-0.4.3a3/docs/utils/batch.rst` & `cellpy-1.0.0a0/docs/examples_and_tutorials/utils/batch.rst`

 * *Files 18% similar despite different names*

```diff
@@ -1,35 +1,37 @@
+.. _utils-batch:
+
 Using the batch utilities
--------------------------
+=========================
 
 The steps given in this tutorial describes how to use the new version of the
 batch utility. The part presented here is chosen such that it resembles how
 the old utility worked. However, under the hood, the new batch utility is very
 different from the old. A more detailed guide will come soon.
 
 So, with that being said, here is the promised description.
 
 Starting (setting things up)
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+----------------------------
 
 A database
-..........
+~~~~~~~~~~
 Currently, the only supported "database" is Excel (yes, I am not kidding). So,
 there is definitely room for improvements if you would like to contribute to
 the code-base.
 
 The Excel work-book must contain a page called ``db_table``. And the top row
 of this page should consist of the correct headers as defined in your cellpy
 config file. You then have to give an identification name for the cells you
 would like to investigate in one of the columns labelled as batch columns (
 typically "b01", "b02", ..., "b07"). You can find an example of such an Excel
 work-book in the test-data.
 
 A tool for running the job
-..........................
+~~~~~~~~~~~~~~~~~~~~~~~~~~
 
 Jupyter Notebooks is the recommended "tool" for running the cellpy batch
 feature. The first step is to import the ``cellpy.utils.batch.Batch``
 class from ``cellpy``.  The ``Batch`` class is a utility class for
 pipe-lining batch processing of cell cycle data.
 
 
@@ -60,28 +62,28 @@
     b.experiment.all_in_memory = True  # store all data in memory, defaults to False
     b.save_cellpy_file = True
 
     b.force_raw_file = False
     b.force_cellpy_file = True
 
 Extracting meta-data
-~~~~~~~~~~~~~~~~~~~~
+--------------------
 
 The next step is to extract and collect the information needed from your data-base into a DataFrame,
 and create an appropriate folder structure (`outdir/project_name/batch_name/raw_data`)
 
 .. code-block:: python
 
     # load info from your db and write the journal pages
     b.create_journal()
     b.create_folder_structure()
 
 
 Processing data
-~~~~~~~~~~~~~~~
+---------------
 
 To run the processing, you should then use the convenience function ``update``. This function
 loads all your data-files and saves csv-files of the results.
 
 .. code-block:: python
 
     b.update()
@@ -93,25 +95,25 @@
 
     b.make_summaries()
     b.plot_summaries()
 
 Now it is time to relax and maybe drink a cup of coffee.
 
 Further investigations and analyses
-~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+-----------------------------------
 
 There are several paths to go from here. I recommend looking at the raw data
 for the different cells briefly to check if everything looks sensible.
 You can get the names of the different datasets (cells) by issuing:
 
 .. code-block:: python
 
     b.experiment.cell_names
 
-You can get the CellpyData-object for a given cell by writing:
+You can get the CellpyCell-object for a given cell by writing:
 
 .. code-block:: python
 
     cell = b.experiment.data[name_of_cell]
     plotutils.raw_plot(my_cell)
 
 If you want to investigate further, you can either use one of the available
```

### Comparing `cellpy-0.4.3a3/docs/utils/figures/tutorials_utils_plotting_fig1.png` & `cellpy-1.0.0a0/docs/examples_and_tutorials/utils/figures/tutorials_utils_plotting_fig1.png`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/docs/utils/figures/tutorials_utils_plotting_fig2.png` & `cellpy-1.0.0a0/docs/examples_and_tutorials/utils/figures/tutorials_utils_plotting_fig2.png`

 * *Files identical despite different names*

### Comparing `cellpy-0.4.3a3/docs/utils/ica.rst` & `cellpy-1.0.0a0/docs/examples_and_tutorials/utils/ica.rst`

 * *Files 14% similar despite different names*

```diff
@@ -1,13 +1,18 @@
+.. _utils-ica:
+
 Extracting ica data
--------------------
+===================
+
+.. note:: This chapter would benefit from some more love and care. Any help
+    on that would be highly appreciated.
 
 
 Example: get dq/dv data for selected cycles
-...........................................
+-------------------------------------------
 
 .. code:: python
 
     import matplotlib.pyplot as plt
     from cellpy.utils import ica
 
     v4, dqdv4 = ica.dqdv_cycle(
@@ -25,15 +30,15 @@
     )
 
     plt.plot(v4,dqdv4, label="cycle 4")
     plt.plot(v10, dqdv10, label="cycle 10")
     plt.legend()
 
 Example: get dq/dv data for selected cycles
-...........................................
+-------------------------------------------
 
 .. code:: python
 
     # assuming that b is a cellpy.utils.batch.Batch object
 
     data = b.experiment.data["20160805_test001_45_cc"]
     tidy_ica = ica.dqdv_frames(data)
```

### Comparing `cellpy-0.4.3a3/docs/utils/plotting.rst` & `cellpy-1.0.0a0/docs/examples_and_tutorials/utils/plotting.rst`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,14 @@
+.. _utils-plotting:
+
 Have a look at the data
------------------------
+=======================
+
+.. note:: This chapter would benefit from some more love and care. Any help
+    on that would be highly appreciated.
 
 Here are some examples how to get a peak at the data. If we need an
 interactive plot of the raw-data, we can use the ``plotutils.raw_plot``
 function. If we would like to see some statistics for some of the
 cycles, the ``plotutils.cycle_info_plot`` is your friend. Lets start by
 importing cellpy and the ``plotutils`` utility:
 
@@ -20,21 +25,21 @@
 
 Here we used the convenience method ``cellpy.get`` to load some
 example data. If everything went well, you will see an output approximately
 like this:
 
 .. parsed-literal::
 
-    (cellpy) - Making CellpyData class and setting prms
+    (cellpy) - Making CellpyCell class and setting prms
     (cellpy) - Loading cellpy-file: ../testdata/hdf5/20160805_test001_45_cc.h5
     (cellpy) - Setting mass: 0.8
     (cellpy) - Creating step table
     (cellpy) - Creating summary data
     (cellpy) - assuming cycling in anode half-cell (discharge before charge) mode
-    (cellpy) - Created CellpyData object
+    (cellpy) - Created CellpyCell object
 
 
 If you have ``holoviews`` installed, you can conjure an
 interactive figure:
 
 .. code:: ipython3
```

### Comparing `cellpy-0.4.3a3/docs/utils/tut_ocv_rlx.rst` & `cellpy-1.0.0a0/docs/examples_and_tutorials/utils/tut_ocv_rlx.rst`

 * *Files 22% similar despite different names*

```diff
@@ -1,7 +1,16 @@
+.. _ocv-relax:
+
+Open Circuit Relaxation
+=======================
+
+.. note:: This chapter would benefit from some more love and care. Any help
+    on that would be highly appreciated.
+
+
 Plotting selected Open Circuit Relaxation points
 ------------------------------------------------
 
 .. code:: python
 
     # assuming b is a cellpy.utils.batch.Batch object
```

### Comparing `cellpy-0.4.3a3/setup.py` & `cellpy-1.0.0a0/setup.py`

 * *Files 7% similar despite different names*

```diff
@@ -34,49 +34,39 @@
     ]
 )
 
 # TODO: update this
 requirements = [
     "scipy",
     "numpy>=1.16.4",
-    "pandas",
+    "pandas>=1.5.0",
     "python-box",
     "setuptools",
     "ruamel.yaml",
     "matplotlib",
     "openpyxl",
     "click",
     "PyGithub",
     "tqdm",
+    "pint",
     'pyodbc;platform_system=="windows"',
-    'sqlalchemy;platform_system=="windows"',
+    "sqlalchemy>=2.0.0",
     'sqlalchemy-access;platform_system=="windows"',
-    # 'pytables', # not available by pip
+    "python-dotenv",
+    "fabric",
+    # 'tables', # not available by pip
 ]
 
 test_requirements = [
-    "scipy",
-    "numpy>=1.16.4",
-    "pandas>=1.5.3",
-    "python-box",
-    "setuptools",
-    "ruamel.yaml",
-    "matplotlib",
     "lmfit",
-    "pyodbc",
-    "openpyxl",
-    "click",
-    "PyGithub",
-    # 'pytables', # not available by pip
     "pytest",
-    "tqdm",
 ]
 
-extra_req_batch = ["ipython", "jupyter"]
-extra_req_fit = ["lmfit", "matplotlib"]
+extra_req_batch = ["ipython", "jupyter", "plotly", "seaborn", "kaleido==0.1.*"]
+extra_req_fit = ["lmfit"]
 extra_req_all = extra_req_batch + extra_req_fit
 
 extra_requirements = {
     "batch": extra_req_batch,
     "fit": extra_req_fit,
     "all": extra_req_all,
 }
@@ -86,15 +76,15 @@
 
 user_dir = os.path.expanduser("~")
 
 version_ns = {}
 with open(os.path.join(here, name, "_version.py")) as f:
     exec(f.read(), {}, version_ns)
 
-description = "Extract and manipulate data from battery cell testers."
+description = "Extract and manipulate data from battery data testers."
 
 setup(
     name=name,
     version=version_ns["__version__"],
     description=description,
     long_description=readme + "\n\n" + history,
     author="Jan Petter Maehlen",
@@ -106,19 +96,19 @@
     entry_points={"console_scripts": ["cellpy=cellpy.cli:cli"]},
     include_package_data=True,
     install_requires=requirements,
     license="MIT license",
     zip_safe=False,
     keywords="cellpy",
     classifiers=[
-        "Development Status :: 3 - Alpha",
+        "Development Status :: 5 - Production/Stable",
         "Intended Audience :: Science/Research",
         "License :: OSI Approved :: MIT License",
         "Natural Language :: English",
-        "Programming Language :: Python :: 3.8",
         "Programming Language :: Python :: 3.9",
         "Programming Language :: Python :: 3.10",
+        "Programming Language :: Python :: 3.11",
     ],
     test_suite="tests",
     tests_require=test_requirements,
     extras_require=extra_requirements,
 )
```

