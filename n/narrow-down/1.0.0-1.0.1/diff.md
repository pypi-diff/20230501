# Comparing `tmp/narrow_down-1.0.0-cp37-abi3-win_amd64.whl.zip` & `tmp/narrow_down-1.0.1-cp37-abi3-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,20 +1,20 @@
-Zip file size: 228526 bytes, number of entries: 18
--rw-r--r--  4.6 unx     6655 b- defN 22-May-17 07:46 narrow_down-1.0.0.dist-info/METADATA
--rw-r--r--  4.6 unx       96 b- defN 22-May-17 07:46 narrow_down-1.0.0.dist-info/WHEEL
--rw-r--r--  4.6 unx      633 b- defN 22-May-17 07:46 narrow_down-1.0.0.dist-info/license_files/LICENSE.rst
--rw-r--r--  4.6 unx      662 b- defN 22-May-17 07:46 narrow_down/hash.py
--rw-r--r--  4.6 unx     1714 b- defN 22-May-17 07:46 narrow_down/proto/stored_document_pb2.py
--rw-r--r--  4.6 unx     3249 b- defN 22-May-17 07:46 narrow_down/proto/stored_document_pb2.pyi
--rw-r--r--  4.6 unx        0 b- defN 22-May-17 07:46 narrow_down/proto/__init__.py
--rw-r--r--  4.6 unx        0 b- defN 22-May-17 07:46 narrow_down/py.typed
--rw-r--r--  4.6 unx    13026 b- defN 22-May-17 07:46 narrow_down/scylladb.py
--rw-r--r--  4.6 unx    15922 b- defN 22-May-17 07:46 narrow_down/similarity_store.py
--rw-r--r--  4.6 unx     7282 b- defN 22-May-17 07:46 narrow_down/sqlite.py
--rw-r--r--  4.6 unx     9684 b- defN 22-May-17 07:46 narrow_down/storage.py
--rw-r--r--  4.6 unx    10210 b- defN 22-May-17 07:46 narrow_down/_minhash.py
--rw-r--r--  4.6 unx     1541 b- defN 22-May-17 07:46 narrow_down/_rust.pyi
--rw-r--r--  4.6 unx     2368 b- defN 22-May-17 07:46 narrow_down/_tokenize.py
--rw-r--r--  4.6 unx      208 b- defN 22-May-17 07:46 narrow_down/__init__.py
--rwxr-xr-x  4.6 unx   491520 b- defN 22-May-17 07:46 narrow_down/_rust.pyd
--rw-r--r--  4.6 unx     1480 b- defN 22-May-17 07:46 narrow_down-1.0.0.dist-info/RECORD
-18 files, 566250 bytes uncompressed, 226108 bytes compressed:  60.1%
+Zip file size: 263193 bytes, number of entries: 18
+-rw-r--r--  4.6 unx     6064 b- defN 23-Apr-30 14:27 narrow_down-1.0.1.dist-info/METADATA
+-rw-r--r--  4.6 unx       96 b- defN 23-Apr-30 14:27 narrow_down-1.0.1.dist-info/WHEEL
+-rw-r--r--  4.6 unx    10351 b- defN 23-Apr-30 14:27 narrow_down-1.0.1.dist-info/license_files/LICENSE
+-rw-r--r--  4.6 unx      662 b- defN 23-Apr-30 14:27 narrow_down/hash.py
+-rw-r--r--  4.6 unx     1712 b- defN 23-Apr-30 14:27 narrow_down/proto/stored_document_pb2.py
+-rw-r--r--  4.6 unx     3249 b- defN 23-Apr-30 14:27 narrow_down/proto/stored_document_pb2.pyi
+-rw-r--r--  4.6 unx       33 b- defN 23-Apr-30 14:27 narrow_down/proto/__init__.py
+-rw-r--r--  4.6 unx        0 b- defN 23-Apr-30 14:27 narrow_down/py.typed
+-rw-r--r--  4.6 unx    13138 b- defN 23-Apr-30 14:27 narrow_down/scylladb.py
+-rw-r--r--  4.6 unx    16368 b- defN 23-Apr-30 14:27 narrow_down/similarity_store.py
+-rw-r--r--  4.6 unx     7229 b- defN 23-Apr-30 14:27 narrow_down/sqlite.py
+-rw-r--r--  4.6 unx     9864 b- defN 23-Apr-30 14:27 narrow_down/storage.py
+-rw-r--r--  4.6 unx    10042 b- defN 23-Apr-30 14:27 narrow_down/_minhash.py
+-rw-r--r--  4.6 unx     1711 b- defN 23-Apr-30 14:27 narrow_down/_rust.pyi
+-rw-r--r--  4.6 unx     2365 b- defN 23-Apr-30 14:27 narrow_down/_tokenize.py
+-rw-r--r--  4.6 unx      237 b- defN 23-Apr-30 14:27 narrow_down/__init__.py
+-rwxr-xr-x  4.6 unx   557056 b- defN 23-Apr-30 14:27 narrow_down/_rust.pyd
+-rw-r--r--  4.6 unx     1479 b- defN 23-Apr-30 14:27 narrow_down-1.0.1.dist-info/RECORD
+18 files, 641656 bytes uncompressed, 260783 bytes compressed:  59.4%
```

## zipnote {}

```diff
@@ -1,14 +1,14 @@
-Filename: narrow_down-1.0.0.dist-info/METADATA
+Filename: narrow_down-1.0.1.dist-info/METADATA
 Comment: 
 
-Filename: narrow_down-1.0.0.dist-info/WHEEL
+Filename: narrow_down-1.0.1.dist-info/WHEEL
 Comment: 
 
-Filename: narrow_down-1.0.0.dist-info/license_files/LICENSE.rst
+Filename: narrow_down-1.0.1.dist-info/license_files/LICENSE
 Comment: 
 
 Filename: narrow_down/hash.py
 Comment: 
 
 Filename: narrow_down/proto/stored_document_pb2.py
 Comment: 
@@ -45,11 +45,11 @@
 
 Filename: narrow_down/__init__.py
 Comment: 
 
 Filename: narrow_down/_rust.pyd
 Comment: 
 
-Filename: narrow_down-1.0.0.dist-info/RECORD
+Filename: narrow_down-1.0.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## narrow_down/proto/stored_document_pb2.py

```diff
@@ -27,12 +27,11 @@
         "__module__": "proto.stored_document_pb2"
         # @@protoc_insertion_point(class_scope:stored_document.StoredDocumentProto)
     },
 )
 _sym_db.RegisterMessage(StoredDocumentProto)
 
 if _descriptor._USE_C_DESCRIPTORS == False:
-
     DESCRIPTOR._options = None
     _STOREDDOCUMENTPROTO._serialized_start = 49
     _STOREDDOCUMENTPROTO._serialized_end = 221
 # @@protoc_insertion_point(module_scope)
```

## narrow_down/proto/__init__.py

```diff
@@ -0,0 +1,3 @@
+00000000: 2222 2247 656e 6572 6174 6564 2070 726f  """Generated pro
+00000010: 746f 6275 6620 7479 7065 732e 2222 220d  tobuf types.""".
+00000020: 0a                                       .
```

## narrow_down/scylladb.py

```diff
@@ -23,15 +23,15 @@
     Based on https://stackoverflow.com/questions/49350346/how-to-wrap-custom-future-to-use-with-asyncio-in-python.
 
     Args:
         f: future to wrap
 
     Returns:
         And asyncio.Future object which can be awaited.
-    """  # noqa: E501
+    """
     loop = asyncio.get_event_loop()
     aio_future = loop.create_future()
 
     def on_result(result):
         loop.call_soon_threadsafe(aio_future.set_result, result)
 
     def on_error(exception, *_):
@@ -45,15 +45,15 @@
 class ScyllaDBStore(StorageBackend):
     """Storage backend for a SimilarityStore using ScyllaDB."""
 
     def __init__(
         self,
         cluster_or_session: Union[cassandra.cluster.Cluster, cassandra.cluster.Session],
         keyspace: str,
-        table_prefix: str = None,
+        table_prefix: Optional[str] = None,
     ) -> None:
         """Create a new empty or connect to an existing SQLite database.
 
         Args:
             cluster_or_session: Can be a cassandra cluster or a session object.
             keyspace: Name of the keyspace to use.
             table_prefix: A prefix to use for all table names in the database.
@@ -82,15 +82,17 @@
             self._scylla_session = self._scylla_cluster.connect()
         yield self._scylla_session
 
     async def _execute(self, session, query, parameters=None, timeout=None):
         """Execute a cassandra query with asyncio."""
         return await _wrap_future(
             session.execute_async(
-                query=query, parameters=parameters, timeout=timeout or cassandra.cluster._NOT_SET
+                query=query,
+                parameters=parameters,
+                timeout=timeout or cassandra.cluster._NOT_SET,  # pylint: disable=protected-access
             )
         )
 
     async def initialize(
         self,
     ) -> "ScyllaDBStore":
         """Initialize the tables in the SQLite database file.
@@ -180,39 +182,39 @@
 
         Returns:
             A string with the value. If the key does not exist or the storage is uninitialized
             None is returned.
 
         Raises:
             cassandra.DriverException: In case the database query fails for any reason.
-        """  # noqa: DAR401
+        """  # pylint: disable=missing-raises-doc
         with self._session() as session:
             try:
                 result_list = await self._execute(
                     session, self._prepared_statements["get_setting"], (key,)
                 )
                 return None if not result_list else result_list[0].value
             except KeyError as e:
                 if "get_setting" in e.args:
                     return None
                 raise  # Don't swallow unknown errors
 
-    async def insert_document(self, document: bytes, document_id: int = None) -> int:
+    async def insert_document(self, document: bytes, document_id: Optional[int] = None) -> int:
         """Add the data of a document to the storage and return its ID."""
         with self._session() as session:
             if document_id:
                 await self._execute(
                     session,
                     self._prepared_statements["set_doc"],
                     (document_id, document),
                 )
                 return document_id
             else:
                 for _ in range(10):
-                    doc_id = random.randint(a=0, b=2**32)
+                    doc_id = random.randint(a=0, b=2**32)  # noqa=S311
                     result = await self._execute(
                         session,
                         self._prepared_statements["set_doc_checked"],
                         (doc_id, document),
                     )
                     inserted_successfully = result[0].applied
                     if (
```

## narrow_down/similarity_store.py

```diff
@@ -1,11 +1,11 @@
 """High-level API for indexing and retrieval of documents."""
 import re
 import warnings
-from typing import Callable, Collection, Iterable, List, Union
+from typing import Callable, Collection, Iterable, List, Optional, Union
 
 from narrow_down import _minhash, _tokenize
 from narrow_down._minhash import MinhashLshConfig
 from narrow_down.storage import (
     InMemoryStore,
     StorageBackend,
     StorageLevel,
@@ -28,32 +28,33 @@
         "_lsh_config",
     )
 
     def __init__(self):  # noqa: D107  # Not meant to be called, therefore omitting docstring.
         warnings.warn(
             "The __init__ function is not meant to be called in isolation. "
             "To create a SimilarityStore object use the coroutine functions "
-            "SimilarityStore.create() or SimilarityStore.load_from_storage()."
+            "SimilarityStore.create() or SimilarityStore.load_from_storage().",
+            stacklevel=2,
         )
         self._minhasher: _minhash.MinHasher
         self._similarity_threshold: float
         self._lsh: _minhash.LSH
         self._storage: StorageBackend
         self._storage_level: StorageLevel
         self._tokenize: Union[str, Callable[[str], Collection[str]]]
         self._tokenize_callable: Callable[[str], Collection[str]]
         self._lsh_config: MinhashLshConfig
 
     @classmethod
     async def create(
         cls,
         *,
-        storage: StorageBackend = None,
+        storage: Optional[StorageBackend] = None,
         storage_level: StorageLevel = StorageLevel.Minimal,
-        tokenize: Union[str, Callable[[str], Collection[str]]] = None,
+        tokenize: Optional[Union[str, Callable[[str], Collection[str]]]] = None,
         max_false_negative_proba: float = 0.05,
         max_false_positive_proba: float = 0.05,
         similarity_threshold: float = 0.75,
     ) -> "SimilarityStore":
         """Create a new SimilarityStore object.
 
         Args:
@@ -92,26 +93,29 @@
             A new SimilarityStore object with already initialized storage.
 
         ..
           # noqa: DAR101 max_false_negative_proba
           # noqa: DAR101 max_false_positive_proba
           # noqa: DAR101 similarity_threshold
         """
+        # pylint: disable=protected-access
         obj = await cls._create_object_base(storage, storage_level, similarity_threshold, tokenize)
         obj._lsh_config = _minhash.find_optimal_config(
             jaccard_threshold=similarity_threshold,
             max_false_negative_proba=max_false_negative_proba,
             max_false_positive_proba=max_false_positive_proba,
         )
         await obj._initialize_storage()
         return obj
 
     @classmethod
     async def load_from_storage(
-        cls, storage: StorageBackend, tokenize: Union[str, Callable[[str], Collection[str]]] = None
+        cls,
+        storage: StorageBackend,
+        tokenize: Optional[Union[str, Callable[[str], Collection[str]]]] = None,
     ) -> "SimilarityStore":
         """Load a SimilarityStore object from already initialized storage.
 
         Args:
             storage: A StorageBackend object which must already have been initialized by a
                 SimilarityStore object before.
             tokenize: The tokenization function originally specified in the init when initializing
@@ -121,14 +125,15 @@
             A SimilarityStore object using the given storage backend and with the settings stored
             in the storage.
 
         Raises:
             TypeError: If settings in the storage are missing, corrupt or cannot be deserialized.
             ValueError: If the function specified with ``tokenize`` cannot be found.
         """
+        # pylint: disable=protected-access
         storage_level = StorageLevel(
             # Let it throw TypeError if None:
             int(await storage.query_setting("storage_level"))  # type: ignore
         )
         similarity_threshold = float(
             # Let it throw TypeError if None:
             await storage.query_setting("similarity_threshold")  # type: ignore
@@ -157,14 +162,15 @@
         return simstore
 
     @classmethod
     async def _create_object_base(
         cls, storage, storage_level, similarity_threshold, tokenize
     ) -> "SimilarityStore":
         """Create a new SimilarityStore object with the given attributes."""
+        # pylint: disable=protected-access
         obj = SimilarityStore.__new__(cls)
         obj._storage = storage or InMemoryStore()
         obj._storage_level = storage_level
         obj._similarity_threshold = similarity_threshold
         if isinstance(tokenize, str) or tokenize is None:
             obj._tokenize = tokenize or "word_ngrams(3)"
             obj._tokenize_callable = obj._get_tokenize_callable(obj._tokenize)
@@ -206,21 +212,26 @@
 
         Must be called exactly once for a new object. Should not be called when connecting to an
         existing database.
         """
         await self._storage.initialize()
         await self._storage.insert_setting("similarity_threshold", str(self._similarity_threshold))
         await self._storage.insert_setting("storage_level", str(self._storage_level.value))
-        await self._storage.insert_setting("tokenize", self._tokenize)
+        await self._storage.insert_setting("tokenize", self._tokenize)  # type: ignore
         await self._storage.insert_setting("lsh_config", self._lsh_config.to_json())
         self._minhasher = _minhash.MinHasher(n_hashes=self._lsh_config.n_hashes)
         self._lsh = _minhash.LSH(self._lsh_config, storage=self._storage)
 
     async def insert(
-        self, document: str, *, document_id: int = None, exact_part: str = None, data: str = None
+        self,
+        document: str,
+        *,
+        document_id: Optional[int] = None,
+        exact_part: Optional[str] = None,
+        data: Optional[str] = None,
     ) -> int:
         """Index a new document.
 
         Args:
             document: A document (as string to index).
             document_id: Optional ID to assign to the document.
             exact_part: Optional exact string to match when searching for the document.
@@ -266,24 +277,25 @@
         candidate_tokens = [set(self._tokenize_callable(c.document)) for c in candidates]
         tokens = set(tokens)
         true_jaccards = [_jaccard_similarity(tokens, ct) for ct in candidate_tokens]
         candidates = [
             c
             for jaccard, c in sorted(
                 filter(
-                    lambda t: t[0] >= self._similarity_threshold, zip(true_jaccards, candidates)
+                    lambda t: t[0] >= self._similarity_threshold,
+                    zip(true_jaccards, candidates),  # noqa=B905
                 ),
                 key=lambda t: (t[0], t[1].id_ or 0),
                 reverse=True,
             )
         ]
         return candidates
 
     async def query(
-        self, document: str, *, exact_part=None, validate: bool = None
+        self, document: str, *, exact_part: Optional[str] = None, validate: Optional[bool] = None
     ) -> Collection[StoredDocument]:
         """Query all similar documents.
 
         Args:
             document: A document for which to search similar items.
             exact_part: Part that should be exactly matched.
             validate: Whether to validate if the results are really above the similarity threshold.
@@ -298,15 +310,20 @@
         fingerprint = self._minhasher.minhash(tokens)
         candidates = await self._lsh.query(fingerprint=fingerprint, exact_part=exact_part)
         if (self._storage_level & StorageLevel.Document) and validate is not False:
             candidates = self._filter_candidates(candidates, tokens, exact_part)
         return candidates
 
     async def query_top_n(
-        self, n: int, document: str, *, exact_part=None, validate: bool = None
+        self,
+        n: int,
+        document: str,
+        *,
+        exact_part: Optional[str] = None,
+        validate: Optional[bool] = None,
     ) -> Collection[StoredDocument]:
         """Query the top n similar documents.
 
         Args:
             n: The number of similar documents to retrieve.
             document: A document for which to search similar items.
             exact_part: Part that should be exactly matched.
```

## narrow_down/sqlite.py

```diff
@@ -81,31 +81,30 @@
                 return setting[0]
             return None
         except sqlite3.OperationalError as e:
             if "no such table: settings" in e.args:
                 return None
             raise
 
-    async def insert_document(self, document: bytes, document_id: int = None) -> int:
+    async def insert_document(self, document: bytes, document_id: Optional[int] = None) -> int:
         """Add the data of a document to the storage and return its ID."""
         with self._connection as conn:
             if document_id:
                 conn.execute(
                     "INSERT INTO documents(id,doc) VALUES (:id,:doc) "
                     "ON CONFLICT(id) DO UPDATE SET doc=:doc",
                     dict(id=document_id, doc=document),
                 )
                 return document_id
             else:
                 cursor = conn.execute(
-                    "INSERT INTO documents(doc) VALUES (:doc) "
-                    "ON CONFLICT(id) DO UPDATE SET doc=:doc",
+                    "INSERT INTO documents(doc) VALUES (:doc)",
                     dict(doc=document),
                 )
-                return cursor.lastrowid
+                return cursor.lastrowid  # type: ignore  # (this always works if the insert works)
 
     async def query_document(self, document_id: int) -> bytes:
         """Get the data belonging to a document.
 
         Args:
             document_id: The id of the document. This ID is created and returned by the
                 `insert_document` method.
@@ -151,29 +150,28 @@
             conn.execute("DELETE FROM documents WHERE id=?", (document_id,))
 
     async def add_document_to_bucket(self, bucket_id: int, document_hash: int, document_id: int):
         """Link a document to a bucket."""
         partition = int(document_hash % self.partitions)
         with self._connection as conn:
             conn.execute(
-                f"INSERT INTO buckets_{partition}(bucket,hash,doc_id) VALUES (?,?,?)",  # noqa: S608
+                f"INSERT INTO buckets_{partition}(bucket,hash,doc_id) VALUES (?,?,?)",
                 (bucket_id, document_hash, document_id),
             )
 
     async def query_ids_from_bucket(self, bucket_id, document_hash: int) -> Iterable[int]:
         """Get all document IDs stored in a bucket for a certain hash value."""
         partition = int(document_hash % self.partitions)
         cursor = self._connection.execute(
-            f"SELECT doc_id FROM buckets_{partition} WHERE bucket=? AND hash=?",  # noqa: S608
+            f"SELECT doc_id FROM buckets_{partition} WHERE bucket=? AND hash=?",
             (bucket_id, document_hash),
         )
         return [r[0] for r in cursor.fetchall()]
 
     async def remove_id_from_bucket(self, bucket_id: int, document_hash: int, document_id: int):
         """Remove a document from a bucket."""
         with self._connection as conn:
             partition = int(document_hash % self.partitions)
             conn.execute(
-                f"DELETE FROM buckets_{partition} "  # noqa: S608
-                "WHERE bucket=? AND hash=? AND doc_id=?",
+                f"DELETE FROM buckets_{partition} " "WHERE bucket=? AND hash=? AND doc_id=?",
                 (bucket_id, document_hash, document_id),
             )
```

## narrow_down/storage.py

```diff
@@ -1,38 +1,37 @@
 """Base classes and interfaces for storage."""
 import asyncio
 import dataclasses
 import enum
-import pickle  # noqa
-from abc import ABC
+from abc import ABC, abstractmethod
 from dataclasses import dataclass
 from typing import Any, Dict, Iterable, List, NewType, Optional
 
 import numpy as np
 from numpy import typing as npt
 
 from narrow_down.proto.stored_document_pb2 import StoredDocumentProto
 
 from ._rust import RustMemoryStore
 
 
-class TooLowStorageLevel(Exception):
+class TooLowStorageLevel(Exception):  # noqa=N818
     """Raised if a feature is used for which a higher storage level is needed."""
 
 
 class StorageLevel(enum.Flag):
     """Detail level of document persistence."""
 
     Minimal = enum.auto()
     """Minimal storage level. Only store the necessary data to perform the search."""
     Fingerprint = enum.auto()
     """In addition to Minimal, also store the fingerprint, e.g. the Minhashes"""
     Document = enum.auto()
     """Store the whole inserted document internally."""
-    Full = Minimal | Fingerprint | Document
+    Full = Minimal | Fingerprint | Document  # pylint: disable=unsupported-binary-operation
     """Store everything."""
 
 
 Fingerprint = NewType("Fingerprint", npt.NDArray[np.uint32])
 """Type representing the result of a minhashing operation"""
 
 
@@ -114,75 +113,83 @@
         """Initialize the database.
 
         Returns:
             self
         """
         return self
 
+    @abstractmethod
     async def insert_setting(self, key: str, value: str):
         """Store a setting as key-value pair."""
         raise NotImplementedError
 
+    @abstractmethod
     async def query_setting(self, key: str) -> Optional[str]:
         """Query a setting with the given key.
 
         Args:
             key: The identifier of the setting
 
         Returns:
             A string with the value. If the key does not exist or the storage is uninitialized
             None is returned.
-        """  # noqa: DAR202,DAR401
+        """
         raise NotImplementedError
 
-    async def insert_document(self, document: bytes, document_id: int = None) -> int:
+    @abstractmethod
+    async def insert_document(self, document: bytes, document_id: Optional[int] = None) -> int:
         """Add the data of a document to the storage and return its ID."""
         raise NotImplementedError()
 
+    @abstractmethod
     async def query_document(self, document_id: int) -> bytes:
         """Get the data belonging to a document.
 
         Args:
             document_id: Key under which the data is stored.
 
         Returns:
             The document value for the given ID.
 
         Raises:
             KeyError: If no document with the given ID is stored.
-        """  # noqa: DAR202,DAR401
+        """
         raise NotImplementedError
 
     async def query_documents(self, document_ids: List[int]) -> List[bytes]:
         """Get the data belonging to multiple documents.
 
         Args:
             document_ids: Key under which the data is stored.
 
         Returns:
             The list of document values for the given IDs.
 
         Raises:
             KeyError: If no document was found for at least one of the ids.
-        """  # noqa: DAR401
+        """
         # Standard implementation of the base class. May be overloaded for specialization.
         return await asyncio.gather(*[self.query_document(doc_id) for doc_id in document_ids])
 
+    @abstractmethod
     async def remove_document(self, document_id: int):
         """Remove a document given by ID from the list of documents."""
         raise NotImplementedError()
 
+    @abstractmethod
     async def add_document_to_bucket(self, bucket_id: int, document_hash: int, document_id: int):
         """Link a document to a bucket."""
         raise NotImplementedError()
 
+    @abstractmethod
     async def query_ids_from_bucket(self, bucket_id: int, document_hash: int) -> Iterable[int]:
         """Get all document IDs stored in a bucket for a certain hash value."""
         raise NotImplementedError
 
+    @abstractmethod
     async def remove_id_from_bucket(self, bucket_id: int, document_hash: int, document_id: int):
         """Remove a document from a bucket."""
         raise NotImplementedError
 
 
 class InMemoryStore(StorageBackend):
     """Rust implementation of InMemoryStore."""
@@ -217,15 +224,15 @@
         """Store a setting as key-value pair."""
         self.rms.insert_setting(key, value)
 
     async def query_setting(self, key: str) -> Optional[str]:
         """Query a setting with the given key."""
         return self.rms.query_setting(key)
 
-    async def insert_document(self, document: bytes, document_id: int = None) -> int:
+    async def insert_document(self, document: bytes, document_id: Optional[int] = None) -> int:
         """Add the data of a document to the storage and return its ID."""
         return self.rms.insert_document(document, document_id)
 
     async def query_document(self, document_id: int) -> bytes:
         """Get the data belonging to a document.
 
         Args:
```

## narrow_down/_minhash.py

```diff
@@ -10,17 +10,17 @@
 import typing
 import warnings
 from dataclasses import dataclass
 from typing import Collection, Optional
 
 import numpy as np
 import numpy.typing as npt
-from scipy.integrate import quad as integrate
 
-from . import _rust, hash
+from . import _rust
+from . import hash as hash_
 from .storage import Fingerprint, StorageBackend, StorageLevel, StoredDocument, TooLowStorageLevel
 
 _MERSENNE_PRIME = np.uint32((1 << 32) - 1)
 
 
 @dataclass(frozen=True)
 class MinhashLshConfig:
@@ -44,14 +44,16 @@
         """Deserialize an object from a json string."""
         return cls(**json.loads(json_str))
 
 
 class MinHasher:
     """Classic Minhash algorithm."""
 
+    # pylint: disable=too-few-public-methods
+
     def __init__(
         self,
         n_hashes: int = 100,
         random_seed: Optional[int] = 42,
     ) -> None:
         """Prepare a Minhash object.
 
@@ -97,17 +99,17 @@
         storage: StorageBackend,
     ):
         """Create a new LSH object."""
         self._storage = storage
         self.n_hashes = lsh_config.n_hashes
         self.n_bands = lsh_config.n_bands
         self.rows_per_band = lsh_config.rows_per_band
-        self._hashfunc = hash.murmur3_32bit
+        self._hashfunc = hash_.murmur3_32bit
 
-    def _hash(self, arr: npt.NDArray, exact_part: str = None) -> int:
+    def _hash(self, arr: npt.NDArray, exact_part: Optional[str] = None) -> int:
         """Merge multiple hashes together to one hash."""
         if arr.dtype != np.uint32:
             # Other data types like the standard int64 have a different binary representation
             arr = arr.astype(np.uint32)
         if exact_part:
             return self._hashfunc(arr.tobytes(order="C") + b"-" + exact_part.encode("utf-8"))
         return self._hashfunc(arr.tobytes(order="C"))
@@ -169,15 +171,15 @@
                     bucket_id=band_number, document_hash=h, document_id=document_id
                 )
             )
         await asyncio.gather(*tasks)
         await self._storage.remove_document(document_id=document_id)
 
     async def query(
-        self, fingerprint: Fingerprint, *, exact_part: str = None
+        self, fingerprint: Fingerprint, *, exact_part: Optional[str] = None
     ) -> Collection[StoredDocument]:
         """Find all similar documents."""
         tasks = []
         for band_number in range(self.n_bands):
             start_index = band_number * self.rows_per_band
             h = self._hash(fingerprint[start_index : start_index + self.rows_per_band], exact_part)
             tasks.append(
@@ -185,15 +187,15 @@
             )
         candidates = set()
         for new_candidates in await asyncio.gather(*tasks):
             candidates.update(new_candidates)
         return await self._query_documents(list(candidates))
 
     async def query_top_n(
-        self, n, fingerprint: Fingerprint, *, exact_part: str = None
+        self, n, fingerprint: Fingerprint, *, exact_part: Optional[str] = None
     ) -> Collection[StoredDocument]:
         """Find n most similar documents."""
         tasks = []
         for band_number in range(self.n_bands):
             start_index = band_number * self.rows_per_band
             h = self._hash(fingerprint[start_index : start_index + self.rows_per_band], exact_part)
             tasks.append(
@@ -203,46 +205,51 @@
         for new_candidates in await asyncio.gather(*tasks):
             candidates.update(new_candidates)
         return await self._query_documents([c for c, _ in candidates.most_common(n)])
 
     async def _query_documents(self, doc_ids: typing.List[int]):
         """Fetch a document from the storage and deserialize it."""
         docs = await self._storage.query_documents(doc_ids)
-        return [StoredDocument.deserialize(doc, doc_id) for doc, doc_id in zip(docs, doc_ids)]
+        return [
+            StoredDocument.deserialize(doc, doc_id)
+            for doc, doc_id in zip(docs, doc_ids)  # noqa=B905
+        ]
 
 
 def find_optimal_config(
     jaccard_threshold: float, max_false_negative_proba: float, max_false_positive_proba: float
 ) -> MinhashLshConfig:
     """Find the optimal configuration given the provided target parameters."""
     num_perm = 16
+    max_num_permutations = 16384
     b, r = _params_given_false_negative_proba(jaccard_threshold, num_perm, max_false_negative_proba)
-    while _false_positive_probability(jaccard_threshold, b, r) > max_false_positive_proba:
-        if num_perm >= 16384:
-            warnings.warn("Unable to reach error thresholds. Taking the best value.")
+    while _rust.false_positive_probability(jaccard_threshold, b, r) > max_false_positive_proba:
+        if num_perm >= max_num_permutations:
+            warnings.warn("Unable to reach error thresholds. Taking the best value.", stacklevel=2)
             break
         num_perm *= 2
         b, r = _params_given_false_negative_proba(
             jaccard_threshold, num_perm, max_false_negative_proba
         )
 
     return MinhashLshConfig(n_hashes=num_perm, n_bands=b, rows_per_band=r)
 
 
 def _params_given_false_negative_proba(
     threshold: float, num_perm: int, max_false_negative_proba: float
 ):
     for b in range(1, num_perm + 1):
         r = num_perm // b
-        fn = _false_negative_probability(threshold, b, r)
+        fn = _rust.false_negative_probability(threshold, b, r)
         if fn <= max_false_negative_proba:
             return b, r
     warnings.warn(
         "Unable to reach max_false_negative_proba. Taking maximum number of bands to maximize "
-        "the number of candidates returned"
+        "the number of candidates returned",
+        stacklevel=2,
     )
     return num_perm, 1
 
 
 # def _params_given_false_positive_proba(
 #     threshold: float, num_perm: int, max_false_positive_proba: float
 # ):
@@ -252,17 +259,7 @@
 #         if fp <= max_false_positive_proba:
 #             return b, r
 #     warnings.warn(
 #         "Unable to reach max_false_positive_proba. Taking minimum number of bands to minimize "
 #         "false positives"
 #     )
 #     return 1, num_perm
-
-
-def _false_positive_probability(threshold: float, b: int, r: int) -> float:
-    a, err = integrate(lambda s: 1 - (1 - s ** float(r)) ** float(b), 0.0, threshold)
-    return a
-
-
-def _false_negative_probability(threshold: float, b: int, r: int) -> float:
-    a, err = integrate(lambda s: 1 - (1 - (1 - s ** float(r)) ** float(b)), threshold, 1.0)
-    return a
```

## narrow_down/_rust.pyi

```diff
@@ -13,22 +13,24 @@
     def to_file(self, file_path: str): ...
     @classmethod
     def deserialize(cls, msgpack: bytes) -> "RustMemoryStore": ...
     @classmethod
     def from_file(cls, file_path: str) -> "RustMemoryStore": ...
     def insert_setting(self, key: str, value: str): ...
     def query_setting(self, key: str) -> Optional[str]: ...
-    def insert_document(self, document: bytes, document_id: int = None) -> int: ...
+    def insert_document(self, document: bytes, document_id: Optional[int] = None) -> int: ...
     def query_document(self, document_id: int) -> bytes: ...
     def remove_document(self, document_id: int): ...
     def add_document_to_bucket(self, bucket_id: int, document_hash: int, document_id: int): ...
     def query_ids_from_bucket(self, bucket_id, document_hash: int) -> Iterable[int]: ...
     def remove_id_from_bucket(self, bucket_id: int, document_hash: int, document_id: int): ...
 
 def murmur3_32bit(s: Union[str, bytes]) -> int: ...
 def xxhash_32bit(s: Union[str, bytes]) -> int: ...
 def xxhash_64bit(s: Union[str, bytes]) -> int: ...
 def minhash(
     shingle_list: List[str], a: npt.NDArray[np.uint32], b: npt.NDArray[np.uint32]
 ) -> npt.NDArray[np.uint32]: ...
+def false_positive_probability(threshold: float, b: int, r: int) -> float: ...
+def false_negative_probability(threshold: float, b: int, r: int) -> float: ...
 def char_ngrams_bytes(s: bytes, n: int, pad_char: Optional[bytes]) -> Set[str]: ...
 def char_ngrams_str(s: str, n: int, pad_char: Optional[str]) -> Set[str]: ...
```

## narrow_down/_tokenize.py

```diff
@@ -18,15 +18,15 @@
         as-is is returned as the only element in the result set.
     """
     if not s:
         return set()
     words = s.split()
     if len(words) <= n:
         return {" ".join(words)}
-    return set(" ".join(words[i : i + n]) for i in range(len(words) - n + 1))
+    return {" ".join(words[i : i + n]) for i in range(len(words) - n + 1)}
 
 
 def char_ngrams(s: str, n: int, pad_char: str = "$") -> Set[str]:
     """Get all character n-grams contained in the string s.
 
     Args:
         s: String to analyze
```

## narrow_down/__init__.py

```diff
@@ -1,7 +1,7 @@
 """Top-level package for narrow-down."""
 
 __author__ = """Christian Krudewig"""
 __email__ = "chr1st1ank@krudewig-online.de"
-__version__ = "1.0.0"
+__version__ = "1.0.1"
 
-from . import hash, similarity_store, storage  # noqa
+from . import hash, similarity_store, storage  # pylint: disable=redefined-builtin
```

## Comparing `narrow_down-1.0.0.dist-info/METADATA` & `narrow_down-1.0.1.dist-info/METADATA`

 * *Files 16% similar despite different names*

```diff
@@ -1,77 +1,64 @@
 Metadata-Version: 2.1
 Name: narrow-down
-Version: 1.0.0
+Version: 1.0.1
 Classifier: Development Status :: 4 - Beta
 Classifier: Intended Audience :: Developers
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Natural Language :: English
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
-Requires-Dist: numpy~=1.18
-Requires-Dist: scipy
+Requires-Dist: numpy<1.22; python_version < '3.8'
+Requires-Dist: numpy; python_version >= '3.8'
 Requires-Dist: typing_extensions
-Requires-Dist: protobuf~=3.15
+Requires-Dist: protobuf>=3.15,<5.0
 Requires-Dist: types-protobuf
-Requires-Dist: pandas~=1.0; extra == 'experiments'
-Requires-Dist: tabulate; extra == 'experiments'
-Requires-Dist: tqdm; extra == 'experiments'
 Requires-Dist: scylla-driver; extra == 'scylladb'
 Requires-Dist: sphinx; extra == 'docs'
 Requires-Dist: myst-parser; extra == 'docs'
 Requires-Dist: nbconvert; extra == 'docs'
 Requires-Dist: furo; extra == 'docs'
 Requires-Dist: pre-commit; extra == 'dev'
-Requires-Dist: invoke; extra == 'dev'
-Requires-Dist: flake8~=3.9; extra == 'dev'
-Requires-Dist: flakehell; extra == 'dev'
-Requires-Dist: flake8-builtins; extra == 'dev'
-Requires-Dist: flake8-blind-except; extra == 'dev'
-Requires-Dist: flake8-logging-format; extra == 'dev'
-Requires-Dist: flake8-bugbear; extra == 'dev'
-Requires-Dist: flake8-annotations; extra == 'dev'
-Requires-Dist: flake8-docstrings; extra == 'dev'
-Requires-Dist: flake8-bandit; extra == 'dev'
-Requires-Dist: darglint~=1.8; extra == 'dev'
 Requires-Dist: isort; extra == 'dev'
-Requires-Dist: black~=22.3.0; extra == 'dev'
-Requires-Dist: safety; extra == 'dev'
+Requires-Dist: black; extra == 'dev'
 Requires-Dist: jupyter; extra == 'dev'
 Requires-Dist: nbqa; extra == 'dev'
 Requires-Dist: nox; extra == 'dev'
 Requires-Dist: mypy; extra == 'dev'
 Requires-Dist: mypy-protobuf; extra == 'dev'
-Requires-Dist: nbmake==1.2; extra == 'dev'
-Requires-Dist: bump2version~=1.0; extra == 'dev'
-Requires-Dist: pytest~=6.2; extra == 'dev'
+Requires-Dist: nbmake; extra == 'dev'
+Requires-Dist: bump2version; extra == 'dev'
+Requires-Dist: pytest; extra == 'dev'
 Requires-Dist: pytest-asyncio; extra == 'dev'
 Requires-Dist: pytest-benchmark; extra == 'dev'
 Requires-Dist: pytest-profiling; extra == 'dev'
-Requires-Dist: xdoctest~=0.15; extra == 'dev'
-Requires-Dist: coverage[toml]~=6.0; extra == 'dev'
-Requires-Dist: pytest-cov~=3.0; extra == 'dev'
-Requires-Dist: watchdog[watchmedo]~=2.1; extra == 'dev'
-Requires-Dist: flake8-pylint~=0.1; extra == 'dev'
+Requires-Dist: xdoctest; extra == 'dev'
+Requires-Dist: coverage[toml]; extra == 'dev'
+Requires-Dist: pytest-cov; extra == 'dev'
+Requires-Dist: watchdog[watchmedo]; extra == 'dev'
 Requires-Dist: protoc-wheel-0; extra == 'dev'
 Requires-Dist: narrow-down[scylladb,docs]; extra == 'dev'
-Provides-Extra: experiments
+Requires-Dist: pandas~=1.0; extra == 'experiments'
+Requires-Dist: tabulate; extra == 'experiments'
+Requires-Dist: tqdm; extra == 'experiments'
 Provides-Extra: scylladb
 Provides-Extra: docs
 Provides-Extra: dev
-License-File: LICENSE.rst
+Provides-Extra: experiments
+License-File: LICENSE
 Summary: Fast fuzzy text search
 Keywords: narrow-down,LSH,minhash
 Requires-Python: <3.11,>=3.7
 Description-Content-Type: text/markdown; charset=UTF-8; variant=GFM
+Project-URL: Bug Tracker, https://github.com/chr1st1ank/narrow-down/issues
 Project-URL: homepage, https://github.com/chr1st1ank/narrow-down
 Project-URL: documentation, https://chr1st1ank.github.io/narrow-down
-Project-URL: Bug Tracker, https://github.com/chr1st1ank/narrow-down/issues
 Project-URL: repository, https://github.com/chr1st1ank/narrow-down
 
 
 # Narrow Down - Efficient near-duplicate search
 
 
 <div align="center">
```

## Comparing `narrow_down-1.0.0.dist-info/RECORD` & `narrow_down-1.0.1.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,18 +1,18 @@
-narrow_down-1.0.0.dist-info/METADATA,sha256=EkF-xvazbSVgmYeaf8xMMwzLLAa6pTLcdpE1sZVaIzI,6655
-narrow_down-1.0.0.dist-info/WHEEL,sha256=B8HCSImhySWJs-pA66tvAUlaAB01lS_8C80JbXklLdI,96
-narrow_down-1.0.0.dist-info/license_files/LICENSE.rst,sha256=HsEEu-z1fpxqB78cQCw6rs_3Vaid_1brbb4aIJM2Boo,633
+narrow_down-1.0.1.dist-info/METADATA,sha256=8f1RVKvA-pK_HGpB4rF4rXPTib4GfKJSAWzA0KtM4xg,6064
+narrow_down-1.0.1.dist-info/WHEEL,sha256=4Z_1TfSRTY1rQBKTLxv_1GTHENUX5wUwdN2m4IaiA94,96
+narrow_down-1.0.1.dist-info/license_files/LICENSE,sha256=6z17VIVGasvYHytJb1latjfSeS4mggayfZnnk722dUk,10351
 narrow_down/hash.py,sha256=yXLpGGFV1HUBA0AcTYfp24qiiEC6xbv9py9hkXPT4qQ,662
-narrow_down/proto/stored_document_pb2.py,sha256=Qu6RnFte9V5METd8aglVTIwHwPCLaHmgwb86MFM2WSg,1714
+narrow_down/proto/stored_document_pb2.py,sha256=g8XC0kh1cn0jxozm7ebQpAL_Ynmcb7jolILYI3PthIg,1712
 narrow_down/proto/stored_document_pb2.pyi,sha256=nVmCajuqoUwfEXFPialnK7mhMhMgRT8zP9Sq4PtyUWk,3249
-narrow_down/proto/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+narrow_down/proto/__init__.py,sha256=kt1a-KIHy3Le6i0AlD-r3JlRC70QvHgUsneEUfJzwAI,33
 narrow_down/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-narrow_down/scylladb.py,sha256=3C6xHXoyDw1wmBh6Hp7F2vJe-PMu7nsVPRpkK0qhllo,13026
-narrow_down/similarity_store.py,sha256=Otok2Rp6L8ZxFYPYm4_bX_2NJrBDqNpZZ_NuLqJPMGo,15922
-narrow_down/sqlite.py,sha256=zoEUwvc4iYu-EqreKM2hkneDhi_eRQC8FAzhRUQKurA,7282
-narrow_down/storage.py,sha256=FucHxsdXu1YrvaiLocxnGuX7wKvb4QQag8M3R2CMCuE,9684
-narrow_down/_minhash.py,sha256=A505oJpv8hcJncp0muKxjlb2EP343r_eDi3inGsZacs,10210
-narrow_down/_rust.pyi,sha256=sZGK-dw71pAr0d_2oD53qyF9LI-n8UwSjGNmldGNHVE,1541
-narrow_down/_tokenize.py,sha256=h9aEuNLgqW8mIPghlbXLkWNEc6M7mrT-JAqUG5Pmx2M,2368
-narrow_down/__init__.py,sha256=CbA743wQYW1rUhJQPba0ysx1JNSgb0suRM47RDM_bw4,208
-narrow_down/_rust.pyd,sha256=2vkFWCckkPTnU0hkuJ6m6zfzytLlK6TD8K6J8xGjOSQ,491520
-narrow_down-1.0.0.dist-info/RECORD,,
+narrow_down/scylladb.py,sha256=Zw3k6uplYI-RlwMFkSYWExM8SFCF1J8dYVXlko-hKb8,13138
+narrow_down/similarity_store.py,sha256=eCoLx_evQCCfsEI3njR1Sf6hxuBt5SIXJ4xrnkOJ6j4,16368
+narrow_down/sqlite.py,sha256=ZpUvtdrHtS-kBQONvpNMrUXh0nH4hGrNnUXQPMdgdHQ,7229
+narrow_down/storage.py,sha256=pJWz23vLwf3BRVr7l-43F0SdvyQHAEehpViyw7zaffU,9864
+narrow_down/_minhash.py,sha256=gM9h_Nyrbj4gNCFlamX_Xkdaj-K_ReX5NHrtmFPzjhA,10042
+narrow_down/_rust.pyi,sha256=BCGPvuyS8zYn_c23wT1bC9IjR73ki9hDb-o1srHiDRc,1711
+narrow_down/_tokenize.py,sha256=O_11Q4EyHB7VK-IACq81iGni5ywpdl7NAkvnsDUqVUM,2365
+narrow_down/__init__.py,sha256=4CdIPLlXU-7WSG2__Ql9xU3rkR1nKQXW0uv1gBmzTLE,237
+narrow_down/_rust.pyd,sha256=bsZ3CIrwGCjTjFdSJNrC83F5mjZcmeyyvLBBZXzOlsQ,557056
+narrow_down-1.0.1.dist-info/RECORD,,
```

