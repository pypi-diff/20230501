# Comparing `tmp/torchrec_nightly-2023.4.7-py39-none-any.whl.zip` & `tmp/torchrec_nightly-2023.4.9-py39-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,139 +1,139 @@
-Zip file size: 331303 bytes, number of entries: 137
--rw-r--r--  2.0 unx      811 b- defN 23-Apr-07 11:26 torchrec/__init__.py
--rw-r--r--  2.0 unx     1638 b- defN 23-Apr-07 11:26 torchrec/streamable.py
--rw-r--r--  2.0 unx      854 b- defN 23-Apr-07 11:26 torchrec/types.py
--rw-r--r--  2.0 unx     1153 b- defN 23-Apr-07 11:26 torchrec/datasets/__init__.py
--rw-r--r--  2.0 unx    41469 b- defN 23-Apr-07 11:26 torchrec/datasets/criteo.py
--rw-r--r--  2.0 unx     4548 b- defN 23-Apr-07 11:26 torchrec/datasets/movielens.py
--rw-r--r--  2.0 unx     6539 b- defN 23-Apr-07 11:26 torchrec/datasets/random.py
--rw-r--r--  2.0 unx    10909 b- defN 23-Apr-07 11:26 torchrec/datasets/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-07 11:26 torchrec/datasets/scripts/__init__.py
--rw-r--r--  2.0 unx     2448 b- defN 23-Apr-07 11:26 torchrec/datasets/scripts/contiguous_preproc_criteo.py
--rw-r--r--  2.0 unx     2847 b- defN 23-Apr-07 11:26 torchrec/datasets/scripts/npy_preproc_criteo.py
--rw-r--r--  2.0 unx     3077 b- defN 23-Apr-07 11:26 torchrec/datasets/scripts/shuffle_preproc_criteo.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-07 11:26 torchrec/datasets/test_utils/__init__.py
--rw-r--r--  2.0 unx     5308 b- defN 23-Apr-07 11:26 torchrec/datasets/test_utils/criteo_test_utils.py
--rw-r--r--  2.0 unx     1912 b- defN 23-Apr-07 11:26 torchrec/distributed/__init__.py
--rw-r--r--  2.0 unx    36247 b- defN 23-Apr-07 11:26 torchrec/distributed/batched_embedding_kernel.py
--rw-r--r--  2.0 unx     2069 b- defN 23-Apr-07 11:26 torchrec/distributed/collective_utils.py
--rw-r--r--  2.0 unx     4925 b- defN 23-Apr-07 11:26 torchrec/distributed/comm.py
--rw-r--r--  2.0 unx    55820 b- defN 23-Apr-07 11:26 torchrec/distributed/comm_ops.py
--rw-r--r--  2.0 unx    35423 b- defN 23-Apr-07 11:26 torchrec/distributed/dist_data.py
--rw-r--r--  2.0 unx    29817 b- defN 23-Apr-07 11:26 torchrec/distributed/embedding.py
--rw-r--r--  2.0 unx     3872 b- defN 23-Apr-07 11:26 torchrec/distributed/embedding_kernel.py
--rw-r--r--  2.0 unx    26074 b- defN 23-Apr-07 11:26 torchrec/distributed/embedding_lookup.py
--rw-r--r--  2.0 unx    14951 b- defN 23-Apr-07 11:26 torchrec/distributed/embedding_sharding.py
--rw-r--r--  2.0 unx    37089 b- defN 23-Apr-07 11:26 torchrec/distributed/embedding_tower_sharding.py
--rw-r--r--  2.0 unx    15021 b- defN 23-Apr-07 11:26 torchrec/distributed/embedding_types.py
--rw-r--r--  2.0 unx    34339 b- defN 23-Apr-07 11:26 torchrec/distributed/embeddingbag.py
--rw-r--r--  2.0 unx     7129 b- defN 23-Apr-07 11:26 torchrec/distributed/fbgemm_qcomm_codec.py
--rw-r--r--  2.0 unx     5273 b- defN 23-Apr-07 11:26 torchrec/distributed/fused_embedding.py
--rw-r--r--  2.0 unx     5110 b- defN 23-Apr-07 11:26 torchrec/distributed/fused_embeddingbag.py
--rw-r--r--  2.0 unx     3807 b- defN 23-Apr-07 11:26 torchrec/distributed/grouped_position_weighted.py
--rw-r--r--  2.0 unx    19345 b- defN 23-Apr-07 11:26 torchrec/distributed/model_parallel.py
--rw-r--r--  2.0 unx    12699 b- defN 23-Apr-07 11:26 torchrec/distributed/quant_embedding.py
--rw-r--r--  2.0 unx    11122 b- defN 23-Apr-07 11:26 torchrec/distributed/quant_embedding_kernel.py
--rw-r--r--  2.0 unx     9878 b- defN 23-Apr-07 11:26 torchrec/distributed/quant_embeddingbag.py
--rw-r--r--  2.0 unx     8710 b- defN 23-Apr-07 11:26 torchrec/distributed/shard.py
--rw-r--r--  2.0 unx    19218 b- defN 23-Apr-07 11:26 torchrec/distributed/sharding_plan.py
--rw-r--r--  2.0 unx    22330 b- defN 23-Apr-07 11:26 torchrec/distributed/train_pipeline.py
--rw-r--r--  2.0 unx    24913 b- defN 23-Apr-07 11:26 torchrec/distributed/types.py
--rw-r--r--  2.0 unx    11373 b- defN 23-Apr-07 11:26 torchrec/distributed/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-07 11:26 torchrec/distributed/composable/__init__.py
--rw-r--r--  2.0 unx     3207 b- defN 23-Apr-07 11:26 torchrec/distributed/composable/table_batched_embedding_slice.py
--rw-r--r--  2.0 unx     1025 b- defN 23-Apr-07 11:26 torchrec/distributed/planner/__init__.py
--rw-r--r--  2.0 unx     3135 b- defN 23-Apr-07 11:26 torchrec/distributed/planner/constants.py
--rw-r--r--  2.0 unx    10318 b- defN 23-Apr-07 11:26 torchrec/distributed/planner/enumerators.py
--rw-r--r--  2.0 unx    12103 b- defN 23-Apr-07 11:26 torchrec/distributed/planner/partitioners.py
--rw-r--r--  2.0 unx      824 b- defN 23-Apr-07 11:26 torchrec/distributed/planner/perf_models.py
--rw-r--r--  2.0 unx    11869 b- defN 23-Apr-07 11:26 torchrec/distributed/planner/planners.py
--rw-r--r--  2.0 unx    11094 b- defN 23-Apr-07 11:26 torchrec/distributed/planner/proposers.py
--rw-r--r--  2.0 unx    34788 b- defN 23-Apr-07 11:26 torchrec/distributed/planner/shard_estimators.py
--rw-r--r--  2.0 unx    21410 b- defN 23-Apr-07 11:26 torchrec/distributed/planner/stats.py
--rw-r--r--  2.0 unx     9125 b- defN 23-Apr-07 11:26 torchrec/distributed/planner/storage_reservations.py
--rw-r--r--  2.0 unx    12611 b- defN 23-Apr-07 11:26 torchrec/distributed/planner/types.py
--rw-r--r--  2.0 unx     1119 b- defN 23-Apr-07 11:26 torchrec/distributed/planner/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-07 11:26 torchrec/distributed/sharding/__init__.py
--rw-r--r--  2.0 unx     2539 b- defN 23-Apr-07 11:26 torchrec/distributed/sharding/cw_sequence_sharding.py
--rw-r--r--  2.0 unx     9519 b- defN 23-Apr-07 11:26 torchrec/distributed/sharding/cw_sharding.py
--rw-r--r--  2.0 unx     2802 b- defN 23-Apr-07 11:26 torchrec/distributed/sharding/dp_sequence_sharding.py
--rw-r--r--  2.0 unx     7452 b- defN 23-Apr-07 11:26 torchrec/distributed/sharding/dp_sharding.py
--rw-r--r--  2.0 unx     5041 b- defN 23-Apr-07 11:26 torchrec/distributed/sharding/rw_sequence_sharding.py
--rw-r--r--  2.0 unx    12850 b- defN 23-Apr-07 11:26 torchrec/distributed/sharding/rw_sharding.py
--rw-r--r--  2.0 unx     3114 b- defN 23-Apr-07 11:26 torchrec/distributed/sharding/sequence_sharding.py
--rw-r--r--  2.0 unx     7620 b- defN 23-Apr-07 11:26 torchrec/distributed/sharding/tw_sequence_sharding.py
--rw-r--r--  2.0 unx    16102 b- defN 23-Apr-07 11:26 torchrec/distributed/sharding/tw_sharding.py
--rw-r--r--  2.0 unx     1284 b- defN 23-Apr-07 11:26 torchrec/distributed/sharding/twcw_sharding.py
--rw-r--r--  2.0 unx    19840 b- defN 23-Apr-07 11:26 torchrec/distributed/sharding/twrw_sharding.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-07 11:26 torchrec/distributed/test_utils/__init__.py
--rw-r--r--  2.0 unx     8078 b- defN 23-Apr-07 11:26 torchrec/distributed/test_utils/infer_utils.py
--rw-r--r--  2.0 unx     4868 b- defN 23-Apr-07 11:26 torchrec/distributed/test_utils/multi_process.py
--rw-r--r--  2.0 unx    33925 b- defN 23-Apr-07 11:26 torchrec/distributed/test_utils/test_model.py
--rw-r--r--  2.0 unx    11193 b- defN 23-Apr-07 11:26 torchrec/distributed/test_utils/test_model_parallel.py
--rw-r--r--  2.0 unx    25075 b- defN 23-Apr-07 11:26 torchrec/distributed/test_utils/test_model_parallel_base.py
--rw-r--r--  2.0 unx    15367 b- defN 23-Apr-07 11:26 torchrec/distributed/test_utils/test_sharding.py
--rw-r--r--  2.0 unx      422 b- defN 23-Apr-07 11:26 torchrec/fx/__init__.py
--rw-r--r--  2.0 unx     6477 b- defN 23-Apr-07 11:26 torchrec/fx/tracer.py
--rw-r--r--  2.0 unx     3839 b- defN 23-Apr-07 11:26 torchrec/fx/utils.py
--rw-r--r--  2.0 unx     1223 b- defN 23-Apr-07 11:26 torchrec/inference/__init__.py
--rw-r--r--  2.0 unx     3614 b- defN 23-Apr-07 11:26 torchrec/inference/client.py
--rw-r--r--  2.0 unx     3957 b- defN 23-Apr-07 11:26 torchrec/inference/model_packager.py
--rw-r--r--  2.0 unx     7834 b- defN 23-Apr-07 11:26 torchrec/inference/modules.py
--rw-r--r--  2.0 unx     3797 b- defN 23-Apr-07 11:26 torchrec/inference/state_dict_transform.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-07 11:26 torchrec/metrics/__init__.py
--rw-r--r--  2.0 unx    11254 b- defN 23-Apr-07 11:26 torchrec/metrics/auc.py
--rw-r--r--  2.0 unx     3703 b- defN 23-Apr-07 11:26 torchrec/metrics/calibration.py
--rw-r--r--  2.0 unx     3465 b- defN 23-Apr-07 11:26 torchrec/metrics/ctr.py
--rw-r--r--  2.0 unx     3836 b- defN 23-Apr-07 11:26 torchrec/metrics/mae.py
--rw-r--r--  2.0 unx    17448 b- defN 23-Apr-07 11:26 torchrec/metrics/metric_module.py
--rw-r--r--  2.0 unx     6001 b- defN 23-Apr-07 11:26 torchrec/metrics/metrics_config.py
--rw-r--r--  2.0 unx     3543 b- defN 23-Apr-07 11:26 torchrec/metrics/metrics_namespace.py
--rw-r--r--  2.0 unx     3904 b- defN 23-Apr-07 11:26 torchrec/metrics/model_utils.py
--rw-r--r--  2.0 unx     4631 b- defN 23-Apr-07 11:26 torchrec/metrics/mse.py
--rw-r--r--  2.0 unx     5605 b- defN 23-Apr-07 11:26 torchrec/metrics/multiclass_recall.py
--rw-r--r--  2.0 unx     6811 b- defN 23-Apr-07 11:26 torchrec/metrics/ne.py
--rw-r--r--  2.0 unx    30635 b- defN 23-Apr-07 11:26 torchrec/metrics/rec_metric.py
--rw-r--r--  2.0 unx     6057 b- defN 23-Apr-07 11:26 torchrec/metrics/throughput.py
--rw-r--r--  2.0 unx    10622 b- defN 23-Apr-07 11:26 torchrec/metrics/tower_qps.py
--rw-r--r--  2.0 unx     2867 b- defN 23-Apr-07 11:26 torchrec/metrics/weighted_avg.py
--rw-r--r--  2.0 unx    16441 b- defN 23-Apr-07 11:26 torchrec/metrics/test_utils/__init__.py
--rw-r--r--  2.0 unx      913 b- defN 23-Apr-07 11:26 torchrec/models/__init__.py
--rw-r--r--  2.0 unx    11325 b- defN 23-Apr-07 11:26 torchrec/models/deepfm.py
--rw-r--r--  2.0 unx    29989 b- defN 23-Apr-07 11:26 torchrec/models/dlrm.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-07 11:26 torchrec/models/experimental/__init__.py
--rw-r--r--  2.0 unx     9823 b- defN 23-Apr-07 11:26 torchrec/models/experimental/test_transformerdlrm.py
--rw-r--r--  2.0 unx     7434 b- defN 23-Apr-07 11:26 torchrec/models/experimental/transformerdlrm.py
--rw-r--r--  2.0 unx     1179 b- defN 23-Apr-07 11:26 torchrec/modules/__init__.py
--rw-r--r--  2.0 unx     1456 b- defN 23-Apr-07 11:26 torchrec/modules/activation.py
--rw-r--r--  2.0 unx    15163 b- defN 23-Apr-07 11:26 torchrec/modules/crossnet.py
--rw-r--r--  2.0 unx     8415 b- defN 23-Apr-07 11:26 torchrec/modules/deepfm.py
--rw-r--r--  2.0 unx     5131 b- defN 23-Apr-07 11:26 torchrec/modules/embedding_configs.py
--rw-r--r--  2.0 unx    12822 b- defN 23-Apr-07 11:26 torchrec/modules/embedding_modules.py
--rw-r--r--  2.0 unx     4858 b- defN 23-Apr-07 11:26 torchrec/modules/embedding_tower.py
--rw-r--r--  2.0 unx    12103 b- defN 23-Apr-07 11:26 torchrec/modules/feature_processor.py
--rw-r--r--  2.0 unx    31184 b- defN 23-Apr-07 11:26 torchrec/modules/fused_embedding_modules.py
--rw-r--r--  2.0 unx    10696 b- defN 23-Apr-07 11:26 torchrec/modules/lazy_extension.py
--rw-r--r--  2.0 unx     6309 b- defN 23-Apr-07 11:26 torchrec/modules/mlp.py
--rw-r--r--  2.0 unx     4022 b- defN 23-Apr-07 11:26 torchrec/modules/utils.py
--rw-r--r--  2.0 unx     1639 b- defN 23-Apr-07 11:26 torchrec/optim/__init__.py
--rw-r--r--  2.0 unx     2012 b- defN 23-Apr-07 11:26 torchrec/optim/apply_optimizer_in_backward.py
--rw-r--r--  2.0 unx     1569 b- defN 23-Apr-07 11:26 torchrec/optim/clipping.py
--rw-r--r--  2.0 unx     1353 b- defN 23-Apr-07 11:26 torchrec/optim/fused.py
--rw-r--r--  2.0 unx    16069 b- defN 23-Apr-07 11:26 torchrec/optim/keyed.py
--rw-r--r--  2.0 unx     4420 b- defN 23-Apr-07 11:26 torchrec/optim/optimizers.py
--rw-r--r--  2.0 unx     7405 b- defN 23-Apr-07 11:26 torchrec/optim/rowwise_adagrad.py
--rw-r--r--  2.0 unx     4865 b- defN 23-Apr-07 11:26 torchrec/optim/warmup.py
--rw-r--r--  2.0 unx      560 b- defN 23-Apr-07 11:26 torchrec/optim/test_utils/__init__.py
--rw-r--r--  2.0 unx     1140 b- defN 23-Apr-07 11:26 torchrec/quant/__init__.py
--rw-r--r--  2.0 unx    21598 b- defN 23-Apr-07 11:26 torchrec/quant/embedding_modules.py
--rw-r--r--  2.0 unx     3691 b- defN 23-Apr-07 11:26 torchrec/quant/utils.py
--rw-r--r--  2.0 unx     1163 b- defN 23-Apr-07 11:26 torchrec/sparse/__init__.py
--rw-r--r--  2.0 unx    52592 b- defN 23-Apr-07 11:26 torchrec/sparse/jagged_tensor.py
--rw-r--r--  2.0 unx     1430 b- defN 23-Apr-07 11:26 torchrec/sparse/test_utils/__init__.py
--rw-r--r--  2.0 unx     5661 b- defN 23-Apr-07 11:26 torchrec/test_utils/__init__.py
--rw-r--r--  2.0 unx     1530 b- defN 23-Apr-07 11:30 torchrec_nightly-2023.4.7.dist-info/LICENSE
--rw-r--r--  2.0 unx     5011 b- defN 23-Apr-07 11:30 torchrec_nightly-2023.4.7.dist-info/METADATA
--rw-r--r--  2.0 unx       93 b- defN 23-Apr-07 11:30 torchrec_nightly-2023.4.7.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 23-Apr-07 11:30 torchrec_nightly-2023.4.7.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    12787 b- defN 23-Apr-07 11:30 torchrec_nightly-2023.4.7.dist-info/RECORD
-137 files, 1346769 bytes uncompressed, 310835 bytes compressed:  76.9%
+Zip file size: 331854 bytes, number of entries: 137
+-rw-r--r--  2.0 unx      811 b- defN 23-Apr-09 11:33 torchrec/__init__.py
+-rw-r--r--  2.0 unx     1638 b- defN 23-Apr-09 11:33 torchrec/streamable.py
+-rw-r--r--  2.0 unx      854 b- defN 23-Apr-09 11:33 torchrec/types.py
+-rw-r--r--  2.0 unx     1153 b- defN 23-Apr-09 11:33 torchrec/datasets/__init__.py
+-rw-r--r--  2.0 unx    41469 b- defN 23-Apr-09 11:33 torchrec/datasets/criteo.py
+-rw-r--r--  2.0 unx     4548 b- defN 23-Apr-09 11:33 torchrec/datasets/movielens.py
+-rw-r--r--  2.0 unx     6539 b- defN 23-Apr-09 11:33 torchrec/datasets/random.py
+-rw-r--r--  2.0 unx    10909 b- defN 23-Apr-09 11:33 torchrec/datasets/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-09 11:33 torchrec/datasets/scripts/__init__.py
+-rw-r--r--  2.0 unx     2448 b- defN 23-Apr-09 11:33 torchrec/datasets/scripts/contiguous_preproc_criteo.py
+-rw-r--r--  2.0 unx     2847 b- defN 23-Apr-09 11:33 torchrec/datasets/scripts/npy_preproc_criteo.py
+-rw-r--r--  2.0 unx     3077 b- defN 23-Apr-09 11:33 torchrec/datasets/scripts/shuffle_preproc_criteo.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-09 11:33 torchrec/datasets/test_utils/__init__.py
+-rw-r--r--  2.0 unx     5308 b- defN 23-Apr-09 11:33 torchrec/datasets/test_utils/criteo_test_utils.py
+-rw-r--r--  2.0 unx     1912 b- defN 23-Apr-09 11:33 torchrec/distributed/__init__.py
+-rw-r--r--  2.0 unx    36247 b- defN 23-Apr-09 11:33 torchrec/distributed/batched_embedding_kernel.py
+-rw-r--r--  2.0 unx     2069 b- defN 23-Apr-09 11:33 torchrec/distributed/collective_utils.py
+-rw-r--r--  2.0 unx     4925 b- defN 23-Apr-09 11:33 torchrec/distributed/comm.py
+-rw-r--r--  2.0 unx    55820 b- defN 23-Apr-09 11:33 torchrec/distributed/comm_ops.py
+-rw-r--r--  2.0 unx    35423 b- defN 23-Apr-09 11:33 torchrec/distributed/dist_data.py
+-rw-r--r--  2.0 unx    29817 b- defN 23-Apr-09 11:33 torchrec/distributed/embedding.py
+-rw-r--r--  2.0 unx     3872 b- defN 23-Apr-09 11:33 torchrec/distributed/embedding_kernel.py
+-rw-r--r--  2.0 unx    26074 b- defN 23-Apr-09 11:33 torchrec/distributed/embedding_lookup.py
+-rw-r--r--  2.0 unx    14951 b- defN 23-Apr-09 11:33 torchrec/distributed/embedding_sharding.py
+-rw-r--r--  2.0 unx    37089 b- defN 23-Apr-09 11:33 torchrec/distributed/embedding_tower_sharding.py
+-rw-r--r--  2.0 unx    15021 b- defN 23-Apr-09 11:33 torchrec/distributed/embedding_types.py
+-rw-r--r--  2.0 unx    34339 b- defN 23-Apr-09 11:33 torchrec/distributed/embeddingbag.py
+-rw-r--r--  2.0 unx     7129 b- defN 23-Apr-09 11:33 torchrec/distributed/fbgemm_qcomm_codec.py
+-rw-r--r--  2.0 unx     5273 b- defN 23-Apr-09 11:33 torchrec/distributed/fused_embedding.py
+-rw-r--r--  2.0 unx     5110 b- defN 23-Apr-09 11:33 torchrec/distributed/fused_embeddingbag.py
+-rw-r--r--  2.0 unx     3807 b- defN 23-Apr-09 11:33 torchrec/distributed/grouped_position_weighted.py
+-rw-r--r--  2.0 unx    19345 b- defN 23-Apr-09 11:33 torchrec/distributed/model_parallel.py
+-rw-r--r--  2.0 unx    12699 b- defN 23-Apr-09 11:33 torchrec/distributed/quant_embedding.py
+-rw-r--r--  2.0 unx    11122 b- defN 23-Apr-09 11:33 torchrec/distributed/quant_embedding_kernel.py
+-rw-r--r--  2.0 unx     9878 b- defN 23-Apr-09 11:33 torchrec/distributed/quant_embeddingbag.py
+-rw-r--r--  2.0 unx     8710 b- defN 23-Apr-09 11:33 torchrec/distributed/shard.py
+-rw-r--r--  2.0 unx    19218 b- defN 23-Apr-09 11:33 torchrec/distributed/sharding_plan.py
+-rw-r--r--  2.0 unx    22330 b- defN 23-Apr-09 11:33 torchrec/distributed/train_pipeline.py
+-rw-r--r--  2.0 unx    24927 b- defN 23-Apr-09 11:33 torchrec/distributed/types.py
+-rw-r--r--  2.0 unx    11373 b- defN 23-Apr-09 11:33 torchrec/distributed/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-09 11:33 torchrec/distributed/composable/__init__.py
+-rw-r--r--  2.0 unx     3207 b- defN 23-Apr-09 11:33 torchrec/distributed/composable/table_batched_embedding_slice.py
+-rw-r--r--  2.0 unx     1025 b- defN 23-Apr-09 11:33 torchrec/distributed/planner/__init__.py
+-rw-r--r--  2.0 unx     3135 b- defN 23-Apr-09 11:33 torchrec/distributed/planner/constants.py
+-rw-r--r--  2.0 unx    10318 b- defN 23-Apr-09 11:33 torchrec/distributed/planner/enumerators.py
+-rw-r--r--  2.0 unx    12103 b- defN 23-Apr-09 11:33 torchrec/distributed/planner/partitioners.py
+-rw-r--r--  2.0 unx      824 b- defN 23-Apr-09 11:33 torchrec/distributed/planner/perf_models.py
+-rw-r--r--  2.0 unx    11869 b- defN 23-Apr-09 11:33 torchrec/distributed/planner/planners.py
+-rw-r--r--  2.0 unx    11094 b- defN 23-Apr-09 11:33 torchrec/distributed/planner/proposers.py
+-rw-r--r--  2.0 unx    40173 b- defN 23-Apr-09 11:33 torchrec/distributed/planner/shard_estimators.py
+-rw-r--r--  2.0 unx    21410 b- defN 23-Apr-09 11:33 torchrec/distributed/planner/stats.py
+-rw-r--r--  2.0 unx     9125 b- defN 23-Apr-09 11:33 torchrec/distributed/planner/storage_reservations.py
+-rw-r--r--  2.0 unx    12611 b- defN 23-Apr-09 11:33 torchrec/distributed/planner/types.py
+-rw-r--r--  2.0 unx     1119 b- defN 23-Apr-09 11:33 torchrec/distributed/planner/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-09 11:33 torchrec/distributed/sharding/__init__.py
+-rw-r--r--  2.0 unx     2539 b- defN 23-Apr-09 11:33 torchrec/distributed/sharding/cw_sequence_sharding.py
+-rw-r--r--  2.0 unx     9519 b- defN 23-Apr-09 11:33 torchrec/distributed/sharding/cw_sharding.py
+-rw-r--r--  2.0 unx     2802 b- defN 23-Apr-09 11:33 torchrec/distributed/sharding/dp_sequence_sharding.py
+-rw-r--r--  2.0 unx     7452 b- defN 23-Apr-09 11:33 torchrec/distributed/sharding/dp_sharding.py
+-rw-r--r--  2.0 unx     5041 b- defN 23-Apr-09 11:33 torchrec/distributed/sharding/rw_sequence_sharding.py
+-rw-r--r--  2.0 unx    12850 b- defN 23-Apr-09 11:33 torchrec/distributed/sharding/rw_sharding.py
+-rw-r--r--  2.0 unx     3114 b- defN 23-Apr-09 11:33 torchrec/distributed/sharding/sequence_sharding.py
+-rw-r--r--  2.0 unx     7620 b- defN 23-Apr-09 11:33 torchrec/distributed/sharding/tw_sequence_sharding.py
+-rw-r--r--  2.0 unx    16102 b- defN 23-Apr-09 11:33 torchrec/distributed/sharding/tw_sharding.py
+-rw-r--r--  2.0 unx     1284 b- defN 23-Apr-09 11:33 torchrec/distributed/sharding/twcw_sharding.py
+-rw-r--r--  2.0 unx    19840 b- defN 23-Apr-09 11:33 torchrec/distributed/sharding/twrw_sharding.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-09 11:33 torchrec/distributed/test_utils/__init__.py
+-rw-r--r--  2.0 unx     8078 b- defN 23-Apr-09 11:33 torchrec/distributed/test_utils/infer_utils.py
+-rw-r--r--  2.0 unx     4868 b- defN 23-Apr-09 11:33 torchrec/distributed/test_utils/multi_process.py
+-rw-r--r--  2.0 unx    33925 b- defN 23-Apr-09 11:33 torchrec/distributed/test_utils/test_model.py
+-rw-r--r--  2.0 unx    11193 b- defN 23-Apr-09 11:33 torchrec/distributed/test_utils/test_model_parallel.py
+-rw-r--r--  2.0 unx    25075 b- defN 23-Apr-09 11:33 torchrec/distributed/test_utils/test_model_parallel_base.py
+-rw-r--r--  2.0 unx    15367 b- defN 23-Apr-09 11:33 torchrec/distributed/test_utils/test_sharding.py
+-rw-r--r--  2.0 unx      422 b- defN 23-Apr-09 11:33 torchrec/fx/__init__.py
+-rw-r--r--  2.0 unx     6477 b- defN 23-Apr-09 11:33 torchrec/fx/tracer.py
+-rw-r--r--  2.0 unx     3839 b- defN 23-Apr-09 11:33 torchrec/fx/utils.py
+-rw-r--r--  2.0 unx     1223 b- defN 23-Apr-09 11:33 torchrec/inference/__init__.py
+-rw-r--r--  2.0 unx     3614 b- defN 23-Apr-09 11:33 torchrec/inference/client.py
+-rw-r--r--  2.0 unx     3957 b- defN 23-Apr-09 11:33 torchrec/inference/model_packager.py
+-rw-r--r--  2.0 unx     7834 b- defN 23-Apr-09 11:33 torchrec/inference/modules.py
+-rw-r--r--  2.0 unx     3797 b- defN 23-Apr-09 11:33 torchrec/inference/state_dict_transform.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-09 11:33 torchrec/metrics/__init__.py
+-rw-r--r--  2.0 unx    11254 b- defN 23-Apr-09 11:33 torchrec/metrics/auc.py
+-rw-r--r--  2.0 unx     3703 b- defN 23-Apr-09 11:33 torchrec/metrics/calibration.py
+-rw-r--r--  2.0 unx     3465 b- defN 23-Apr-09 11:33 torchrec/metrics/ctr.py
+-rw-r--r--  2.0 unx     3836 b- defN 23-Apr-09 11:33 torchrec/metrics/mae.py
+-rw-r--r--  2.0 unx    17448 b- defN 23-Apr-09 11:33 torchrec/metrics/metric_module.py
+-rw-r--r--  2.0 unx     6001 b- defN 23-Apr-09 11:33 torchrec/metrics/metrics_config.py
+-rw-r--r--  2.0 unx     3543 b- defN 23-Apr-09 11:33 torchrec/metrics/metrics_namespace.py
+-rw-r--r--  2.0 unx     3904 b- defN 23-Apr-09 11:33 torchrec/metrics/model_utils.py
+-rw-r--r--  2.0 unx     4631 b- defN 23-Apr-09 11:33 torchrec/metrics/mse.py
+-rw-r--r--  2.0 unx     5605 b- defN 23-Apr-09 11:33 torchrec/metrics/multiclass_recall.py
+-rw-r--r--  2.0 unx     6811 b- defN 23-Apr-09 11:33 torchrec/metrics/ne.py
+-rw-r--r--  2.0 unx    30635 b- defN 23-Apr-09 11:33 torchrec/metrics/rec_metric.py
+-rw-r--r--  2.0 unx     6057 b- defN 23-Apr-09 11:33 torchrec/metrics/throughput.py
+-rw-r--r--  2.0 unx    10622 b- defN 23-Apr-09 11:33 torchrec/metrics/tower_qps.py
+-rw-r--r--  2.0 unx     2867 b- defN 23-Apr-09 11:33 torchrec/metrics/weighted_avg.py
+-rw-r--r--  2.0 unx    16441 b- defN 23-Apr-09 11:33 torchrec/metrics/test_utils/__init__.py
+-rw-r--r--  2.0 unx      913 b- defN 23-Apr-09 11:33 torchrec/models/__init__.py
+-rw-r--r--  2.0 unx    11325 b- defN 23-Apr-09 11:33 torchrec/models/deepfm.py
+-rw-r--r--  2.0 unx    29989 b- defN 23-Apr-09 11:33 torchrec/models/dlrm.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-09 11:33 torchrec/models/experimental/__init__.py
+-rw-r--r--  2.0 unx     9823 b- defN 23-Apr-09 11:33 torchrec/models/experimental/test_transformerdlrm.py
+-rw-r--r--  2.0 unx     7434 b- defN 23-Apr-09 11:33 torchrec/models/experimental/transformerdlrm.py
+-rw-r--r--  2.0 unx     1179 b- defN 23-Apr-09 11:33 torchrec/modules/__init__.py
+-rw-r--r--  2.0 unx     1456 b- defN 23-Apr-09 11:33 torchrec/modules/activation.py
+-rw-r--r--  2.0 unx    15163 b- defN 23-Apr-09 11:33 torchrec/modules/crossnet.py
+-rw-r--r--  2.0 unx     8415 b- defN 23-Apr-09 11:33 torchrec/modules/deepfm.py
+-rw-r--r--  2.0 unx     5131 b- defN 23-Apr-09 11:33 torchrec/modules/embedding_configs.py
+-rw-r--r--  2.0 unx    12822 b- defN 23-Apr-09 11:33 torchrec/modules/embedding_modules.py
+-rw-r--r--  2.0 unx     4858 b- defN 23-Apr-09 11:33 torchrec/modules/embedding_tower.py
+-rw-r--r--  2.0 unx    12103 b- defN 23-Apr-09 11:33 torchrec/modules/feature_processor.py
+-rw-r--r--  2.0 unx    31184 b- defN 23-Apr-09 11:33 torchrec/modules/fused_embedding_modules.py
+-rw-r--r--  2.0 unx    10696 b- defN 23-Apr-09 11:33 torchrec/modules/lazy_extension.py
+-rw-r--r--  2.0 unx     6309 b- defN 23-Apr-09 11:33 torchrec/modules/mlp.py
+-rw-r--r--  2.0 unx     4022 b- defN 23-Apr-09 11:33 torchrec/modules/utils.py
+-rw-r--r--  2.0 unx     1639 b- defN 23-Apr-09 11:33 torchrec/optim/__init__.py
+-rw-r--r--  2.0 unx     2012 b- defN 23-Apr-09 11:33 torchrec/optim/apply_optimizer_in_backward.py
+-rw-r--r--  2.0 unx     1569 b- defN 23-Apr-09 11:33 torchrec/optim/clipping.py
+-rw-r--r--  2.0 unx     1353 b- defN 23-Apr-09 11:33 torchrec/optim/fused.py
+-rw-r--r--  2.0 unx    16069 b- defN 23-Apr-09 11:33 torchrec/optim/keyed.py
+-rw-r--r--  2.0 unx     4420 b- defN 23-Apr-09 11:33 torchrec/optim/optimizers.py
+-rw-r--r--  2.0 unx     7405 b- defN 23-Apr-09 11:33 torchrec/optim/rowwise_adagrad.py
+-rw-r--r--  2.0 unx     4865 b- defN 23-Apr-09 11:33 torchrec/optim/warmup.py
+-rw-r--r--  2.0 unx      560 b- defN 23-Apr-09 11:33 torchrec/optim/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1140 b- defN 23-Apr-09 11:33 torchrec/quant/__init__.py
+-rw-r--r--  2.0 unx    21598 b- defN 23-Apr-09 11:33 torchrec/quant/embedding_modules.py
+-rw-r--r--  2.0 unx     3691 b- defN 23-Apr-09 11:33 torchrec/quant/utils.py
+-rw-r--r--  2.0 unx     1163 b- defN 23-Apr-09 11:33 torchrec/sparse/__init__.py
+-rw-r--r--  2.0 unx    52592 b- defN 23-Apr-09 11:33 torchrec/sparse/jagged_tensor.py
+-rw-r--r--  2.0 unx     1430 b- defN 23-Apr-09 11:33 torchrec/sparse/test_utils/__init__.py
+-rw-r--r--  2.0 unx     5661 b- defN 23-Apr-09 11:33 torchrec/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1530 b- defN 23-Apr-09 11:37 torchrec_nightly-2023.4.9.dist-info/LICENSE
+-rw-r--r--  2.0 unx     5011 b- defN 23-Apr-09 11:37 torchrec_nightly-2023.4.9.dist-info/METADATA
+-rw-r--r--  2.0 unx       93 b- defN 23-Apr-09 11:37 torchrec_nightly-2023.4.9.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 23-Apr-09 11:37 torchrec_nightly-2023.4.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    12787 b- defN 23-Apr-09 11:37 torchrec_nightly-2023.4.9.dist-info/RECORD
+137 files, 1352168 bytes uncompressed, 311386 bytes compressed:  77.0%
```

## zipnote {}

```diff
@@ -390,23 +390,23 @@
 
 Filename: torchrec/sparse/test_utils/__init__.py
 Comment: 
 
 Filename: torchrec/test_utils/__init__.py
 Comment: 
 
-Filename: torchrec_nightly-2023.4.7.dist-info/LICENSE
+Filename: torchrec_nightly-2023.4.9.dist-info/LICENSE
 Comment: 
 
-Filename: torchrec_nightly-2023.4.7.dist-info/METADATA
+Filename: torchrec_nightly-2023.4.9.dist-info/METADATA
 Comment: 
 
-Filename: torchrec_nightly-2023.4.7.dist-info/WHEEL
+Filename: torchrec_nightly-2023.4.9.dist-info/WHEEL
 Comment: 
 
-Filename: torchrec_nightly-2023.4.7.dist-info/top_level.txt
+Filename: torchrec_nightly-2023.4.9.dist-info/top_level.txt
 Comment: 
 
-Filename: torchrec_nightly-2023.4.7.dist-info/RECORD
+Filename: torchrec_nightly-2023.4.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## torchrec/distributed/types.py

```diff
@@ -121,14 +121,15 @@
         ...
 
     def decode(
         self, input_grad: torch.Tensor, ctx: Optional[QuantizationContext] = None
     ) -> torch.Tensor:
         ...
 
+    @property
     def quantized_dtype(self) -> torch.dtype:
         """
         tensor.dtype of the resultant encode(input_tensor)
         """
         ...
 
     def calc_quantized_size(
```

## torchrec/distributed/planner/shard_estimators.py

```diff
@@ -29,15 +29,15 @@
     PlannerError,
     ShardEstimator,
     ShardingOption,
     Storage,
     Topology,
 )
 from torchrec.distributed.planner.utils import prod, sharder_name
-from torchrec.distributed.types import ModuleSharder, ShardingType
+from torchrec.distributed.types import CommOp, ModuleSharder, ShardingType
 
 from torchrec.modules.embedding_modules import EmbeddingBagCollectionInterface
 
 
 class EmbeddingPerfEstimator(ShardEstimator):
     """
     Embedding Wall Time Perf Estimator
@@ -105,25 +105,37 @@
                 and self._constraints.get(sharding_option.name)
                 and self._constraints[sharding_option.name].is_weighted
             ):
                 is_weighted = self._constraints[sharding_option.name].is_weighted
             else:
                 is_weighted = False
 
+            table_data_type_size = sharding_option.tensor.element_size()
+            (
+                fwd_a2a_comm_data_type_size,
+                bwd_a2a_comm_data_type_size,
+                fwd_sr_comm_data_type_size,
+                bwd_sr_comm_data_type_size,
+            ) = _extract_comm_data_type_size(sharder, sharding_option)
+
             shard_perfs = perf_func_emb_wall_time(
                 shard_sizes=[shard.size for shard in sharding_option.shards],
                 compute_kernel=sharding_option.compute_kernel,
                 compute_device=self._topology.compute_device,
                 sharding_type=sharding_option.sharding_type,
                 batch_sizes=batch_sizes,
                 world_size=self._topology.world_size,
                 local_world_size=self._topology.local_world_size,
                 input_lengths=sharding_option.input_lengths,
                 input_data_type_size=BIGINT_DTYPE,
-                output_data_type_size=sharding_option.tensor.element_size(),
+                table_data_type_size=table_data_type_size,
+                fwd_a2a_comm_data_type_size=fwd_a2a_comm_data_type_size,
+                bwd_a2a_comm_data_type_size=bwd_a2a_comm_data_type_size,
+                fwd_sr_comm_data_type_size=fwd_sr_comm_data_type_size,
+                bwd_sr_comm_data_type_size=bwd_sr_comm_data_type_size,
                 num_poolings=num_poolings,
                 hbm_mem_bw=self._topology.hbm_mem_bw,
                 ddr_mem_bw=self._topology.ddr_mem_bw,
                 intra_host_bw=self._topology.intra_host_bw,
                 inter_host_bw=self._topology.inter_host_bw,
                 is_pooled=sharding_option.is_pooled,
                 is_weighted=is_weighted,
@@ -142,15 +154,19 @@
     compute_device: str,
     sharding_type: str,
     batch_sizes: List[int],
     world_size: int,
     local_world_size: int,
     input_lengths: List[float],
     input_data_type_size: float,
-    output_data_type_size: float,
+    table_data_type_size: float,
+    fwd_a2a_comm_data_type_size: float,
+    bwd_a2a_comm_data_type_size: float,
+    fwd_sr_comm_data_type_size: float,
+    bwd_sr_comm_data_type_size: float,
     num_poolings: List[float],
     hbm_mem_bw: float,
     ddr_mem_bw: float,
     intra_host_bw: float,
     inter_host_bw: float,
     is_pooled: bool,
     is_weighted: bool = False,
@@ -170,16 +186,19 @@
         batch_sizes (List[int]): batch size for each input feature.
         world_size (int): the number of devices for all hosts.
         local_world_size (int): the number of the device for each host.
         input_lengths (List[float]): the list of the average number of lookups of each
             input query feature.
         input_data_type_size (float): the data type size of the distributed
             data_parallel input.
-        output_data_type_size (float): the data type size of the distributed
-            data_parallel output.
+        table_data_type_size (float): the data type size of the table.
+        fwd_comm_data_type_size (float): the data type size of the distributed
+            data_parallel input during forward communication.
+        bwd_comm_data_type_size (float): the data type size of the distributed
+            data_parallel input during backward communication.
         num_poolings (List[float]): number of poolings per sample, typically 1.0.
         hbm_mem_bw (float): the bandwidth of the device HBM.
         ddr_mem_bw (float): the bandwidth of the system DDR memory.
         intra_host_bw (float): the bandwidth within a single host like multiple threads.
         inter_host_bw (float): the bandwidth between two hosts like multiple machines.
         is_pooled (bool): True if embedding output is pooled (ie. `EmbeddingBag`), False
             if unpooled/sequential (ie. `Embedding`).
@@ -212,15 +231,17 @@
             shard_perf = _get_tw_sharding_perf(
                 batch_sizes=batch_sizes,
                 world_size=world_size,
                 local_world_size=local_world_size,
                 input_lengths=input_lengths,
                 emb_dim=emb_dim,
                 input_data_type_size=input_data_type_size,
-                output_data_type_size=output_data_type_size,
+                table_data_type_size=table_data_type_size,
+                fwd_a2a_comm_data_type_size=fwd_a2a_comm_data_type_size,
+                bwd_a2a_comm_data_type_size=bwd_a2a_comm_data_type_size,
                 num_poolings=num_poolings,
                 device_bw=device_bw,
                 inter_host_bw=inter_host_bw,
                 intra_host_bw=intra_host_bw,
                 is_pooled=is_pooled,
                 is_weighted=is_weighted,
                 is_inference=is_inference,
@@ -230,15 +251,19 @@
             shard_perf = _get_rw_sharding_perf(
                 batch_sizes=batch_sizes,
                 world_size=world_size,
                 local_world_size=local_world_size,
                 input_lengths=input_lengths,
                 emb_dim=emb_dim,
                 input_data_type_size=input_data_type_size,
-                output_data_type_size=output_data_type_size,
+                table_data_type_size=table_data_type_size,
+                fwd_a2a_comm_data_type_size=fwd_a2a_comm_data_type_size,
+                bwd_a2a_comm_data_type_size=bwd_a2a_comm_data_type_size,
+                fwd_sr_comm_data_type_size=fwd_sr_comm_data_type_size,
+                bwd_sr_comm_data_type_size=bwd_sr_comm_data_type_size,
                 num_poolings=num_poolings,
                 device_bw=device_bw,
                 inter_host_bw=inter_host_bw,
                 intra_host_bw=intra_host_bw,
                 is_pooled=is_pooled,
                 is_weighted=is_weighted,
                 has_feature_processor=has_feature_processor,
@@ -247,15 +272,19 @@
             shard_perf = _get_twrw_sharding_perf(
                 batch_sizes=batch_sizes,
                 world_size=world_size,
                 local_world_size=local_world_size,
                 input_lengths=input_lengths,
                 emb_dim=emb_dim,
                 input_data_type_size=input_data_type_size,
-                output_data_type_size=output_data_type_size,
+                table_data_type_size=table_data_type_size,
+                fwd_a2a_comm_data_type_size=fwd_a2a_comm_data_type_size,
+                bwd_a2a_comm_data_type_size=bwd_a2a_comm_data_type_size,
+                fwd_sr_comm_data_type_size=fwd_sr_comm_data_type_size,
+                bwd_sr_comm_data_type_size=bwd_sr_comm_data_type_size,
                 num_poolings=num_poolings,
                 device_bw=device_bw,
                 inter_host_bw=inter_host_bw,
                 intra_host_bw=intra_host_bw,
                 is_pooled=is_pooled,
                 is_weighted=is_weighted,
                 has_feature_processor=has_feature_processor,
@@ -265,15 +294,15 @@
                 batch_sizes=batch_sizes,
                 world_size=world_size,
                 local_world_size=local_world_size,
                 input_lengths=input_lengths,
                 grad_num_elem=hash_size * emb_dim,
                 emb_dim=emb_dim,
                 input_data_type_size=input_data_type_size,
-                output_data_type_size=output_data_type_size,
+                table_data_type_size=table_data_type_size,
                 num_poolings=num_poolings,
                 device_bw=device_bw,
                 inter_host_bw=inter_host_bw,
                 is_pooled=is_pooled,
                 is_weighted=is_weighted,
                 has_feature_processor=has_feature_processor,
             )
@@ -289,15 +318,17 @@
 def _get_tw_sharding_perf(
     batch_sizes: List[int],
     world_size: int,
     local_world_size: int,
     input_lengths: List[float],
     emb_dim: int,
     input_data_type_size: float,
-    output_data_type_size: float,
+    table_data_type_size: float,
+    fwd_a2a_comm_data_type_size: float,
+    bwd_a2a_comm_data_type_size: float,
     num_poolings: List[float],
     device_bw: float,
     inter_host_bw: float,
     intra_host_bw: float,
     is_pooled: bool,
     is_weighted: bool = False,
     is_inference: bool = False,
@@ -314,40 +345,45 @@
 
     input_read_size = math.ceil(batch_inputs * world_size * input_data_type_size)
     if is_weighted or has_feature_processor:
         input_read_size *= 2
 
     # minimum embedding dim is set to 32 due to kernel usage
     embedding_lookup_size = (
-        batch_inputs * world_size * max(emb_dim, 32) * output_data_type_size
+        batch_inputs * world_size * max(emb_dim, 32) * table_data_type_size
     )
 
-    output_write_size = batch_outputs * world_size * emb_dim * output_data_type_size
+    fwd_output_write_size = (
+        batch_outputs * world_size * emb_dim * fwd_a2a_comm_data_type_size
+    )
+    bwd_output_write_size = (
+        batch_outputs * world_size * emb_dim * bwd_a2a_comm_data_type_size
+    )
 
     # embedding dim below 128 will reduce kernel efficency
     block_usage_penalty = 1
     if emb_dim < FULL_BLOCK_EMB_DIM:
         if emb_dim >= 64:
             block_usage_penalty = HALF_BLOCK_PENALTY
         else:  # emb_dim >= 32
             block_usage_penalty = QUARTER_BLOCK_PENALTY
 
     comms_bw = inter_host_bw if world_size > local_world_size else intra_host_bw
-    fwd_comms = output_write_size / comms_bw
+    fwd_comms = fwd_output_write_size / comms_bw
 
     fwd_compute = (
-        (input_read_size + embedding_lookup_size + output_write_size)
+        (input_read_size + embedding_lookup_size + fwd_output_write_size)
         * block_usage_penalty
         / device_bw
     )
     if is_inference:
         # only consider forward compute and comms for inference
         return fwd_compute + fwd_comms
 
-    bwd_comms = fwd_comms
+    bwd_comms = bwd_output_write_size / comms_bw
 
     bwd_grad_indice_weights_kernel = (
         fwd_compute * WEIGHTED_KERNEL_MULTIPLIER
         if is_weighted or has_feature_processor
         else 0
     )
 
@@ -368,15 +404,19 @@
 def _get_rw_sharding_perf(
     batch_sizes: List[int],
     world_size: int,
     local_world_size: int,
     input_lengths: List[float],
     emb_dim: int,
     input_data_type_size: float,
-    output_data_type_size: float,
+    table_data_type_size: float,
+    fwd_a2a_comm_data_type_size: float,
+    bwd_a2a_comm_data_type_size: float,
+    fwd_sr_comm_data_type_size: float,
+    bwd_sr_comm_data_type_size: float,
     num_poolings: List[float],
     device_bw: float,
     inter_host_bw: float,
     intra_host_bw: float,
     is_pooled: bool,
     is_weighted: bool = False,
     has_feature_processor: bool = False,
@@ -391,28 +431,37 @@
         else batch_inputs
     )
 
     input_read_size = math.ceil(batch_inputs * world_size * input_data_type_size)
     if is_weighted or has_feature_processor:
         input_read_size *= 2
 
-    embedding_lookup_size = batch_inputs * world_size * emb_dim * output_data_type_size
+    embedding_lookup_size = batch_inputs * world_size * emb_dim * table_data_type_size
 
-    output_write_size = batch_outputs * world_size * emb_dim * output_data_type_size
+    fwd_output_write_size = (
+        batch_outputs * world_size * emb_dim * fwd_sr_comm_data_type_size
+        if is_pooled
+        else batch_outputs * world_size * emb_dim * fwd_a2a_comm_data_type_size
+    )
+    bwd_output_write_size = (
+        batch_outputs * world_size * emb_dim * bwd_sr_comm_data_type_size
+        if is_pooled
+        else batch_outputs * world_size * emb_dim * bwd_a2a_comm_data_type_size
+    )
 
     comms_bw = inter_host_bw if world_size > local_world_size else intra_host_bw
-    fwd_comms = output_write_size / comms_bw
+    fwd_comms = fwd_output_write_size / comms_bw
 
     fwd_compute = (
-        input_read_size + embedding_lookup_size + output_write_size
+        input_read_size + embedding_lookup_size + fwd_output_write_size
     ) / device_bw
 
-    bwd_comms = fwd_comms
+    bwd_comms = bwd_output_write_size / comms_bw
 
-    bwd_batched_copy = output_write_size * BATCHED_COPY_PERF_FACTOR / device_bw
+    bwd_batched_copy = bwd_output_write_size * BATCHED_COPY_PERF_FACTOR / device_bw
 
     bwd_grad_indice_weights_kernel = (
         fwd_compute * WEIGHTED_KERNEL_MULTIPLIER
         if is_weighted or has_feature_processor
         else 0
     )
 
@@ -431,15 +480,19 @@
 def _get_twrw_sharding_perf(
     batch_sizes: List[int],
     world_size: int,
     local_world_size: int,
     input_lengths: List[float],
     emb_dim: int,
     input_data_type_size: float,
-    output_data_type_size: float,
+    table_data_type_size: float,
+    fwd_a2a_comm_data_type_size: float,
+    bwd_a2a_comm_data_type_size: float,
+    fwd_sr_comm_data_type_size: float,
+    bwd_sr_comm_data_type_size: float,
     num_poolings: List[float],
     device_bw: float,
     inter_host_bw: float,
     intra_host_bw: float,
     is_pooled: bool,
     is_weighted: bool = False,
     has_feature_processor: bool = False,
@@ -454,36 +507,50 @@
         else batch_inputs
     )
 
     input_read_size = math.ceil(batch_inputs * world_size * input_data_type_size)
     if is_weighted or has_feature_processor:
         input_read_size *= 2
 
-    embedding_lookup_size = batch_inputs * world_size * emb_dim * output_data_type_size
+    embedding_lookup_size = batch_inputs * world_size * emb_dim * table_data_type_size
 
-    output_write_size = batch_outputs * world_size * emb_dim * output_data_type_size
+    fwd_output_write_size = (
+        batch_outputs * world_size * emb_dim * fwd_sr_comm_data_type_size
+    )
+    bwd_output_write_size = (
+        batch_outputs * world_size * emb_dim * bwd_sr_comm_data_type_size
+    )
 
-    fwd_comms = output_write_size / intra_host_bw
+    # intra host comm
+    fwd_comms = fwd_output_write_size / intra_host_bw
 
+    # inter host comm
     if world_size > local_world_size:
-        fwd_comms += output_write_size * (local_world_size / world_size) / inter_host_bw
+        inter_host_fwd_fwd_output_write_size = (
+            batch_outputs * world_size * emb_dim * fwd_a2a_comm_data_type_size
+        )
+        fwd_comms += (
+            inter_host_fwd_fwd_output_write_size
+            * (local_world_size / world_size)
+            / inter_host_bw
+        )
 
     fwd_compute = (
-        input_read_size + embedding_lookup_size + output_write_size
+        input_read_size + embedding_lookup_size + fwd_output_write_size
     ) / device_bw
 
-    bwd_comms = fwd_comms
+    bwd_comms = bwd_output_write_size / intra_host_bw
 
     bwd_grad_indice_weights_kernel = (
         fwd_compute * WEIGHTED_KERNEL_MULTIPLIER
         if is_weighted or has_feature_processor
         else 0
     )
 
-    bwd_batched_copy = output_write_size * BATCHED_COPY_PERF_FACTOR / device_bw
+    bwd_batched_copy = bwd_output_write_size * BATCHED_COPY_PERF_FACTOR / device_bw
 
     bwd_compute = fwd_compute * BWD_COMPUTE_MULTIPLIER
 
     return (
         bwd_comms
         + bwd_batched_copy
         + bwd_grad_indice_weights_kernel
@@ -497,15 +564,15 @@
     batch_sizes: List[int],
     world_size: int,
     local_world_size: int,
     input_lengths: List[float],
     grad_num_elem: int,
     emb_dim: int,
     input_data_type_size: float,
-    output_data_type_size: float,
+    table_data_type_size: float,
     num_poolings: List[float],
     device_bw: float,
     inter_host_bw: float,
     is_pooled: bool,
     is_weighted: bool = False,
     has_feature_processor: bool = False,
 ) -> float:
@@ -518,18 +585,18 @@
         else batch_inputs
     )
 
     input_read_size = math.ceil(batch_inputs * input_data_type_size)
     if is_weighted or has_feature_processor:
         input_read_size *= 2
 
-    embedding_lookup_size = batch_inputs * emb_dim * output_data_type_size
+    embedding_lookup_size = batch_inputs * emb_dim * table_data_type_size
 
-    output_write_size = batch_outputs * emb_dim * output_data_type_size
-    table_size = grad_num_elem * output_data_type_size
+    output_write_size = batch_outputs * emb_dim * table_data_type_size
+    table_size = grad_num_elem * table_data_type_size
 
     fwd_compute = (
         input_read_size + embedding_lookup_size + output_write_size
     ) / device_bw
 
     num_nodes = min(world_size / local_world_size, 2)
 
@@ -560,14 +627,72 @@
         + optimizer_kernels
         + bwd_grad_indice_weights_kernel
         + bwd_compute
         + fwd_compute
     )
 
 
+def _extract_comm_data_type_size(
+    sharder: ModuleSharder[nn.Module], sharding_option: ShardingOption
+) -> Tuple[float, float, float, float]:
+    table_data_type_size = sharding_option.tensor.element_size()
+
+    fwd_a2a_comm_data_type_size = table_data_type_size
+    bwd_a2a_comm_data_type_size = table_data_type_size
+    fwd_sr_comm_data_type_size = table_data_type_size
+    bwd_sr_comm_data_type_size = table_data_type_size
+
+    if sharder.qcomm_codecs_registry is not None:
+        qcomm_codecs_registry = sharder.qcomm_codecs_registry
+        if (
+            sharding_option.is_pooled
+            and CommOp.POOLED_EMBEDDINGS_ALL_TO_ALL.name in qcomm_codecs_registry
+        ):
+            codecs = sharder.qcomm_codecs_registry[
+                CommOp.POOLED_EMBEDDINGS_ALL_TO_ALL.name
+            ]
+            fwd_a2a_comm_data_type_size = torch.tensor(
+                [], dtype=codecs.forward.quantized_dtype
+            ).element_size()
+            bwd_a2a_comm_data_type_size = torch.tensor(
+                [], dtype=codecs.backward.quantized_dtype
+            ).element_size()
+
+        if (
+            not sharding_option.is_pooled
+            and CommOp.SEQUENCE_EMBEDDINGS_ALL_TO_ALL.name in qcomm_codecs_registry
+        ):
+            codecs = qcomm_codecs_registry[CommOp.SEQUENCE_EMBEDDINGS_ALL_TO_ALL.name]
+            fwd_a2a_comm_data_type_size = torch.tensor(
+                [], dtype=codecs.forward.quantized_dtype
+            ).element_size()
+            bwd_a2a_comm_data_type_size = torch.tensor(
+                [], dtype=codecs.backward.quantized_dtype
+            ).element_size()
+
+        if (
+            sharding_option.is_pooled
+            and CommOp.POOLED_EMBEDDINGS_REDUCE_SCATTER.name in qcomm_codecs_registry
+        ):
+            codecs = qcomm_codecs_registry[CommOp.POOLED_EMBEDDINGS_REDUCE_SCATTER.name]
+            fwd_sr_comm_data_type_size = torch.tensor(
+                [], dtype=codecs.forward.quantized_dtype
+            ).element_size()
+            bwd_sr_comm_data_type_size = torch.tensor(
+                [], dtype=codecs.backward.quantized_dtype
+            ).element_size()
+
+    return (
+        fwd_a2a_comm_data_type_size,
+        bwd_a2a_comm_data_type_size,
+        fwd_sr_comm_data_type_size,
+        bwd_sr_comm_data_type_size,
+    )
+
+
 class EmbeddingStorageEstimator(ShardEstimator):
     """
     Embedding Storage Usage Estimator
     """
 
     def __init__(
         self,
```

## Comparing `torchrec_nightly-2023.4.7.dist-info/LICENSE` & `torchrec_nightly-2023.4.9.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `torchrec_nightly-2023.4.7.dist-info/METADATA` & `torchrec_nightly-2023.4.9.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: torchrec-nightly
-Version: 2023.4.7
+Version: 2023.4.9
 Summary: Pytorch domain library for recommendation systems
 Home-page: https://github.com/pytorch/torchrec
 Author: TorchRec Team
 Author-email: packages@pytorch.org
 License: BSD-3
 Keywords: pytorch,recommendation systems,sharding
 Classifier: Development Status :: 4 - Beta
```

## Comparing `torchrec_nightly-2023.4.7.dist-info/RECORD` & `torchrec_nightly-2023.4.9.dist-info/RECORD`

 * *Files 1% similar despite different names*

```diff
@@ -32,26 +32,26 @@
 torchrec/distributed/model_parallel.py,sha256=cwpTtQSD9S6XwQENh9tv9A0DqNkd2jfIBmVH3XJF4p4,19345
 torchrec/distributed/quant_embedding.py,sha256=YLMXlSGyeJVXzvbZlIk6VUp0o8zp0-7JdhApBKEV3Vk,12699
 torchrec/distributed/quant_embedding_kernel.py,sha256=uR6EQ3G-5Sou6kPRR2ckdkaaGlXyBYtVZKFuSbZzMqQ,11122
 torchrec/distributed/quant_embeddingbag.py,sha256=A-bDFL-BEUa2pyvC1Hg2efbnSb-bPCo4-C8bQTMjYnw,9878
 torchrec/distributed/shard.py,sha256=qoqahqZ5Sy-wIbgj1xm5wxdRW9asXkXr7ncW9lfNUQo,8710
 torchrec/distributed/sharding_plan.py,sha256=1CZyCmPp_lJPeitOgufRNfe77aXp9WtxsKDqVOm8XnU,19218
 torchrec/distributed/train_pipeline.py,sha256=RotmGdM_UduxSbrf3l8JKrowIeMhrB2DaNJHFwXPjIY,22330
-torchrec/distributed/types.py,sha256=eOYVk2eHG9zO7iBHVaoLF1flDwrEiFKUgmnFmhh_VOw,24913
+torchrec/distributed/types.py,sha256=tNd_B5PXjaOqPmUXxSZLXB3a71sjU8LBtSY9kuGxe7I,24927
 torchrec/distributed/utils.py,sha256=isPXbyPhHlbjiA0280ZY0V_wPUBuvDvvH4w6ziP5XA8,11373
 torchrec/distributed/composable/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchrec/distributed/composable/table_batched_embedding_slice.py,sha256=x439M8TTXQtzoihan1OKKbmGYkqJlAxxTHCDz5295RY,3207
 torchrec/distributed/planner/__init__.py,sha256=UWnxb-SuE211uJGdwtSkKRVADT3plQozB2l6fvs6Ve0,1025
 torchrec/distributed/planner/constants.py,sha256=MkeVqYO2QGg57i6fs29lZb2dScaaR9mdQVsee4NxyFc,3135
 torchrec/distributed/planner/enumerators.py,sha256=hZzhnfMrOz65lBoddxTKBb01hm43R_5tdpysJFSzCLE,10318
 torchrec/distributed/planner/partitioners.py,sha256=I2fTqnKWcaL7G8Cz6aXC61S1sa3ZtHSz1Xp8xGbEAr8,12103
 torchrec/distributed/planner/perf_models.py,sha256=srFJ0AkOhLVhumdcuyYzl5J1qzezt6knt-TxZvK6Zy4,824
 torchrec/distributed/planner/planners.py,sha256=ZccWhKg9QUA2f3oAtVnFWjPfKEiSjRzZVCMGcenyG7c,11869
 torchrec/distributed/planner/proposers.py,sha256=A-adzCCshGZUIyCP7DPOGDuh4hbojawWuObxsIf-rOg,11094
-torchrec/distributed/planner/shard_estimators.py,sha256=RrqugawNv1dAMjOnI1WpJ4ZshUwaB4NK9a82-Cd7mFw,34788
+torchrec/distributed/planner/shard_estimators.py,sha256=H9rZJV9_BwgRyImzXT0YXiymJ6FhRzudTg9NXrLoTOs,40173
 torchrec/distributed/planner/stats.py,sha256=my1OB-r1HO-ixc7joRz1CltZlQJDBrjJOtLIXDY7B5c,21410
 torchrec/distributed/planner/storage_reservations.py,sha256=rPqeD03f3mg8yoSqy9CZfOmxVMUGrCz98nPqwvxhApc,9125
 torchrec/distributed/planner/types.py,sha256=R-NCgPEq_QImroSWlreGPCtq_fNBKkdiIegzM8CJF4Y,12611
 torchrec/distributed/planner/utils.py,sha256=YwOUrqb-D6HaVtWCRFz9RWhBdMRBqjMaYYhoQgyOkQg,1119
 torchrec/distributed/sharding/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 torchrec/distributed/sharding/cw_sequence_sharding.py,sha256=o8RZAs2zivNP8MZ5QUWF0alw7KwWZnJVG9iKHM4EJdE,2539
 torchrec/distributed/sharding/cw_sharding.py,sha256=PLJHgxkQvX_sNI8RJlz4xIemb4vO2KHOe_9PylZ2je8,9519
@@ -126,12 +126,12 @@
 torchrec/quant/__init__.py,sha256=A6NIA6ztq6iP1JTLRLNzlgnCcd-LaN8efnxGub3Ii4A,1140
 torchrec/quant/embedding_modules.py,sha256=pxHWBEDwtYDeAPrfcl2IkMJ1JBlF7RO7h0o495gMjfk,21598
 torchrec/quant/utils.py,sha256=2oUJIsrzE7ijvPs5DYUa06wOfmRvU1KdU1aQI7DUccs,3691
 torchrec/sparse/__init__.py,sha256=dLqSye4Jo6obnNNTUKdPDxPQb9sL2U4weemSn-DjpYk,1163
 torchrec/sparse/jagged_tensor.py,sha256=B-97CiiaT8iS5PwceZAXul0ka3UooRpg64zqK7Jm-tU,52592
 torchrec/sparse/test_utils/__init__.py,sha256=BLxfGKJvwjjCiQM64O5wGAA_Cea0sG-buw9lTDWuqug,1430
 torchrec/test_utils/__init__.py,sha256=JncJcXS4N3gI7-fsizQ2-qiWM6MhIrpvskF_9gDf0Go,5661
-torchrec_nightly-2023.4.7.dist-info/LICENSE,sha256=e0Eotbf_rHOYPuEUlppIbvwy4SN98CZnl_hqwvbDA4Q,1530
-torchrec_nightly-2023.4.7.dist-info/METADATA,sha256=Qi4Z92i7iSXpp8hfuIAMP_3BzBG4y6zsxJcZOwu6Di0,5011
-torchrec_nightly-2023.4.7.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
-torchrec_nightly-2023.4.7.dist-info/top_level.txt,sha256=LoLcTAPLj_7x62AuyYmhEVBcx2WJ1Z1Nrknv0Jnk_gQ,9
-torchrec_nightly-2023.4.7.dist-info/RECORD,,
+torchrec_nightly-2023.4.9.dist-info/LICENSE,sha256=e0Eotbf_rHOYPuEUlppIbvwy4SN98CZnl_hqwvbDA4Q,1530
+torchrec_nightly-2023.4.9.dist-info/METADATA,sha256=q8tPaPHD-VBSU99GfvFvkMyBoSvXkIwhOZw_ERCAbpY,5011
+torchrec_nightly-2023.4.9.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
+torchrec_nightly-2023.4.9.dist-info/top_level.txt,sha256=LoLcTAPLj_7x62AuyYmhEVBcx2WJ1Z1Nrknv0Jnk_gQ,9
+torchrec_nightly-2023.4.9.dist-info/RECORD,,
```

